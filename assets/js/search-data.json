{"0": {
    "doc": "AR Models",
    "title": "Autoregressive (AR) Models",
    "content": "Autoregressive (AR) models are fundamental to time series analysis. They are estimated via regressing a variable on one or more of its lagged values. That is, AR models take the form: \\(Y_t = c + \\sum_{i = 1}^{p} \\beta_i Y_{t-i} + \\epsilon_t\\) Where we say p is the order of our auto regression. Their estimation in statistical software packages is generally straightforward. For additional information, see Wikipedia: Autoregressive model. ",
    "url": "/Time_Series/AR-models.html#autoregressive-ar-models",
    "relUrl": "/Time_Series/AR-models.html#autoregressive-ar-models"
  },"1": {
    "doc": "AR Models",
    "title": "Keep In Mind",
    "content": ". | An AR model can be univariate (scalar) or multivariate (vector). This may be important to implementing an AR model in your statisical package of choice. | Data should be properly formatted before estimation. If not, non-time series objects (e.g., a date column) may be interpereted by software as a time series variable, leading to erroneous output. | . ",
    "url": "/Time_Series/AR-models.html#keep-in-mind",
    "relUrl": "/Time_Series/AR-models.html#keep-in-mind"
  },"2": {
    "doc": "AR Models",
    "title": "Implementations",
    "content": "Following the instructions for creating and formatting Time Series Data, we will use quaterly GDP data downloaded from FRED as an example. ",
    "url": "/Time_Series/AR-models.html#implementations",
    "relUrl": "/Time_Series/AR-models.html#implementations"
  },"3": {
    "doc": "AR Models",
    "title": "Python",
    "content": "In Python, the statsmodels package provides a range of tools to fit models using maximum likelihood estimation. In the example below, we will use the AutoReg function. This can fit models of the form: . \\[y_t = \\delta_0 + \\delta_1 t + \\phi_1 y_{t-1} + \\ldots + \\phi_p y_{t-p} + \\sum_{i=1}^{s-1} \\gamma_i d_i + \\sum_{j=1}^{m} \\kappa_j x_{t,j} + \\epsilon_t.\\] where \\(d_i\\) are seasonal dummies, \\(x_{t,j}\\) are exogenous regressors, and the \\(\\phi_p\\) are the coefficients of the auto-regressive components of the model. Using GDP data, let’s fit an auto-regressive model of order 1, an AR(1), with AutoReg: . # Install pandas and statsmodels using 'pip install' or 'conda install' on the command line import pandas as pd from statsmodels.tsa.ar_model import AutoReg, ar_select_order gdp = pd.read_csv(\"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv\", index_col=0) ar1_model = AutoReg(gdp, 1) results = ar1_model.fit() print(results.summary()) . ## AutoReg Model Results ## ============================================================================== ## Dep. Variable: GDPC1 No. Observations: 292 ## Model: AutoReg(1) Log Likelihood -1625.980 ## Method: Conditional MLE S.D. of innovations 64.626 ## Date: 0000000000000000 AIC 8.358 ## Time: 00:00:00 BIC 8.396 ## Sample: 04-01-1947 HQIC 8.373 ## - 10-01-2019 ## ============================================================================== ## coef std err z P&gt;|z| [0.025 0.975] ## ------------------------------------------------------------------------------ ## intercept 21.8083 7.439 2.931 0.003 7.227 36.389 ## GDPC1.L1 1.0043 0.001 1356.492 0.000 1.003 1.006 ## Roots ## ============================================================================= ## Real Imaginary Modulus Frequency ## ----------------------------------------------------------------------------- ## AR.1 0.9957 +0.0000j 0.9957 0.0000 ## ----------------------------------------------------------------------------- . Now let’s use the automatic option to choose how many lags to include (this uses the BIC criterion to choose, though over criteria are available): . select_model = ar_select_order(gdp, maxlag=10) print(select_model.ar_lags) . ## [1 2 3] . This tells us to include lags up to 3. We can pass the list of lags right back to the Auto_Reg function: . arp_model = AutoReg(gdp, select_model.ar_lags) results_p = arp_model.fit() print(results_p.summary()) . ## AutoReg Model Results ## ============================================================================== ## Dep. Variable: GDPC1 No. Observations: 292 ## Model: AutoReg(3) Log Likelihood -1593.285 ## Method: Conditional MLE S.D. of innovations 59.989 ## Date: 0000000000000000 AIC 8.223 ## Time: 00:00:00 BIC 8.286 ## Sample: 10-01-1947 HQIC 8.248 ## - 10-01-2019 ## ============================================================================== ## coef std err z P&gt;|z| [0.025 0.975] ## ------------------------------------------------------------------------------ ## intercept 13.3707 7.098 1.884 0.060 -0.541 27.282 ## GDPC1.L1 1.2921 0.058 22.273 0.000 1.178 1.406 ## GDPC1.L2 -0.1253 0.095 -1.315 0.189 -0.312 0.062 ## GDPC1.L3 -0.1646 0.058 -2.826 0.005 -0.279 -0.050 ## Roots ## ============================================================================= ## Real Imaginary Modulus Frequency ## ----------------------------------------------------------------------------- ## AR.1 0.9960 +0.0000j 0.9960 0.0000 ## AR.2 1.7430 +0.0000j 1.7430 0.0000 ## AR.3 -3.5005 +0.0000j 3.5005 0.5000 ## ----------------------------------------------------------------------------- . ",
    "url": "/Time_Series/AR-models.html#python",
    "relUrl": "/Time_Series/AR-models.html#python"
  },"4": {
    "doc": "AR Models",
    "title": "R",
    "content": "#load data gdp = read.csv(\"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv\") #estimation via ols: pay attention to the selection of the 'GDPC1' column. #if the column is not specified, the function call also interprets the date column as a time series variable! ar_gdp = ar.ols(gdp$GDPC1) ar_gdp #lag order is automatically selected by minimizing AIC #disable this feature with the optional command 'aic = F'. Note: you will also likely wish to specify the argument 'order.max'. #ar.ols() defaults to demeaning the data automatically. Also consider taking logs and first differencing for statistically meaningful results. ",
    "url": "/Time_Series/AR-models.html#r",
    "relUrl": "/Time_Series/AR-models.html#r"
  },"5": {
    "doc": "AR Models",
    "title": "STATA",
    "content": "*load data import delimited \"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv\", clear *Generate the new date variable *To generalize to a different set of data, replace '1947q1' with your own series' start date. generate date_index = tq(1947q1) + _n-1 *Index the new variable format as quarter format date_index %tq *Convert a variable into time-series data tsset date_index *Specifiy and Run AR regression: this STATA method will not automatically select a lag order. *The 'L.' operator indicates the lagged value of a variable in STATA, 'L2.' its second lag, and so on. reg gdpc1 L.gdpc1 L2.gdpc1 *variables are not demeaned automatically by STATA. Also consider taking logs and first differencing for statistically meaningful results. ",
    "url": "/Time_Series/AR-models.html#stata",
    "relUrl": "/Time_Series/AR-models.html#stata"
  },"6": {
    "doc": "AR Models",
    "title": "AR Models",
    "content": " ",
    "url": "/Time_Series/AR-models.html",
    "relUrl": "/Time_Series/AR-models.html"
  },"7": {
    "doc": "ARCH Model",
    "title": "Autoregressive Conditional Heteroscedasticity (ARCH) model",
    "content": "The autoregressive conditional heteroscedasticity (ARCH) model is a statistical model for time series data that models the variance of the current error as a function of the actual sizes of the previous time periods’ errors. The ARCH model is appropriate when the error variance in a time series follows an autoregressive (AR) model. An ARCH(q) process can be written as $y_{t} = a_{0} + \\sum_{i=1}^{q}y_{t-q}+\\epislon_{t}$ where $\\epsilon_{t}$ denote the error terms. These $\\epsilon_{t}$ are split into a stochastic piece $z_{t}$ and a time-dependent standard deviation $\\sigma_{t}$ characterizing the typical size of the terms so that $\\epsilon_{t}=\\sigma_{t}z_{t}$. The random variable $z_{t}$ is a strong white noise process. The series $\\sigma_{t}^{2}$ is modeled by \\(\\sigma_{t}^{2} = \\alpha_{0} + \\alpha_{1}\\epsilon_{t-1}^{2} + \\dots + \\alpha_{q}\\epsilon_{t-1}^{2} = \\alpha_{0} + \\sum_{i=1}^{q}\\alpha_{i}\\epsilon_{t-i}^{2}\\) where $\\alpha_{0} &gt; 0$ and $\\alpha_{i} \\ geq 0$, $i &gt; 0$. An ARCH(q) model can be estimated using ordinary least squares. This procedure is as follows: . | Estimate the best fitting autoregressive model AR(q) \\(y_{t}=a_{0}+a_{1}y_{t-1}+\\dots +a_{q}y_{t-q}+\\epsilon_{t} =a_{0}+\\sum_{i=1}^{q}a_{i}y_{t-i}+\\epsilon_{t}\\). | Obtain the squares of the error $\\hat{\\epsilon}^{2}$ and regress them on a constant and q lagged values: \\(\\hat{\\epsilon}^{2}=\\hat {\\alpha}_{0}+\\sum_{i=1}^{q}\\hat{\\alpha}_{i}\\hat{\\epsilon}_{t-i}^{2}\\) where q is the length of ARCH lags. | Null Hypothesis: $\\alpha_{i}=0$ for all $i=1,\\dots ,q$. Alternative hypothesis : At least one of the estimated $\\alpha_{i}$ coefficients must be significant. Under the null hypothesis of no ARCH errors, the test statistic $T’R²$ follows $\\chi^{2}$ distribution with q degrees of freedom, where $T’$ is the number of equations in the model which fits the residuals vs the lags (i.e. $T’=T-q$). If $T’R²$ is greater than the Chi-square table value, we reject the null hypothesis and conclude there is an ARCH effect, otherwise we do not reject the null hypothesis. | . For additional information, see Wikipedia: Autoregressive conditional heteroskedasticity. ",
    "url": "/Time_Series/ARCH_Model.html#autoregressive-conditional-heteroscedasticity-arch-model",
    "relUrl": "/Time_Series/ARCH_Model.html#autoregressive-conditional-heteroscedasticity-arch-model"
  },"8": {
    "doc": "ARCH Model",
    "title": "Keep in Mind",
    "content": ". | Data should be properly formatted for estimation as a time-series. See creating a time series data set. If not, you may fail to execute or receive erroneous output. | ARCH can be used to model time-varying conditional variance. | . ",
    "url": "/Time_Series/ARCH_Model.html#keep-in-mind",
    "relUrl": "/Time_Series/ARCH_Model.html#keep-in-mind"
  },"9": {
    "doc": "ARCH Model",
    "title": "Also Consider",
    "content": ". | ARCH models can be univariate (scalar) or multivariate (vector). | ARCH models are commonly employed in modeling financial time series that exhibit time-varying volatility and volatility clustering, i.e. periods of swings interspersed with periods of relative calm. | If an autoregressive moving average (ARMA) model is assumed for the error variance, the model is a generalized autoregressive conditional heteroskedasticity (GARCH) model. For more information on GARCH models, see Wikipedia: GARCH. For information about estimating an GARCH models, see LOST: GARCH models. | . ",
    "url": "/Time_Series/ARCH_Model.html#also-consider",
    "relUrl": "/Time_Series/ARCH_Model.html#also-consider"
  },"10": {
    "doc": "ARCH Model",
    "title": "Implementations",
    "content": " ",
    "url": "/Time_Series/ARCH_Model.html#implementations",
    "relUrl": "/Time_Series/ARCH_Model.html#implementations"
  },"11": {
    "doc": "ARCH Model",
    "title": "Python",
    "content": "from random import gauss from random import seed from matplotlib import pyplot from arch import arch_model import numpy as np # seed the process np.random.seed(1) # Simulating a ARCH(1) process a0 = 1 a1 = .5 w = np.random.normal(size=1000) e = np.random.normal(size=1000) Y = np.empty_like(w) for t in range(1, len(w)): Y[t] = w[t] * np.sqrt((a0 + a1*Y[t-1]**2)) # fit model model = arch_model(Y, vol = \"ARCH\", rescale = \"FALSE\") model_fit = model.fit() print(model_fit.summary) . ",
    "url": "/Time_Series/ARCH_Model.html#python",
    "relUrl": "/Time_Series/ARCH_Model.html#python"
  },"12": {
    "doc": "ARCH Model",
    "title": "R",
    "content": "# setup library(fGarch) # seed pseudorandom number generator set.seed(1) # Simulating a ARCH(1) process e &lt;- NULL obs &lt;- 1000 e[1] &lt;- rnorm(1) for (i in 2:obs) {e[i] &lt;- rnorm(1)*(1+0.5*(e[i-1])^2)^0.5} # fit the model arch.fit &lt;- garchFit(~garch(1,0), data = e, trace = F) summary(arch.fit) . ",
    "url": "/Time_Series/ARCH_Model.html#r",
    "relUrl": "/Time_Series/ARCH_Model.html#r"
  },"13": {
    "doc": "ARCH Model",
    "title": "Stata",
    "content": "* seed pseudorandom number generator set seed 1 * Simulating a ARCH(1) process set obs 1000 gen time=_n tsset time gen e=. replace e=rnormal() if time==1 replace e=rnormal()*(1 + .5*(e[_n-1])^2)^.5 if time&gt;=2 &amp; time&lt;=2000 * Estimate arch parameters.. arch e, arch(1) . ",
    "url": "/Time_Series/ARCH_Model.html#stata",
    "relUrl": "/Time_Series/ARCH_Model.html#stata"
  },"14": {
    "doc": "ARCH Model",
    "title": "ARCH Model",
    "content": " ",
    "url": "/Time_Series/ARCH_Model.html",
    "relUrl": "/Time_Series/ARCH_Model.html"
  },"15": {
    "doc": "ARMA Models",
    "title": "Autoregressive Moving-Average (ARMA) Models",
    "content": "Auto regressive moving average (ARMA) models are a combination of two commonly used time series processes, the autoregressive (AR) process and the moving-average (MA) process. As such, ARMA models have the form . \\[Y_t = c + \\sum_{i = 1}^{p} \\beta_i Y_{t-i} + \\sum_{j = 1}^{q} \\theta_j \\varepsilon_{t-j} + \\varepsilon_t\\] If an ARMA model has an AR component of order \\(p\\) and an MA component of order \\(q\\), then the model is commonly refered to as an \\(ARMA(p,q)\\) . For additional information, see Wikipedia: Autoregressive Moving-Average model. ",
    "url": "/Time_Series/ARMA-models.html#autoregressive-moving-average-arma-models",
    "relUrl": "/Time_Series/ARMA-models.html#autoregressive-moving-average-arma-models"
  },"16": {
    "doc": "ARMA Models",
    "title": "Keep In Mind",
    "content": ". | Data must be properly formatted for estimation as a time-series. See creating a time series data set. If this is not done, then depending on your statistical package of choice, either your estimation will fail to execute or you will receive erroneous output. | ARMA models include some number of lagged error terms from the MA component, which are inherently unobservable. Consequently these models cannot be estimated using OLS alone, unlike AR models. | ARMA models are most commonly estimated using maximum likelihood estimation (MLE). One consequence of this is that, given some time series and some specified order \\((p,q)\\), the estimates obtained from the estimated \\(ARMA(p,q)\\) model will vary depending on the type of MLE estimation used. | As is the case in many situations where one is trying to estimate a time-series process, model selection is important. For ARMA models, model selection meaning chosing the number of AR and MA parameters, the \\(p\\) and \\(q\\), for which a coefficient will be estimated. In practice, it is common to estimate several different potential models, then use some criterion to determine which model best fits the time-series. Common criteria used to evaluate ARMA models are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC), also referred to as the Schwarz Information Criterion (SIC). For more information on these and other model selection criteria, see Wikipedia: Model Selection. | Estimating a time series using an ARMA model relies on two assumptions. The first is the standard assumption that we have selected the correct functional form for the time series. In this case, that means assuming that we have selected the correct \\(p\\) and \\(q\\). Second, we also have to assume that our time series is stationary. For a discussion of the stationarity assumption and what constraints this assumption imposes on our model, again see Wikipedia: Autoregressive Moving-Average model. | . ",
    "url": "/Time_Series/ARMA-models.html#keep-in-mind",
    "relUrl": "/Time_Series/ARMA-models.html#keep-in-mind"
  },"17": {
    "doc": "ARMA Models",
    "title": "Also Consider",
    "content": ". | ARMA models can only be estimated for univariate time series. If you are interested in estimating a time series process using multiple time series on the right hand side of your model, consider using a vector AR (VAR) model or a VARMA model. | Before estimating an ARMA model, it is standard practice to try to determine whether or not the time series appears to be stationary. See LOST: Stationarity and Weak Dependence for more details. | If the time series you are trying to estimate does not appear to be stationary, then using an ARMA model to estimate the series is innappropriate. For simpler forms of nonstationarity, an ARIMA model may be useful. An \\(ARIMA(p,d,q)\\) model is a more general model for a time-series than an \\(ARMA(p,q)\\). In these models, \\(p\\) still signifies an \\(AR(p)\\) component, and \\(q\\) an \\(MA(q)\\) component. For more information on ARIMA models, see Wikipedia: ARIMA. For information about estimating an ARIMA model, see LOST: ARIMA models | . ",
    "url": "/Time_Series/ARMA-models.html#also-consider",
    "relUrl": "/Time_Series/ARMA-models.html#also-consider"
  },"18": {
    "doc": "ARMA Models",
    "title": "Implementations",
    "content": "First, follow the instructions for creating and formatting time-series data using your software of choice. We will again use quarterly US GDP data downloaded from FRED as an example. This time, though, we will try to estimate the quarterly log change in GDP with an \\(ARMA(3,1)\\) process. Note that an \\(ARMA(3,1)\\) model is almost certainly not the best way to estimate this time series, and is used here solely as an example. ",
    "url": "/Time_Series/ARMA-models.html#implementations",
    "relUrl": "/Time_Series/ARMA-models.html#implementations"
  },"19": {
    "doc": "ARMA Models",
    "title": "Python",
    "content": "The statsmodels library offers a way to fit ARIMA(p, d, q) models, with its ARIMA function. To get an ARMA model, just set \\(d\\) to zero. In the example below, we’ll take the first difference of the log of the data, then fit a model with \\(p=3\\) auto-regressive terms and \\(q=1\\) moving average terms. import numpy as np import pandas as pd from statsmodels.tsa.arima.model import ARIMA gdp = pd.read_csv(\"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv\", index_col=0) # Take 1st diff of log of gdp d_ln_gdp = np.log(gdp).diff() print(d_ln_gdp.head()) . GDPC1 DATE 1947-01-01 NaN 1947-04-01 -0.002670 1947-07-01 -0.002067 1947-10-01 0.015521 1948-01-01 0.014931 . You can see that the first value is NaN. That’s because, for the first value, there is no previous value to do the differencing with. Let’s fit the model: . p = 3 d = 0 q = 1 mod = ARIMA(d_ln_gdp, order=(p, d, q)) res = mod.fit() print(res.summary()) . SARIMAX Results ============================================================================== Dep. Variable: GDPC1 No. Observations: 292 Model: ARIMA(3, 0, 1) Log Likelihood 972.763 Date: 00:00:00 AIC -1933.526 Time: 00:00:00 BIC -1911.466 Sample: 01-01-1947 HQIC -1924.690 - 10-01-2019 Covariance Type: opg ============================================================================== coef std err z P&gt;|z| [0.025 0.975] ------------------------------------------------------------------------------ const 0.0077 0.001 9.163 0.000 0.006 0.009 ar.L1 0.1918 0.417 0.461 0.645 -0.625 1.008 ar.L2 0.1980 0.145 1.368 0.171 -0.086 0.482 ar.L3 -0.0961 0.072 -1.332 0.183 -0.237 0.045 ma.L1 0.1403 0.410 0.342 0.732 -0.663 0.944 sigma2 7.301e-05 4.29e-06 17.006 0.000 6.46e-05 8.14e-05 =================================================================================== Ljung-Box (L1) (Q): 0.02 Jarque-Bera (JB): 59.78 Prob(Q): 0.90 Prob(JB): 0.00 Heteroskedasticity (H): 0.26 Skew: 0.15 Prob(H) (two-sided): 0.00 Kurtosis: 5.20 =================================================================================== Warnings: [1] Covariance matrix calculated using the outer product of gradients (complex-step). ",
    "url": "/Time_Series/ARMA-models.html#python",
    "relUrl": "/Time_Series/ARMA-models.html#python"
  },"20": {
    "doc": "ARMA Models",
    "title": "R",
    "content": "There are numerous packages to estimate ARMA models in R. For this tutorial, we will use the arima() function, which comes preloaded into R from the stats package. For our purposes, it is sufficient to note that estimating an \\(ARIMA(p,0,q)\\) model is largely equivalent to estimating an \\(ARMA(p,q)\\). For more information about estimating a true ARIMA process (where \\(d&gt;0\\)), see the Also Consider section above. Additionally, the tsibble package can also be used to easily construct our quarterly log change in GDP variable. The arima() function does require that we specify the order of the model (ie, pick the values of \\(p\\) and \\(q\\)). For an alternative function that will evaluate multiple models and select the best performing, see the auto.arima function available through the forecast package. ## Load and install time series packages if (!require(\"tsibble\")) install.packages(\"tsibble\") library(tsibble) #load data gdp = read.csv(\"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv\") #set our data up as a time-series gdp$DATE &lt;- as.Date(gdp$DATE) gdp_ts &lt;- as_tsibble(gdp, index = DATE, regular = FALSE) %&gt;% index_by(qtr = ~ yearquarter(.)) #construct our first difference of log gdp variable gdp_ts$lgdp=log(gdp_ts$GDPC1) gdp_ts$ldiffgdp=difference(gdp_ts$lgdp, lag=1, difference=1) #Estimate our ARMA(3,1) ##Note that because we are modeling for the first difference of log GDP, we cannot use our first observation of ##log GDP to estimate our model. arma_gdp = arima(gdp_ts$lgdp[2:292], order=c(3,0,1)) arma_gdp . ",
    "url": "/Time_Series/ARMA-models.html#r",
    "relUrl": "/Time_Series/ARMA-models.html#r"
  },"21": {
    "doc": "ARMA Models",
    "title": "Stata",
    "content": "In Stata we will again estimate an \\(ARMA(p,q)\\) by estimating an \\(ARIMA(p,0,q)\\) using the Stata command arima. This command works similarly to Stata’s reg command. For information about the specific estimation procedure used by this function, optional arguments, etc, see Stata: ARIMA manual . *load data import delimited \"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv\" *Generate the new date variable generate date_index = tq(1947q1) + _n-1 *Index the date variable as quarterly format date_index %tq *Convert a variable into time-series data tsset date_index *construct our first difference of log gdp variable gen lgdp = ln(gdpc1) gen dlgdp = D.lgdp *Specify the ARMA model using the arima command *Stata will automatically drop the first entry, since we do not have a value for the first difference of GDP *for this entry. arima dlgdp, arima(3,0,1) . ",
    "url": "/Time_Series/ARMA-models.html#stata",
    "relUrl": "/Time_Series/ARMA-models.html#stata"
  },"22": {
    "doc": "ARMA Models",
    "title": "ARMA Models",
    "content": " ",
    "url": "/Time_Series/ARMA-models.html",
    "relUrl": "/Time_Series/ARMA-models.html"
  },"23": {
    "doc": "Balance Tables",
    "title": "Balance Tables",
    "content": "Balance Tables are a method by which you can statistically compare differences in characteristics between a treatment and control group. Common in experimental work and when using matching estimators, balance tables show if the treatment and control group are ‘balanced’ and can be seen as similarly ‘identical’ for comparison of a causal effect. ",
    "url": "/Presentation/Tables/Balance_Tables.html",
    "relUrl": "/Presentation/Tables/Balance_Tables.html"
  },"24": {
    "doc": "Balance Tables",
    "title": "Keep in Mind",
    "content": ". | When a characteristic is statistically different between control and treatment, your study is unbalanced in respect to that attribute. | When a characteristic is unbalanced in your study, you may want to consider controlling for that attribute as a variable in your model specification. | Balance tables can only report numeric differences and are not suitable for string value comparisions | . ",
    "url": "/Presentation/Tables/Balance_Tables.html#keep-in-mind",
    "relUrl": "/Presentation/Tables/Balance_Tables.html#keep-in-mind"
  },"25": {
    "doc": "Balance Tables",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Tables/Balance_Tables.html#implementations",
    "relUrl": "/Presentation/Tables/Balance_Tables.html#implementations"
  },"26": {
    "doc": "Balance Tables",
    "title": "R",
    "content": "# Import Dependency library(\"cobalt\") # Load Data data(mtcars) # Create Balance Table bal.tab(am ~ mpg + hp, data = mtcars) . ",
    "url": "/Presentation/Tables/Balance_Tables.html#r",
    "relUrl": "/Presentation/Tables/Balance_Tables.html#r"
  },"27": {
    "doc": "Balance Tables",
    "title": "Stata",
    "content": "* Import Dependency: 'ssc install table1' * Load Data sysuse auto, clear * Create Balance Table * You need to declare the kind of variable for each, as well as the variable by which you define treatment and control. * Adding test gives the statistical difference between the two groups. The ending saves your output as an .xls file table1, by(foreign) vars(price conts \\ mpg conts \\ weight contn \\ length conts) test saving(bal_tab.xls, replace) . Also Consider . The World Bank’s very useful ietoolkit for Stata has a very flexible command for creating balance tables, iebaltab. You can learn more about how to use it on their Wiki page on the command. ",
    "url": "/Presentation/Tables/Balance_Tables.html#stata",
    "relUrl": "/Presentation/Tables/Balance_Tables.html#stata"
  },"28": {
    "doc": "Contributing",
    "title": "HOW TO CONTRIBUTE",
    "content": ". | Get a GitHub account. You do not need to know Git to contribute to LOST, but you do need a GitHub account. | Read the Guide to GitHub Markdown which will show the syntax that is used on LOST pages. | Read the below LOST Writing Guide, which shows what a good LOST page looks like from top to bottom. Even if you are just adding another language to an existing page, be sure to read the Implementations section at the bottom. | Explore LOST using the navigation bar on the left, find a page that needs to be expanded, and add more content. Or find one that doesn’t exist but should (perhaps on the Desired Nonexistent Pages list, and write it yourself! Go to the GitHub Repository for LOST to find the appropriate file to edit or folder to create your new file in. | If you are a “Contributor” to the project, you can make your edits and changes directly to the repository. If not, you will need to issue a pull request to get your work on LOST. We will add you as a contributor after your first accepted pull request. If you don’t know Git or how to do a pull request, please post in Issues asking to be added as a contributor so you can edit LOST directly. | If you’ve made a new page, make sure it’s saved as a .md file, put it in the appropriate folder, and add Navigation Information at the top (see below). If you’ve written a Desired Nonexistent Page, be sure to remove it from the list. Or, if your page links to some new nonexistent pages, add those to the list! Also, try to see if other pages have attempted to link to the page you’re working on, and update their links so they go to the right place. | . ",
    "url": "/Contributing/Contributing.html#how-to-contribute",
    "relUrl": "/Contributing/Contributing.html#how-to-contribute"
  },"29": {
    "doc": "Contributing",
    "title": "LOST WRITING GUIDE",
    "content": "A LOST page is intended to be a set of instructions for performing a statistical technique, where “statistical technique” is broadly defined as “the things you do in statistical software”, which includes everything from loading data to estimating models to cleaning data to visualization to reproducible notebooks. After someone reads a LOST page, they should have a decent idea of: . | How to implement at least a basic version of what they want to do in the language/software they’re using | What common pitfalls there might be in what they’re doing | What are the pros and cons of meaningfully-different ways of doing the same thing, if relevant | Given what they’re doing, what else should they consider doing (for example, if they’re running a regression discontinuity design, you might suggest they also run a test for bunching on either side of the cutoff) | . Things to remember while writing: . | Be as clear as possible. You’re writing a set of instructions, in effect. People should be able to follow them. | The technical ability of the reader may vary by page. People reading a LOST page about how to calculate a mean probably have little experience with their software and will need a lot of hand-holding. You can assume that people reading a LOST page about Markov-Chain Monte Carlo methods probably already have a fairly solid background. | . ",
    "url": "/Contributing/Contributing.html#lost-writing-guide",
    "relUrl": "/Contributing/Contributing.html#lost-writing-guide"
  },"30": {
    "doc": "Contributing",
    "title": "Markdown",
    "content": "LOST pages are written in Markdown. Markdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for . Syntax highlighted code block # Header 1 ## Header 2 ### Header 3 - Bulleted - List 1. Numbered 2. List **Bold** and _Italic_ and `Code` text ![Image](src) . Note that links in LOST are relative links - when linking to another LOST page, don’t use the full URL. Instead of regular-markdown . [Page](url) . instead do, for example: . /Category/page.html . For more details see GitHub Flavored Markdown. ",
    "url": "/Contributing/Contributing.html#markdown",
    "relUrl": "/Contributing/Contributing.html#markdown"
  },"31": {
    "doc": "Contributing",
    "title": "Math",
    "content": "Math is rendered with MathJax, which provides support for \\(\\LaTeX\\) math formatting. To use on a specific page, make sure that the YAML at the top on the underlying Markdown (i.e.md) file includes a line saying mathjax: true. This should already be the default on most existing pages, but it is worth emphasising. For example, here is a screenshot of the “Contributing.md” file that you are reading right now. After that, equations and other math sections can be delimited with two dollar symbol pairs. For example, $$x = \\frac{1}{2}$$ is rendered inline as \\(x = \\frac{1}{2}\\). Similarly, we can render math in display mode (i.e. as a distinct block) by wrapping the dollar symbol pairs on separate lines. For example, . $$ y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\epsilon $$ . is rendered as display math: . \\[y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\epsilon\\] While we don’t include such examples here, note that standard \\(\\LaTeX\\) math environments such as \\begin{equation} ... \\end{equation} (for numbered equations) and \\begin{align} ... \\end{align} (for aligned equation systems) are all supported. Just remember to wrap them between a pair of dollar symbols. More information about MathJax can be found here. ",
    "url": "/Contributing/Contributing.html#math",
    "relUrl": "/Contributing/Contributing.html#math"
  },"32": {
    "doc": "Contributing",
    "title": "STRUCTURE OF A LOST PAGE",
    "content": "When starting a LOST page, you should copy the New Page Template. There are four main sections of a LOST page: . ",
    "url": "/Contributing/Contributing.html#structure-of-a-lost-page",
    "relUrl": "/Contributing/Contributing.html#structure-of-a-lost-page"
  },"33": {
    "doc": "Contributing",
    "title": "Navigation Information",
    "content": "Your page will begin with what’s known as YAML, i.e. something that looks like this: . --- title: Observation level parent: Data Manipulation has_children: false nav_order: 1 mathjax: true --- . You don’t need to worry too much about YAML syntax (here’s the Wikipedia entry for those interested). The important thing is that the YAML provides a set of very basic instructions for the website navigation and page structure. Make sure to fill in the title with a relevant and brief title. Also be sure to put the appropriate name for the parent — this will ensure that your page shows up in the appropriate spot in the navigation structure. Options for parent include: . | Data Manipulation | Geo-Spatial | Machine Learning | Model Estimation | Presentation | Summary Statistics | Time Series | Other | . For the most part, you should generally ignore has_children. (An exception is if you are creating a new section that does have new child pages, but then you are probably better off filing an issue with us to make sure this is done correctly.) You can also ignore nav_order — leaving this at 1 for everything will put everything in alphabetical order. ",
    "url": "/Contributing/Contributing.html#navigation-information",
    "relUrl": "/Contributing/Contributing.html#navigation-information"
  },"34": {
    "doc": "Contributing",
    "title": "Introduction",
    "content": "This is an introduction to the technique. Most of the time this will be just a few sentences about what it is and does, and perhaps why it is used. However, in cases of more niche or complex material, there may be a reason to include more detailed information or general non-language-specific instructions here. In general, however, for more detailed explanations or discussions of statistical properties, you can always just link to an outside trusted source like Wikipedia or a (non-paywalled) academic paper. ",
    "url": "/Contributing/Contributing.html#introduction",
    "relUrl": "/Contributing/Contributing.html#introduction"
  },"35": {
    "doc": "Contributing",
    "title": "Keep in Mind",
    "content": "This is a list of details and reminders for people using the method, especially if they are not yet an expert at it or if the detail is not well-known. This may include: . | Important assumptions that an estimation method makes. | Notes about interpreting the results. | Settings where the technique seems like it might be a good idea, but actually isn’t. | Features of the technique that might surprise users or be unexpected. | Rules of thumb for use (“you will want to set the number of bootstrap samples to at least 1,000 (citation)”) | . ",
    "url": "/Contributing/Contributing.html#keep-in-mind",
    "relUrl": "/Contributing/Contributing.html#keep-in-mind"
  },"36": {
    "doc": "Contributing",
    "title": "Also Consider",
    "content": "This is a list of other techniques that are commonly used in addition to this page’s technique, or as an alternative to this page’s technique. If not obvious, include a very brief explanation of why you might want to use that other technique in addition to/instead of the current one. Note that you can link to another LOST page even if that page doesn’t exist yet. Maybe it will inspite someone to write it! . For example, pages about estimation techniques might list standard robustness tests to be used in addition to the technique, or adjustments to standard errors they might want to use. A page about a data visualization technique might include a link to a page about setting color palettes to be used in addition. Or, they might list an alternative technique that might be used if a certain assumption fails (“This technique requires continuous variables. So if your data is discrete, use this other method.”). To link to other LOST pages (even if they don’t exist yet — don’t forget to add these to Desired Nonexistent Pages!), we ask that you spell the url in a way our markdown renderer understands. Specifically, please write the url as {{ \"/Category_name/page.html\" | relative_url }}. For example, if you’d like to link to this page, please write [Guide to Contributing]({{ \"/Contributing/Contributing.html\" | relative_url}}) which will then be rendered as Guide to Contributing. ",
    "url": "/Contributing/Contributing.html#also-consider",
    "relUrl": "/Contributing/Contributing.html#also-consider"
  },"37": {
    "doc": "Contributing",
    "title": "Implementations",
    "content": "Implementations contains multiple subsections, one for each statistical software environment/programming language that the technique can be implemented in. | Implementations should be listed in alphabetical order of software/language. eViews, then Python, then R, then SAS, then Stata, etc. | For each language, include well-commented and as-brief-as-reasonably-possible example code that provides an example of performing the technique. Readers should be able to copy the code and have it run the technique from beginning to end, including steps like loading in data if necessary. See existing pages for examples. | If someone else on the internet has already written a clear, thorough, and general implementation example, then linking to it is perfectly fine! This includes StackOverflow/StackExchange answers, which you can link to using the share button. Extremely long demonstrations for super-complex methods may be better left as links only (perhaps with a tiny example pulled out and put on LOST). If the example is short enough, though, including the example directly in the LOST page is preferable, with link attribution of the source, so readers don’t have to go elsewhere. | Avoid creating a long list of examples showing every variant or optional setting of the technique. Instead, focus on one main example, with variants included only if they are especially important. If you like, you can mention in comments additional useful options/settings the reader might want to look into and what they do. | If the technique requires that a package or library be installed, include the code for installing the package in a comment (or if you are using a language where libraries cannot be installed inside the code, include a comment directing the user to install the library). | If a given language has multiple ways of performing the same technique, ideally report only one “best” method, whatever that might be. If other methods are only different in trivial ways, then you can describe them as being alternatives, but avoid providing examples for them. If other methods are different in important ways, then include an example for each, with text explanations of what is different about them. If two contributors seriously disagree about which way is best, then they’re probably different in some meaningful way so you can include both as long as you can explain what that difference is. | It is fine to add implementations for software that only has a graphical interface rather than code (such as Excel) using screenshots. Be sure to keep images well-cropped and small so they don’t crowd the page. If your graphical instructions are necessarily very long, consider posting them as a blog post somewhere and just put a link to that post in Implementations. | . Images . Images can be added to Implementation sections if relevant, for example if you’re working with GUI-only software, or demonstrating the output of a data visualization technique. How can you add these images? You can upload the images somewhere on the internet that allows image linking, and include the image in your instructions with ![Image](src). Ideally, upload the image directly to the Images/name-of-your-page/ subfolder of whatever directory you’re working in, and link to the images there. Please be sure to add alt text to images for sight-impaired users. Image filenames should make reference to the language used to make them, i.e. python_scatterplot.png. Data . Ideally, the same data set will be uploaded to LOST directly in a format accessible by many languages (like CSV) in the Data/name-of-your-page/ subfolder of whatever directory you’re wokring in, and then that data can be used for implementation in all languages on the page. This is not required, but is encouraged. ",
    "url": "/Contributing/Contributing.html#implementations",
    "relUrl": "/Contributing/Contributing.html#implementations"
  },"38": {
    "doc": "Contributing",
    "title": "FREQUENTLY ASKED QUESTIONS",
    "content": ". | What techniques are important enough to be their own page? This is a little subjective, but if you’re writing about X, which is a minor option/variant of Y, then you can just include it on the Y page. If X is a different technique or a variant of Y that is used in different circumstances or produces meaningfully different output, then give X its own page. | How should I title my page? Pick a single, concise description of the technique you’re talking about. If there are multiple ways to refer to the technique you’re doing, pick one. You will also need to select a file name, which should be in lower_case_with_underscores.md and you might want to make a bit shorter. So Ordinary Least Squares (Linear Regression) might be the title and H1 heading, and ordinary_least_squares.md might be the file name. | What languages can I include in Implementations? Any language is valid as long as it’s something people actually do statistical analysis in. Don’t include something just because you can (I mean, you can technically do OLS in assembly but is that useful for anyone?), but because you think someone will find it useful. | Should I include the output of my code? For data visualization, yes! Just keep the images relatively small so they don’t crowd the page. See the Implementations section above for how to add images. If your output is not visual, there’s probably no need to include output unless you think that it is especially important for some technique. | How can I discuss what I’m doing with other contributors? Head to the Issues page and find (or post) a thread with the title of the page you’re talking about. | How can I [add an image/link to another LOST page/add an external link/bold text] in the LOST wiki? See the Markdown section above. | I want to contribute but I do not like all the rules and structure on this page. I don’t even want my FAQ entry to be a question. Just let me write what I want. If you have valuable knowledge about statistical techniques to share with people and are able to explain things clearly, I don’t want to stop you. So go for it. Maybe post something in Issues when you’re done and perhaps someone else will help make your page more consistent with the rest of the Wiki. I mean, it would be nicer if you did that yourself, but hey, we all have different strengths, right? | . ",
    "url": "/Contributing/Contributing.html#frequently-asked-questions",
    "relUrl": "/Contributing/Contributing.html#frequently-asked-questions"
  },"39": {
    "doc": "Contributing",
    "title": "Contributing",
    "content": " ",
    "url": "/Contributing/Contributing.html",
    "relUrl": "/Contributing/Contributing.html"
  },"40": {
    "doc": "Figures",
    "title": "Figures",
    "content": " ",
    "url": "/Presentation/Figures/Figures.html",
    "relUrl": "/Presentation/Figures/Figures.html"
  },"41": {
    "doc": "GARCH Model",
    "title": "Generalized Autoregressive Conditional Heteroscedasticity (GARCH) model",
    "content": "Generalized AutoRegressive Conditional Heteroskedasticity (GARCH) is a statistical model used in analyzing time-series data where the variance error is believed to be serially autocorrelated. GARCH models assume that the variance of the error term follows an autoregressive moving average process. GARCH (p, q) model (where p is the order of the GARCH terms $\\sigma^{2}$ and q is the order of the ARCH terms $\\epsilon^{2}$) is a model which $\\epsilon_{t}$, the error terms, can be split into a stochastic piece $z_{t}$ and a time-dependent standard deviation $\\sigma_{t}$ characterizing the typical size of the terms so that $\\epsilon_{t}=\\sigma_{t}z_{t}$. The random variable $z_{t}$ is a strong white noise process while $\\sigma_{t}^{2}$ is an ARMA process, i.e., \\(\\sigma_{t}^{2} = \\alpha_{0} + \\sum_{i=1}^{q}\\alpha_{i}\\epsilon_{t-i}^{2} + \\sum_{i=1}^{p}\\beta_{i}\\sigma_{t-i}^{2}\\). ",
    "url": "/Time_Series/GARCH_Model.html#generalized-autoregressive-conditional-heteroscedasticity-garch-model",
    "relUrl": "/Time_Series/GARCH_Model.html#generalized-autoregressive-conditional-heteroscedasticity-garch-model"
  },"42": {
    "doc": "GARCH Model",
    "title": "Keep in Mind",
    "content": ". | Data should be properly formatted for estimation as a time-series. See creating a time series data set. If not, you may fail to execute or receive erroneous output. | GARCH is appropriate for time series data where the variance of the error term is serially autocorrelated following an autoregressive moving average process. | . ",
    "url": "/Time_Series/GARCH_Model.html#keep-in-mind",
    "relUrl": "/Time_Series/GARCH_Model.html#keep-in-mind"
  },"43": {
    "doc": "GARCH Model",
    "title": "Also Consider",
    "content": ". | GARCH can be used to help predict the volatility of returns on financial assets. | GARCH is useful to assess risk and expected returns for assets that exhibit clustered periods of volatility in returns. | If an autoregressive(AR) model is assumed for the error variance, the model is an autoregressive conditional heteroskedasticity (ARCH) model. For more information on GARCH models, see Wikipedia: ARCH. For information about estimating an ARCH model, see LOST: ARCH models. | . ",
    "url": "/Time_Series/GARCH_Model.html#also-consider",
    "relUrl": "/Time_Series/GARCH_Model.html#also-consider"
  },"44": {
    "doc": "GARCH Model",
    "title": "Implementations",
    "content": " ",
    "url": "/Time_Series/GARCH_Model.html#implementations",
    "relUrl": "/Time_Series/GARCH_Model.html#implementations"
  },"45": {
    "doc": "GARCH Model",
    "title": "Python",
    "content": "# setup from random import gauss from random import seed from matplotlib import pyplot from arch import arch_model import numpy as np # seed the process np.random.seed(1) # Simulating a GARCH(1, 1) process a0 = 0.2 a1 = 0.5 b1 = 0.3 n = 1000 w = np.random.normal(size=n) eps = np.zeros_like(w) sigsq = np.zeros_like(w) for i in range(1, n): sigsq[i] = a0 + a1*(eps[i-1]**2) + b1*sigsq[i-1] eps[i] = w[i] * np.sqrt(sigsq[i]) model = arch_model(eps) model_fit = model.fit() print(model_fit.summary) . ",
    "url": "/Time_Series/GARCH_Model.html#python",
    "relUrl": "/Time_Series/GARCH_Model.html#python"
  },"46": {
    "doc": "GARCH Model",
    "title": "R",
    "content": "# setup library(fGarch) # seed pseudorandom number generator set.seed(1) # Simulating a GARCH(1,1) process a0 &lt;- 0.2 a1 &lt;- 0.5 b1 &lt;- 0.3 obs &lt;- 1000 eps &lt;- rep(0, obs) sigsq &lt;- rep(0,obs) for (i in 2:obs) { sigsq[i] = a0 + a1*(eps[i-1]^2) + b1*sigsq[i-1] eps[i] &lt;- rnorm(1)*sqrt(sigsq[i])} # fit the model garch.fit &lt;- garchFit(~garch(1,1), data = eps, trace = F) summary(garch.fit) . ",
    "url": "/Time_Series/GARCH_Model.html#r",
    "relUrl": "/Time_Series/GARCH_Model.html#r"
  },"47": {
    "doc": "GARCH Model",
    "title": "GARCH Model",
    "content": " ",
    "url": "/Time_Series/GARCH_Model.html",
    "relUrl": "/Time_Series/GARCH_Model.html"
  },"48": {
    "doc": "Generalised Least Squares",
    "title": "Generalised Least Squares",
    "content": " ",
    "url": "/Model_Estimation/GLS/GLS.html",
    "relUrl": "/Model_Estimation/GLS/GLS.html"
  },"49": {
    "doc": "Geo-Spatial",
    "title": "Geo-Spatial",
    "content": " ",
    "url": "/Geo-Spatial/Geo-spatial.html",
    "relUrl": "/Geo-Spatial/Geo-spatial.html"
  },"50": {
    "doc": "Granger Causality",
    "title": "Introduction",
    "content": "Economic theory usually suggests other variables that could help to forecast the variable of interest over than itself. When we add other variables and their lags the result is what is known as The Autoregressive Lag (ADL) Model. For example, if we want to predict future changes in inflation, the theory (Phillips Curve) suggests that lagged values of the unemployment rate might be a good predictor. In particular, the method for indicating when one variable possibly causes a response in another is called the Granger Causality Test. But be careful and do not get confused with the name. The test does not strictly mean that we have estimated the causal effect of one variable on another. It means that the signal of the first one is a useful predictor of the second. A variable $X$ is said to Granger cause another variable $Y$ if $Y$ can be better predicted from the past of $X$ and $Y$ together than the past of $Y$ alone, other relevant information being used in the prediction (Pierce, 1977). ",
    "url": "/Time_Series/Granger_Causality.html#introduction",
    "relUrl": "/Time_Series/Granger_Causality.html#introduction"
  },"51": {
    "doc": "Granger Causality",
    "title": "Keep in Mind",
    "content": ". | Check that both series are stationary. If necessary, transform the data via logarithms or differences. | Estimate the model with lags enough to ensure white noise residuals. | . | When you are choosing the number of lags one variable might affect the other, there is a trade-off between bias and power. To see more click here. | . | Re-estimate both models, including the lags of the other variable. | . \\[Y_t = \\alpha + \\sum_{j=1}^{p} \\beta_j Y_{t-j} + \\sum_{j=1}^r \\theta_j X_{t-j}+ \\epsilon_t\\] \\[X_t = \\alpha^{\\ast} + \\sum_{j=1}^{p} \\beta_j^{\\ast} Y_{t-j} + \\sum_{j=1}^r \\theta_j^{\\ast} X_{t-j}+ \\epsilon_t^{\\ast}\\] . | $X_t$ helps to predict $Y_t$ if $\\theta_j \\neq 0$ for some j . | $Y_t$ helps to predict $X_t$ if $\\theta_j^{\\ast} \\neq 0$ for some j . | . | Use an F test to determine significance of the new variables. Consider the following ADL model: | . \\[H_0: \\theta_j^{\\ast} =0 \\quad \\text{for all} \\quad j = 1,\\dots, r \\quad \\text{by means of F-test}.\\] . | Interpretation: $X$ Granger causes $Y$ if it helps to predict $Y$, whereas $Y$ does not help to predict $X$ | . ",
    "url": "/Time_Series/Granger_Causality.html#keep-in-mind",
    "relUrl": "/Time_Series/Granger_Causality.html#keep-in-mind"
  },"52": {
    "doc": "Granger Causality",
    "title": "Also consider",
    "content": ". | You might also be interested in a Nonparametric Test for Granger Causality. Especially useful to examine a large number of lags, and flexible to find Granger causality in specific regions on the distribution. See more here . | More about ADL model using R. You can visit this online book (Currently in Open Review) . | . ",
    "url": "/Time_Series/Granger_Causality.html#also-consider",
    "relUrl": "/Time_Series/Granger_Causality.html#also-consider"
  },"53": {
    "doc": "Granger Causality",
    "title": "Implementation",
    "content": " ",
    "url": "/Time_Series/Granger_Causality.html#implementation",
    "relUrl": "/Time_Series/Granger_Causality.html#implementation"
  },"54": {
    "doc": "Granger Causality",
    "title": "R",
    "content": "Simulation ADL model . NOTE: Feel free to skip this section if you just interested in how to apply the test. # set seed set.seed(1234) # Simulate error n &lt;- 200 # Sample size rho &lt;- 0.5 # Correlation between Y errors coe &lt;- 1.2 # Coefficient of X in model Y alpha &lt;- 0.5 # Intercept of the model Y # Function to create the error of Y ARsim2 &lt;- function(rho,first,serieslength,distribution){ if(distribution==\"runif\"){a &lt;- runif(serieslength,min=0,max=1)} else {a &lt;- rnorm(serieslength,0,1)} Y &lt;- first for (i in (length(rho)+1):serieslength){ Y[i] &lt;- rho*Y[i-1]+(sqrt(1-(rho^2)))*a[i] } return(Y)} # Error for Y model error&lt;-ARsim2(rho,c(0,0),n,\"rnorm\") # times series X (simulation) X &lt;- arima.sim(list(order = c(1, 0, 0), ar = c(0.2)), n) # times series Y (simulation) Y &lt;- NULL for(i in 2:200){ Y[i] &lt;- alpha + (coe*X[i-1]) + error[i] } . Data . data &lt;- as.data.frame(cbind(1:200,X,as.ts(Y))) colnames(data) &lt;- c(\"time\", \"X\",\"Y\") . Graph . # If necessary # install.packages(\"tidyr\") # install.packages(\"ggplot2\") library(tidyr) # For data manipulation library(ggplot2) # Plots graphdata &lt;- data[2:200,] %&gt;% pivot_longer( cols = -c(time), names_to=\"variable\", values_to=\"value\" ) ggplot(graphdata, aes(x = time, y = value, group=variable)) + geom_line(aes(color = variable), size = 0.7) + scale_color_manual(values = c(\"#00AFBB\", \"#E7B800\")) + theme_minimal()+ labs(title = \"Simulated ADL models\")+ theme(text = element_text(size = 15)) . | It seems that both series are stationary (later is check with the ADF test) and, | Disturbances in variable X are visible after periods in Y (as expected). | . Check Stationarity . # If necessary # install.packages(\"tseries\") library(tseries) ## ADF test adf.test(X, k=3) adf.test(na.omit(Y), k=3) #na.omit() to delete the first 2 periods of lag . | With a p-value of 0.01 and 0.01 for series X, and Y, we assure that both are stationary. | No transformation needed for the series. | . Granger Test . Note: grangertest() only performs tests for Granger causality in bivariate series. Step 1. $(Y\\sim X)$ . # If neccesary # install.packages(\"lmtest\") library(lmtest) grangertest(Y ~ X, order = 2, data = data) . ## Granger causality test ## ## Model 1: Y ~ Lags(Y, 1:2) + Lags(X, 1:2) ## Model 2: Y ~ Lags(Y, 1:2) ## Res.Df Df F Pr(&gt;F) ## 1 192 ## 2 194 -2 198.42 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 . Step 2. $(X \\sim Y)$ . grangertest(X ~ Y, order = 2, data = data) . ## Granger causality test ## ## Model 1: X ~ Lags(X, 1:2) + Lags(Y, 1:2) ## Model 2: X ~ Lags(X, 1:2) ## Res.Df Df F Pr(&gt;F) ## 1 192 ## 2 194 -2 1.2028 0.3026 . | We see that the effect of lags of number of X is highly significant, and conclude that X predicts the future of Y. | The null hypothesis is not rejected for the converse relationship. Thus, we conclude that $X$ is Granger causal for $Y$ . | . References . Granger, C. W. (1969). Investigating Causal Relations by Econometric Models and Cross-Spectral Methods. Econometrica, 37(3), 424–438. Pierce, D.A. (1977). $R^2$ Measures for Time Series. Special Studies Paper No. 93, Washington, D.C.: Federal Reserve Board. ",
    "url": "/Time_Series/Granger_Causality.html#r",
    "relUrl": "/Time_Series/Granger_Causality.html#r"
  },"55": {
    "doc": "Granger Causality",
    "title": "Granger Causality",
    "content": " ",
    "url": "/Time_Series/Granger_Causality.html",
    "relUrl": "/Time_Series/Granger_Causality.html"
  },"56": {
    "doc": "Machine Learning",
    "title": "Machine Learning",
    "content": " ",
    "url": "/Machine_Learning/Machine_Learning.html",
    "relUrl": "/Machine_Learning/Machine_Learning.html"
  },"57": {
    "doc": "Model Estimation",
    "title": "Model Estimation",
    "content": " ",
    "url": "/Model_Estimation/Model_Estimation.html",
    "relUrl": "/Model_Estimation/Model_Estimation.html"
  },"58": {
    "doc": "Multilevel Models",
    "title": "Multilevel Models",
    "content": " ",
    "url": "/Model_Estimation/Multilevel_Models/Multilevel_Models.html",
    "relUrl": "/Model_Estimation/Multilevel_Models/Multilevel_Models.html"
  },"59": {
    "doc": "Title of page",
    "title": "Name of Page",
    "content": "INTRODUCTION SECTION . Remember that you can use inline math, e.g. \\(x + y\\). In general, you should render variables in math mode (\\(X\\), \\(Y\\), etc.) . You can also render math in display mode: . \\[\\int_a^b f(x)dx\\] ",
    "url": "/NewPageTemplate.html#name-of-page",
    "relUrl": "/NewPageTemplate.html#name-of-page"
  },"60": {
    "doc": "Title of page",
    "title": "Keep in Mind",
    "content": ". | LIST OF IMPORTANT THINGS TO REMEMBER ABOUT USING THE TECHNIQUE | . ",
    "url": "/NewPageTemplate.html#keep-in-mind",
    "relUrl": "/NewPageTemplate.html#keep-in-mind"
  },"61": {
    "doc": "Title of page",
    "title": "Also Consider",
    "content": ". | LIST OF OTHER TECHNIQUES THAT WILL COMMONLY BE USED ALONGSIDE THIS PAGE’S TECHNIQUE | (E.G. LINEAR REGRESSION LINKS TO ROBUST STANDARD ERRORS), | OR INSTEAD OF THIS TECHNIQUE | (E.G. PIE CHART LINKS TO A BAR PLOT AS AN ALTERNATIVE) | WITH EXPLANATION | INCLUDE LINKS TO OTHER LOST PAGES WITH THE FORMAT Description. Categories include Data_Manipulation, Geo-Spatial, Machine_Learning, Model_Estimation, Presentation, Summary_Statistics, Time_Series, and Other | . ",
    "url": "/NewPageTemplate.html#also-consider",
    "relUrl": "/NewPageTemplate.html#also-consider"
  },"62": {
    "doc": "Title of page",
    "title": "Implementations",
    "content": " ",
    "url": "/NewPageTemplate.html#implementations",
    "relUrl": "/NewPageTemplate.html#implementations"
  },"63": {
    "doc": "Title of page",
    "title": "NAME OF LANGUAGE/SOFTWARE 1",
    "content": "```identifier for language type, see this page: https://github.com/jmm/gfm-lang-ids/wiki/GitHub-Flavored-Markdown-%28GFM%29-language-IDs Commented code demonstrating the technique . ## NAME OF LANGUAGE/SOFTWARE 2 WHICH HAS MULTIPLE APPROACHES There are two ways to perform this technique in language/software 2. First, explanation of what is different about the first way: ```identifier for language type, see this page: https://github.com/jmm/gfm-lang-ids/wiki/GitHub-Flavored-Markdown-%28GFM%29-language-IDs Commented code demonstrating the technique . Second, explanation of what is different about the second way: . identifier for language type, see this page: https://github.com/jmm/gfm-lang-ids/wiki/GitHub-Flavored-Markdown-%28GFM%29-language-IDs Commented code demonstrating the technique . ",
    "url": "/NewPageTemplate.html#name-of-languagesoftware-1",
    "relUrl": "/NewPageTemplate.html#name-of-languagesoftware-1"
  },"64": {
    "doc": "Title of page",
    "title": "Title of page",
    "content": " ",
    "url": "/NewPageTemplate.html",
    "relUrl": "/NewPageTemplate.html"
  },"65": {
    "doc": "Ordinary Least Squares",
    "title": "Ordinary Least Squares",
    "content": " ",
    "url": "/Model_Estimation/OLS/OLS.html",
    "relUrl": "/Model_Estimation/OLS/OLS.html"
  },"66": {
    "doc": "Other",
    "title": "Other",
    "content": " ",
    "url": "/Other/Other.html",
    "relUrl": "/Other/Other.html"
  },"67": {
    "doc": "Presentation",
    "title": "Presentation",
    "content": " ",
    "url": "/Presentation/Presentation.html",
    "relUrl": "/Presentation/Presentation.html"
  },"68": {
    "doc": "Regression Tables",
    "title": "Regression Tables",
    "content": "Statistical packages often report regression results in a way that is not how you would want to display them in a paper or on a website. Additionally, they rarely provide an option to display multiple regression results in the same table. Two (bad) options for including regression results in your paper include copying over each desied number by hand, or taking a screenshot of your regression output. Much better is using a command that outputs regression results in a nice format, in a way you can include in your presentation. ",
    "url": "/Presentation/Tables/Regression_Tables.html",
    "relUrl": "/Presentation/Tables/Regression_Tables.html"
  },"69": {
    "doc": "Regression Tables",
    "title": "Keep in Mind",
    "content": ". | Any good regression table exporting command should include an option to limit the number of significant digits in your result. You should almost always make use of this option. It is very rare that the seventh or eighth decimal place (commonly reported in statistics packages) is actually meaningful, and it makes it difficult to read your table. | Variable names serve different purposes in statistical coding and in papers. Variable names in papers should be changed to be readable in the language of the paper. So for example, while employment may be recorded as EMP_STAT in your statistics package, you should rename it Employment for your paper. Most table exporting commands include options to perform this renaming. But if it doesn’t, you can always change it by hand after exporting. | If you use asterisks to indicate significance, be sure to check the significance levels that different numbers of asterisks indicate in the command you’re using, as standards for what significance levels the asterisks mean vary across fields (and so vary across commands as well). Most commands include an option to change the significance levels used. On that note, always include a table note saying what the different asterisk indicators mean! These commands should all include one by default - don’t take it out! | . ",
    "url": "/Presentation/Tables/Regression_Tables.html#keep-in-mind",
    "relUrl": "/Presentation/Tables/Regression_Tables.html#keep-in-mind"
  },"70": {
    "doc": "Regression Tables",
    "title": "Also Consider",
    "content": ". | If you are a Word user, and the command you are using does not export to Word or RTF, you can get the table into Word by exporting an HTML, CSV, or LaTeX, then opening up the result in your browser, Excel, or TtH, respectively. Excel and HTML tables can generally be copy/pasted directly into Word (and then formatted within Word). You may at that point want to use Word’s “Convert Text to Table” command, especially if you’ve pasted in HTML. | By necessity, regression-output commands often have about ten million options, and they can’t all be covered on this page. If you want it to do something, it probably can. To reduce errors, it is probably a good idea to do as little formatting and copy/pasting by hand as possible. So if you want to do something it doesn’t do by default, like adding additional calculations, check out the help file for your command to see how you can do it automatically. | . ",
    "url": "/Presentation/Tables/Regression_Tables.html#also-consider",
    "relUrl": "/Presentation/Tables/Regression_Tables.html#also-consider"
  },"71": {
    "doc": "Regression Tables",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Tables/Regression_Tables.html#implementations",
    "relUrl": "/Presentation/Tables/Regression_Tables.html#implementations"
  },"72": {
    "doc": "Regression Tables",
    "title": "R",
    "content": "There are many, many packages for exporting regression results in R, including RStudio’s gt, texreg, and xtable. Here we will focus on two: stargazer, which is probably the easiest to use, and huxtable, which is slightly more up-to-date and offers advanced formatting options, outlined on its website. # Install stargazer if necessary # install.packages('stargazer') library(stargazer) # Get mtcars data data(mtcars) # Let's give it two regressions to output lm1 &lt;- lm(mpg ~ cyl, data = mtcars) lm2 &lt;- lm(mpg ~ cyl + hp, data = mtcars) # Let's output an HTML table, perhaps for pasting into Word # We could instead set type = 'latex' for LaTeX or type = 'text' for a text-only table. stargazer(lm1, lm2, type = 'html', out = 'my_reg_table.html') # In line with good practices, we should use readable names for our variables stargazer(lm1, lm2, type = 'html', out = 'my_reg_table.html', covariate.labels = c('Cylinders','Horsepower'), dep.var.labels = 'Miles per Gallon') . This produces: . | | (1) | (2) | . | Cylinders | -2.876 *** | -2.265 *** | . | | (0.322)&nbsp;&nbsp;&nbsp; | (0.576)&nbsp;&nbsp;&nbsp; | . | Horsepower | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | -0.019&nbsp;&nbsp;&nbsp;&nbsp; | . | | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | (0.015)&nbsp;&nbsp;&nbsp; | . | N | 32&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | 32&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | . | R2 | 0.726&nbsp;&nbsp;&nbsp;&nbsp; | 0.741&nbsp;&nbsp;&nbsp;&nbsp; | . | logLik | -81.653&nbsp;&nbsp;&nbsp;&nbsp; | -80.781&nbsp;&nbsp;&nbsp;&nbsp; | . | AIC | 169.306&nbsp;&nbsp;&nbsp;&nbsp; | 169.562&nbsp;&nbsp;&nbsp;&nbsp; | . | *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. | . Now we will do the same thing with huxtable, using mostly defaults. # Install huxtable and magrittr if necessary # install.packages('huxtable', 'magrittr') # huxtable works more easily with the pipe %&gt;% # which can come from magrittr or dplyr or tidyverse, etc. library(huxtable) library(magrittr) # First we build a huxreg object, using readable names huxreg(lm1, lm2, coefs=c('Cylinders' = 'cyl', 'Horsepower' = 'hp')) %&gt;% # We can send it to the screen to view it instantly print_screen() # Or we can send it to a file with the quick_ functions, which can # output to pdf, docx, html, xlsx, pptx, rtf, or latex. huxreg(lm1, lm2, coefs=c('Cylinders' = 'cyl', 'Horsepower' = 'hp')) %&gt;% # Let's make an HTML file quick_html(file = 'my_reg_output.html') . Which produces (note the different asterisks behavior, which can be changed with huxreg’s stars option): . | . | | Dependent variable: | . | | . | | Miles per Gallon | . | | (1) | (2) | . | . | Cylinders | -2.876*** | -2.265*** | . | | (0.322) | (0.576) | . | | | . | Horsepower | | -0.019 | . | | | (0.015) | . | | | . | Constant | 37.885*** | 36.908*** | . | | (2.074) | (2.191) | . | | | . | . | Observations | 32 | 32 | . | R2 | 0.726 | 0.741 | . | Adjusted R2 | 0.717 | 0.723 | . | Residual Std. Error | 3.206 (df = 30) | 3.173 (df = 29) | . | F Statistic | 79.561*** (df = 1; 30) | 41.422*** (df = 2; 29) | . | . | Note: | *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 | . ",
    "url": "/Presentation/Tables/Regression_Tables.html#r",
    "relUrl": "/Presentation/Tables/Regression_Tables.html#r"
  },"73": {
    "doc": "Regression Tables",
    "title": "Stata",
    "content": "There are two main ways of outputting regression results in Stata, both of which must be installed from ssc install: outreg2 and estout. We will use estout here, as it is more flexible. More detail is available on the estout website. Also note that, in a pinch, if you’re using a strange command that does not play nicely with estout, you can often select any Stata regression output, select the output, right-click, do “Copy Table”, and paste the result into Excel. This is only if all else fails. * Install estout if necessary * ssc install estout * Load auto data sysuse auto.dta, clear * Let's provide it two regressions * Making sure to store the results each time reg mpg weight estimates store weightonly reg mpg weight foreign estimates store weightandforeign * Now let's export the table using estout * while renaming the variables for readability using the variable labels already in Stata * replacing any table we've already made * and making an HTML table with style(html) * style(tex) also works, and the default is tab-delimited data for use in Excel. * Note also the default is to display t-statistics in parentheses. If we want * standard errors instead, we say so with se esttab weightonly weightandforeign using my_reg_output.html, label replace style(html) se . Which produces: . | . | . | | (1) | (2) | . | | Mileage (mpg) | Mileage (mpg) | . | . | . | Weight (lbs.) | -0.00601*** | -0.00659*** | . | | (0.000518) | (0.000637) | . | &nbsp; | . | Car type | | -1.650 | . | | | (1.076) | . | &nbsp; | . | Constant | 39.44*** | 41.68*** | . | | (1.614) | (2.166) | . | . | . | Observations | 74 | 74 | . | . | . | Standard errors in parentheses* p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001 | . ",
    "url": "/Presentation/Tables/Regression_Tables.html#stata",
    "relUrl": "/Presentation/Tables/Regression_Tables.html#stata"
  },"74": {
    "doc": "Research Design",
    "title": "Research Design",
    "content": " ",
    "url": "/Model_Estimation/Research_Design/Research_Design.html",
    "relUrl": "/Model_Estimation/Research_Design/Research_Design.html"
  },"75": {
    "doc": "Scatterplots",
    "title": "Introduction",
    "content": "Scatterplots are a useful tool for visualizing data and the possible relationships present in that data. This introduction contains a brief tutorial on how to implement scatterplots, as well as some basic techniques for formatting them. ",
    "url": "/Presentation/Figures/Scatterplots.html#introduction",
    "relUrl": "/Presentation/Figures/Scatterplots.html#introduction"
  },"76": {
    "doc": "Scatterplots",
    "title": "Keep in Mind",
    "content": ". | Scatterplots can become cluttered if there are too many datapoints or if datapoints are too large. | . ",
    "url": "/Presentation/Figures/Scatterplots.html#keep-in-mind",
    "relUrl": "/Presentation/Figures/Scatterplots.html#keep-in-mind"
  },"77": {
    "doc": "Scatterplots",
    "title": "Also Consider",
    "content": ". | Scatterplots are not always the best presentation method for data. | If the independent variable that is being presented is categorical or discrete, a bar graph might be a better presentation method. A guide to implementing bar graphs can be found here. | If the goal is to represent distributions of continuous variables, a histogram would be a good option for presentation. | . | Additional techniques for formatting scatterplots can by found here. | . ",
    "url": "/Presentation/Figures/Scatterplots.html#also-consider",
    "relUrl": "/Presentation/Figures/Scatterplots.html#also-consider"
  },"78": {
    "doc": "Scatterplots",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Figures/Scatterplots.html#implementations",
    "relUrl": "/Presentation/Figures/Scatterplots.html#implementations"
  },"79": {
    "doc": "Scatterplots",
    "title": "Python",
    "content": "There are many plotting libraries in Python, covering both imperative (specify all of the steps to get the desired outcome) and declarative (specify the desired outcome without the steps) approaches. Imperative plotting gives more control and some people may find each step clearer to read, but it can also be fiddly and cumbersome, especially with simple plots. Declarative plotting trades away control in favour of tried and tested processes that can quickly produce standardised charts, but the specialised syntax can be a barrier for newcomers. The code below shows examples of scatterplots using both methods using the declarative library seaborn, drawing from the packages’ website. For imperative scatter plots, use matplotlib. As usual with Python, you may need to install seaborn using pip install seaborn or conda install seaborn on the command line. import seaborn as sns # Load the tips dataset df = sns.load_dataset(\"tips\") # Plot the data. hue sets the colour of points. # alpha sets the transparency of points. There # are various other keyword arguments to add other # dimensions of information too, eg size. sns.scatterplot(data=df, x=\"total_bill\", y=\"tip\", alpha=.8, hue='time').set_title('Tips data', loc='right') . This results in: . ",
    "url": "/Presentation/Figures/Scatterplots.html#python",
    "relUrl": "/Presentation/Figures/Scatterplots.html#python"
  },"80": {
    "doc": "Scatterplots",
    "title": "R",
    "content": "In R, one of the best tools for creating scatterplots is the function ggplot(), found in the ggplot2 package. For this demonstration, we will also be using a dataset already built in to R called mtcars. To begin we will need to make sure we install and load ggplot2 as well as any other packages that are useful. #install and load necessary packages if(!require(\"pacman\")) install.packages(\"pacman\") pacman::p_load(ggplot2) #load the dataset data(mtcars) . Next, we will use ggplot(), aes(), and geom_point() in order to create a basic scatterplot. For this plot, we will put car weight on the x-axis and miles-per-gallon on the y-axis. #assign the mtcars dataset to the plot and set each axis ggplot(data = mtcars,aes(x=wt,y=mpg)) + #create points on the plot for each observation geom_point() . It is important to remember to include the + after each line when creating a plot using ggplot(). This + tells R that the lines of code belong together and omitting it will lead to our plot not having important parts. Labelling is also an important task. In order to give our scatterplot axis labels and title, we will use the labs() function, in conjunction with our previous code. Don’t forget your +’s! . #assign our dataset and variables of interest to the plot ggplot(data = mtcars, aes(x = wt, y = mpg)) + #create the points geom_point() + #create axis labels and a title labs(x = \"Weight\", y = \"Miles Per Gallon\", title = \"Car MPG by Weight\") . That is starting to look better, but our graph could still use a little variety to it. Next, we will learn how to change the size and color of our plot points. | To change the size of the points in our scatterplot, we need to use the option size. The default size of points in ggplot is 1.5. We’re going to make the points 4, just in case someone is having trouble seeing them. | To change the color of our points, we will use color. In this example we will make our points blue. | . #assign our dataset and variables of interest ggplot(data = mtcars, aes(x =wt, y = mpg)) + #create points and tell ggplot we want them to be size 4 and blue geom_point(size = 4, color = blue) + #don't forget the labels labs(x = \"Weight\", y = \"Miles Per Gallon\", title = \"Car MPG by Weight\") . Finally, lets label our points. We can do this by adding a new element to our plot, geom_text(). For this example we will label the points on our plot with their horse power. This will allow us to see how horsepower is related to weight and miles-per-gallon. We are also going to set the size of our points to 0.5 to avoid cluttering the scatterplot too much. Just like we can change the color of our points, we can change the color of the labels we put on them. We’ll make them red in this example, but feel free to choose another color. #assign our dataset and variables of interest ggplot(data = mtcars, aes(x =wt, y = mpg)) + #create points and tell ggplot we want them to be size 0.5 and blue geom_point(size = 0.5, color = 'blue') + #add the labels for our points geom_text(label = mtcars$hp, color = 'red') #don't forget the labels labs(x = \"Weight\", y = \"Miles Per Gallon\", title = \"Car MPG by Weight\") . Congrats!!! You’re well on your way to becoming a scatterplot master! Don’t forget to check out the LOST page on styling scatterplots if you would like to learn more. ",
    "url": "/Presentation/Figures/Scatterplots.html#r",
    "relUrl": "/Presentation/Figures/Scatterplots.html#r"
  },"81": {
    "doc": "Scatterplots",
    "title": "Scatterplots",
    "content": " ",
    "url": "/Presentation/Figures/Scatterplots.html",
    "relUrl": "/Presentation/Figures/Scatterplots.html"
  },"82": {
    "doc": "Statistical Inference",
    "title": "Statistical Inference",
    "content": " ",
    "url": "/Model_Estimation/Statistical_Inference/Statistical_Inference.html",
    "relUrl": "/Model_Estimation/Statistical_Inference/Statistical_Inference.html"
  },"83": {
    "doc": "Styling Scatterplots",
    "title": "Introduction",
    "content": "A scatterplot is a useful and straightforward way to visualize the relationship between two variables,eventually revealing a correlation. It is often used to make initial diagnoses before any other statistical analyses are conducted.This tutorial will not only teach you how to make scatterplots, but also explore the ways to help you design your own styling scatterplots. ",
    "url": "/Presentation/Figures/Styling_Scatterplots.html#introduction",
    "relUrl": "/Presentation/Figures/Styling_Scatterplots.html#introduction"
  },"84": {
    "doc": "Styling Scatterplots",
    "title": "Keep in Mind",
    "content": ". | REMEMBER always clean your dataset before you try to make scatterplots since in the real world, the dataset is always messier than the iris dataset used below. | Scatterplots may not work well if the variables that you are interested in are discrete, or if there are a large number of data points. | Be more careful if you have Date (which is time-series data) as your x-variable, Date can be very tricky in many ways. | . ",
    "url": "/Presentation/Figures/Styling_Scatterplots.html#keep-in-mind",
    "relUrl": "/Presentation/Figures/Styling_Scatterplots.html#keep-in-mind"
  },"85": {
    "doc": "Styling Scatterplots",
    "title": "Also Consider",
    "content": ". | If one of your variables is discrete, then instead of scatterplots, you may want to check how to make bar graphs here. | . Specifically in R: . | Formatting graph legends is important for styling scatterplots. So check here if you want to work with graph legends. | If you are working with time series visualization with ggplot2 package, see here for more help. | Check here for more data visualization with ggplot2 package. | . ",
    "url": "/Presentation/Figures/Styling_Scatterplots.html#also-consider",
    "relUrl": "/Presentation/Figures/Styling_Scatterplots.html#also-consider"
  },"86": {
    "doc": "Styling Scatterplots",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Figures/Styling_Scatterplots.html#implementations",
    "relUrl": "/Presentation/Figures/Styling_Scatterplots.html#implementations"
  },"87": {
    "doc": "Styling Scatterplots",
    "title": "R",
    "content": "For this R demonstration, we will introduce how to use ggplot2 package to create nice scatterplots. | Before we create the scatterplots, we need to make sure that we install and library all the packages we need. Using the function p_load() in the pacman package is able to allow us to install and library all the packages we need at once time. | . if (!require(\"pacman\")) install.packages(\"pacman\") pacman::p_load(ggplot2,viridis,dplyr, RColorBrewer,tidyverse,ggthemes,ggpubr) . Step 1: Basic Scatterplot . Let’s start with the basic scatterplot. Say we want to check the relationship between Sepal width and Sepal length of the iris species. There are a few steps to construct the scatterplot: . | Step1: specify the dataset that we want to visualize | Step2: tell which variable to show on x and y axis | Step3: add a geom_point() in order to show the points | . If you have questions about how to use ggplot and aes, check Here for more help. ggplot(data = iris, aes( ## Put Sepal.Length on the x-axis, Sepal.Width on the y-axis x=Sepal.Length, y=Sepal.Width))+ ## Make it a scatterplot with geom_point() geom_point() . Step 2: Map a variable to marker feature . One of the most powerful and magic abilities of the ggplot2 package is to map a variable to marker features. Notice that attributes set outside of aes() apply to all points (like size=4 here), while attributes set inside of aes() set the attribute separately for the values of the variable. Transparency . We can distinguish the Species by alpha (transparency). ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, ## Where transparency comes in alpha=Species)) + geom_point(size =4, color=\"seagreen\") . Shape . shape is also a common way to help us to see relationship between two variables within different groups. Additionally, you can always change the shape of the points. Check here for more ideas. ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, ## Where shape comes in shape=Species)) + geom_point(size = 4,color=\"orange\") . Size . size is a great option that we can take a look at as well. However, note that size will work better with continuous variables. ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, ## Where size comes in size=Species)) + geom_point(shape = 18, color = \"#FC4E07\") . Color . Last but not least, let’s color these points depends on the variable Species in the iris dataset. ## First, we need to make sure that 'Species' is a factor variable ## class(iris$Species) ## Since 'Species' is already a factor variable, we do not need to do conversion ## However, in case 'Species' is not a factor variable, we can solve this question using as.factor() function, like below ## iris$Species &lt;- as.factor(iris$Species) ## Then, we are ready to plot ggplot(data = iris, aes(x=Sepal.Length, y=Sepal.Width, ## distinguish the species by color color=Species))+ geom_point() . | Note . | If you do not like the default colors in the ggplot2, there are a couple of ways to change that.The RColorBrewerpackage will definitely help. If you want to know more about RColorBrewer package,see here. Additionally,the viridis package is also very helpful to change the default colors. For more information of the viridis package, check here. | If you do not like all the options that the RColorBrewer and viridis packages provide, see here to work with color in the ggplot2 package. | . | . ggplot(data = iris, aes(x=Sepal.Length, y=Sepal.Width, color=Species))+ geom_point()+ ## Where RColorBrewer package comes in scale_colour_brewer(palette = \"Set1\") ## There are more options available for palette ggplot(data = iris, aes(x=Sepal.Length, y=Sepal.Width, color=Species))+ geom_point()+ ## Where viridis package comes in scale_color_viridis(discrete=TRUE,option = \"D\") ## There are more options to choose . This first graph is using RColorBrewer package,and the second graph is using viridis package. Put all the options together . Of course, we can always mix color,transparency,shape and size together to get prettier plot. Simply set more than one of them in aes()! . Step 3: Find the comfortable themes . The next step that we can do is to figure out what the most fittable themes to match all the hard work we have done above. Themes from **ggplot2 package** . In fact, ggplot2 package has many cool themes available alreay such as theme_classic(), theme_minimal() and theme_bw(). Another famous theme is the dark theme: theme_dark(). Let’s check out some of them. ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, col=Species, shape=Species)) + geom_point(size=3) + scale_color_viridis(discrete=TRUE,option = \"D\") + theme_minimal(base_size = 12) . Themes from the ggthemes package . ggthemes package is also worth to check out for working any plots (maps,time-series data, and any other plots) that you are working on. theme_gdocs(), theme_tufte(), and theme_calc() all work very well. See here to get more cool themes. ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, col=Species, shape=Species)) + geom_point(size=3) + scale_color_viridis(discrete=TRUE,option = \"D\") + ## Using the theme_tufte() theme_tufte() . Create by your own . If you do not like themes that ggplot2 and ggthemes packages have, don’t worry. You can always create your own style for your themes. Check here to desgin your own unique style. Step 4: Play with labels . It is time to label all the useful information to make the plot be clear to your audiences. Basic Labelling . Both labs() and ggtitle() are great tools to deal with labelling information. In the following code, we provide the example how to use labs() to label the all the things that we need. Take a look here if you want to learn how to use ggtitle(). ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, col=Species, shape=Species)) + geom_point(size=3) + scale_color_viridis(discrete=TRUE,option = \"D\") + theme_minimal(base_size = 12)+ ## Where the labelling comes in labs( ## Tell people what x and y variables are x=\"Sepal Length\", y=\"Sepal Width\", ## Title of the plot title = \"Sepal length vs. Sepal width\", subtitle = \" plot within different Iris Species\" ) . Postion and Appearance . After the basic labelling, we want to make them nicer by playing around the postion and appearance (text size, color and faces). ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, col=Species, shape=Species)) + geom_point(size=3) + scale_color_viridis(discrete=TRUE,option = \"D\") + labs( x=\"Sepal Length\", y=\"Sepal Width\", title = \"Sepal length vs. Sepal width\", subtitle = \"plot within different Iris Species\" )+ theme_minimal(base_size = 12) + ## Change the title and subtitle position to the center theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))+ ## Change the appearance of the title and subtitle theme (plot.title = element_text(color = \"black\", size = 14, face = \"bold\"), plot.subtitle = element_text(color = \"grey40\",size = 10, face = 'italic') ) . Step 5: Show some patterns . After done with step 4, you should end with a very neat and unquie plot. Let’s end up with this tutorial by checking whether there are some specific patterns in our dataset. Linear Trend . According to the plot, it seems like there exists a linear relationship between sepal length and sepal width. Thus, let’s add a linear trend to our scattplot to help readers see the pattern more directly using geom_smooth(). Note that the method argument in geom_smooth() allows to apply different smoothing method like glm, loess and more. See the doc for more. ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, col=Species, shape=Species)) + geom_point(size=3) + scale_color_viridis(discrete=TRUE,option = \"D\") + labs( x=\"Sepal Length\", y=\"Sepal Width\", title = \"Sepal length vs. Sepal width\", subtitle = \"plot within different Iris Species\" )+ theme_minimal(base_size = 12) + theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))+ theme (plot.title = element_text(color = \"black\", size = 14, face = \"bold\"), plot.subtitle = element_text(color = \"grey40\",size = 10, face = 'italic')) + ## Where linear trend + confidence interval come in geom_smooth(method = 'lm',se=TRUE) . Congratulations!!! You just make your own style of scatterplots if you are following all the steps above and try to play around the different options. ",
    "url": "/Presentation/Figures/Styling_Scatterplots.html#r",
    "relUrl": "/Presentation/Figures/Styling_Scatterplots.html#r"
  },"88": {
    "doc": "Styling Scatterplots",
    "title": "Styling Scatterplots",
    "content": " ",
    "url": "/Presentation/Figures/Styling_Scatterplots.html",
    "relUrl": "/Presentation/Figures/Styling_Scatterplots.html"
  },"89": {
    "doc": "Summary Statistics Tables",
    "title": "Summary Statistics Tables",
    "content": "Before looking at relationships between variables, it is generally a good idea to show a reader what the distributions of individual variables look like. A common way to do this, which allows you to show information about many variables at once, is a “Summary statistics table” or “descriptive statistics table” in which each row is one variable in your data, and the columns include things like number of observations, mean, median, standard deviation, and range. ",
    "url": "/Presentation/Tables/Summary_Statistics_Tables.html",
    "relUrl": "/Presentation/Tables/Summary_Statistics_Tables.html"
  },"90": {
    "doc": "Summary Statistics Tables",
    "title": "Keep in Mind",
    "content": ". | Make sure that you are using the appropriate summary measures for the variables that you have. For example, if you have a variable indicating the country someone is from coded as that country’s international calling code, don’t include it in a table that reports the mean - you’d get an answer but that answer wouldn’t make any sense. | If you have categorical variables, you can generally still incorporate them into a summary statistics table by turning them into binary “dummy” variables. | . ",
    "url": "/Presentation/Tables/Summary_Statistics_Tables.html#keep-in-mind",
    "relUrl": "/Presentation/Tables/Summary_Statistics_Tables.html#keep-in-mind"
  },"91": {
    "doc": "Summary Statistics Tables",
    "title": "Also Consider",
    "content": ". | Graphs can be more informative ways of showing the distribution of a variable, and you may want to show a graph of your variable’s distribution in addition to its inclusion on a summary statistics table. There are many ways to do this, but two common ones are density plots or histograms for continuous variables, or bar plots for categorical variables. | . ",
    "url": "/Presentation/Tables/Summary_Statistics_Tables.html#also-consider",
    "relUrl": "/Presentation/Tables/Summary_Statistics_Tables.html#also-consider"
  },"92": {
    "doc": "Summary Statistics Tables",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Tables/Summary_Statistics_Tables.html#implementations",
    "relUrl": "/Presentation/Tables/Summary_Statistics_Tables.html#implementations"
  },"93": {
    "doc": "Summary Statistics Tables",
    "title": "R",
    "content": "Probably the most straightforward and simplest way to do a summary statistics table in R is with the stargazer package, which also has many options for customization. There are also other options like summary_table() in qwraps2 or table1() in table1, both of which have more cross-tabulation and categorical-variable functionality but require more work to set up. See this page for a comparison of different packages other than stargazer. # If necessary # install.packages('stargazer') library(stargazer) data(mtcars) # Feed stargazer a data.frame with the variables you want summarized mt_tosum &lt;- mtcars[,c('mpg','cyl','disp')] # Type = 'text' to print the table to screen, or 'latex' or 'html' to get LaTeX or HTML tables stargazer(mt_tosum, type = 'text') # There are *many* options and customizations. For all of them, see # help(stargazer) # Some useful ones include out, which designates a file to send the table to # (note that HTML tables can be copied straight into Word from an output file) stargazer(mt_tosum, type = 'html', out = 'my_summary.html', median = TRUE) # Also note that stargazer does not accept tibbles. # Use as.data.frame() to stargazer a tibble library(tidyverse) data(\"storms\") storms %&gt;% select(year, wind, pressure, ts_diameter) %&gt;% as.data.frame() %&gt;% stargazer(type = 'text') . But if you plan on maybe R Markdown instead of LaTeX formatting, there are a ton of summary statistic table packages for you to choose from. The package skimr is an excellent alternative to base::summary. skimr::skim takes different data types and outputs a summary statistic data frame. Numeric data gets miniature histograms and all types of data get information about the number of missing entries. # If necessary # install.packages('dplyr') # install.packages('skimr') library(dplyr) library(skimr) skim(starwars) #If you're wondering which columns have missing values, you can use skim() in a pipeline. starwars %&gt;% skim() %&gt;% dplyr::filter(n_missing &gt; 0) %&gt;% dplyr::select(skim_variable, n_missing, complete_rate) #You can analyze grouped data with skimr. You can also easily customize the output table using skim_with(). my_skim &lt;- skim_with(base = sfl( n = length )) starwars %&gt;% group_by(species) %&gt;% my_skim() %&gt;% dplyr::filter(skim_variable == \"height\" &amp; n &gt; 1) . Another alternative is summarytools::dfsummary. It’s even more extended than skim(). dfsummary() can handle data in the forms of characters, factors, numerics, and dates, and outputs a data frame with statistics and graphs for all variables. That data frame can’t be viewed from the console however, you’ll open it in the viewer. ",
    "url": "/Presentation/Tables/Summary_Statistics_Tables.html#r",
    "relUrl": "/Presentation/Tables/Summary_Statistics_Tables.html#r"
  },"94": {
    "doc": "Summary Statistics Tables",
    "title": "Stata",
    "content": "The built-in Stata command summarize (which can be referred to in short as su or summ) easily creates summary statistics tables. However, while summarize is well-suited for viewing descriptive statistics on your own, it is not well-suited for making tables to publish in a paper, since it is difficult to limit the number of significant digits, and does not offer an easy way to export the table other than selecting the Stata output, selecting “Copy Table”, and pasting into a spreadsheet. For more flexible tables that can be easily exported, we will be using the highly flexible estout package. For more information on the many different options and specifications for estout summary tables, see this page. We will also see how to use outreg2 in the outreg2 package, which is less flexible but is slightly less work to use for standard tables that are basically summarize but nicer-looking and output to a file. * If necessary * ssc install estout * ssc install outreg2 sysuse auto.dta, clear * summarize will give us a table that is great for our own purposes, not so much for exporting summarize price mpg rep78 i.foreign * Instead using estpost summarize will give us an esttab-compatible table * Note that factor variables no longer work - we must make dummies by hand xi i.foreign, pre(f_) noomit estpost summarize price mpg rep78 f_* * We can then use esttab and cells() to pick columns * Now it's nicely formatted * The quotes around the statistics put all the statistics in one row esttab, cells(\"count mean sd min max\") * If we want to limit the number of significant digits we must do this stat by stat * Using a standard format option (see help format) esttab, cells(\"count mean(fmt(%9.2f)) sd(fmt(%9.2f)) min max\") * And write out to file with \"using\" esttab using mytable.rtf, cells(\"count mean(fmt(%9.2f)) sd(fmt(%9.2f)) min max\") replace * Or we can work with outreg2 * First, limit the data to the variables we want to summarize preserve keep price mpg rep78 f_* * Then outreg2 with the sum(log) option to get summary statistics outreg2 using myoutreg2table.doc, word sum(log) replace * Defaults are very similar to what you'd get with summarize, but you can do things like change * number of significant digits with dec(), or which stats are in there with eqkeep() outreg2 using mysmalltable.doc, word sum(log) eqkeep(N mean) dec(3) replace restore . ",
    "url": "/Presentation/Tables/Summary_Statistics_Tables.html#stata",
    "relUrl": "/Presentation/Tables/Summary_Statistics_Tables.html#stata"
  },"95": {
    "doc": "Tables",
    "title": "Tables",
    "content": " ",
    "url": "/Presentation/Tables/Tables.html",
    "relUrl": "/Presentation/Tables/Tables.html"
  },"96": {
    "doc": "Time Series",
    "title": "Time Series",
    "content": " ",
    "url": "/Time_Series/Time_Series.html",
    "relUrl": "/Time_Series/Time_Series.html"
  },"97": {
    "doc": "Artificial Neural Network",
    "title": "Artificial Neural Network",
    "content": "Artificial neural networks are universal function approximators that consist of nodes, each of which does a computation on an input, and layers, which are collections of nodes that have access to the same inputs. There are many variations of neural networks but the most common is the multi-layer perceptron. They can be applied to supervised learning (e.g. regression and classification), unsupervised learning, and reinforcement learning. As an example, a simple single-layer perceptron for regression with \\(M\\) neurons in the hidden layer is defined as . \\[Z_m = \\sigma(\\alpha_{0m} + \\alpha_{m}^TX), \\quad m = 1, ..., M\\] \\[f(X) = \\beta_0 + \\beta^TZ\\] for feature matrix \\(X\\) and activation function $\\sigma$. A popular choice of activation function is the rectified linear unit, \\(\\sigma(v) = \\max(0, v)\\). ",
    "url": "/Machine_Learning/artificial_neural_network.html",
    "relUrl": "/Machine_Learning/artificial_neural_network.html"
  },"98": {
    "doc": "Artificial Neural Network",
    "title": "Keep in Mind",
    "content": ". | There are many different choices of activation function. | There are many different choices of network architecture, appropriate for different tasks. | Neural networks are prone to overfitting, and are sensitive to both the scale of the inputs and the choice of starting weights in the hidden layer. There are many techniques available to reduce overfitting and other issues with neural networks. | . ",
    "url": "/Machine_Learning/artificial_neural_network.html#keep-in-mind",
    "relUrl": "/Machine_Learning/artificial_neural_network.html#keep-in-mind"
  },"99": {
    "doc": "Artificial Neural Network",
    "title": "Implementations",
    "content": " ",
    "url": "/Machine_Learning/artificial_neural_network.html#implementations",
    "relUrl": "/Machine_Learning/artificial_neural_network.html#implementations"
  },"100": {
    "doc": "Artificial Neural Network",
    "title": "Python",
    "content": "There are many libraries for artificial neural networks in Python, including the widely-used, production-oriented tensorflow (from Google) and PyTorch (from Facebook). scitkit-learn has a simple neural network regressor that’s just a single line: . from sklearn.neural_network import MLPRegressor from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split # Generate synthetic data X, y = make_regression(n_samples=1000, n_features=10) # Split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1) # Create and fit model regr = MLPRegressor(hidden_layer_sizes=(100,), activation='relu').fit(X_train, y_train) # Compute R^2 score regr.score(X_test, y_test) . ",
    "url": "/Machine_Learning/artificial_neural_network.html#python",
    "relUrl": "/Machine_Learning/artificial_neural_network.html#python"
  },"101": {
    "doc": "Bar Graphs",
    "title": "Introduction",
    "content": "This is a brief tutorial on how to make bar graphs. It also provides a little information on how to stylize bar graphs to make them look better. There are a plethora of options to make a bar graph look like the visualization that you want it to. Lets dive in! . ",
    "url": "/Presentation/Figures/bar_graphs.html#introduction",
    "relUrl": "/Presentation/Figures/bar_graphs.html#introduction"
  },"102": {
    "doc": "Bar Graphs",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Figures/bar_graphs.html#implementations",
    "relUrl": "/Presentation/Figures/bar_graphs.html#implementations"
  },"103": {
    "doc": "Bar Graphs",
    "title": "Python",
    "content": "There are many plotting libraries in Python, including declarative (say what you want) and imperative (build what you want) options. In the example below, we’ll explore several different options for plotting bar chart data. For even greater control over plot elements, users may want to explore the matplotlib library (and its bar chart functionality here), but the examples below will cover most use cases. By far the quickest way to plot a bar chart is to use data analysis package pandas’ built-in bar chart option. import pandas as pd df = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/DAAG/Manitoba.lakes.csv\", index_col=0) df.plot.bar(y='area', legend=False, title='Area of lakes in Manitoba'); . This produces a functional, if not hugely attractive, plot. Calling the function without the y='area' keyword argument causes pandas to plot two columns for each lake based on the two variables in the dataframe, one for area and one for elevation (while sharing the same y-axis). pandas uses the plotting library matplotlib under the hood. Many extra configuration options are available using matplotlib. In this case, let’s just tidy the plot up a bit by applying a style, adding in a label, and putting the title on the left. import matplotlib.pyplot as plt plt.style.use('seaborn') ax = df.plot.bar(y='area', legend=False, ylabel='Area', rot=15) ax.set_title('Area of lakes in Manitoba', loc='left'); . For more sophisticated visualisations, let’s look first at the seaborn library. We’ll use the tips dataset. Note that if seaborn finds more than one row per category for the bar chart, it will automatically create error bars based on the standard deviation of your data. Although it is declarative, seaborn is built on matplotlib (like pandas built-in plots), so finer control of plots is available should it be needed. (Like df.plot.bar, sns.barplot returns an ax object when not used with the ; character.) . import seaborn as sns tips = sns.load_dataset(\"tips\") sns.barplot(x=\"day\", y=\"total_bill\", hue=\"sex\", data=tips); . Yet another declarative option comes from plotnine, which is a port of R’s ggplot and so has nearly identical syntax that library. from plotnine import ggplot, geom_bar, aes, labs ( ggplot(tips) + geom_bar(aes(x='day'), colour='black', fill='blue') + labs(x = \"Day\", y = \"Number\", title = \"Number of diners\") ) . Other packages for bar charts include proplot, an imperative library for publication-quality charts that wraps matplotlib, and altair, a declarative library which produces high-quality, web-ready graphics. ",
    "url": "/Presentation/Figures/bar_graphs.html#python",
    "relUrl": "/Presentation/Figures/bar_graphs.html#python"
  },"104": {
    "doc": "Bar Graphs",
    "title": "R",
    "content": "For the R demonstration, we will be calling the tidyverse package. if (!require(\"pacman\")) install.packages(\"pacman\") pacman::p_load(tidyverse) . This tutorial will use a dataset that already exists in R, so no need to load any new data into your environment. The dataset we will use is called starwars, which uses data collected from the Star Wars Universe. The tidyverse package uses ggplot2 to construct bar graphs. For our first example, let’s look at species’ appearences in Star Wars movies. Follow along below! . | First for our graph, we need write a line that calls ggplot. However we just use ‘ggplot’ to do so. Note the + after ggplot(). This + ties the subsequent lines together to form the graph. A common error when making any type of graph in ggplot() is to forget these + symbols at the end of a code line, so just remember to use them! | There are a couple of steps to construct a bar graph. First we need to specify the data we want to visulaize. We are making a bar graph, so we will use geom_bar. Since we want to use the 'starwars' dataset, we set data = starwars. Remember the comma after this, otherwise an error will appear. | Next we want to tell ggplot what we want to map. We use the mapping function to do this. We set mapping to the aesthetic function. (mapping = aes(x = species)) Within the aes function we want to specify what we want our x value to be, in this case species. Copy the code below to make your first bar graph! | . ggplot() + geom_bar(data = starwars, mapping = aes(x = species)) . As you can see, there are some issues. We can’t tell what the individual species are on the x axis. We also might want to give our graph a title, maybe give it some color, etc. How do we do this? By adding additional functions to our graph! . ggplot(data = starwars) + geom_bar( mapping = aes(x = species), color = \"black\", fill = \"blue\") + labs(x = \"Species\", y = \"Total\", title = \"Character Appearences in Movies by Species\") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) . This graph looks much more interpretable to me, though appearences are subjective. Let’s look at what we did. First there are two additional parts to our mapping function, color and fill. The “color = ” provides an outline color to the bars on the graph, while “fill = ” provides the color within the bars. The x and y axis have been renamed, and the graph has been given a title. This was done using the labs() function in R. This function has additional options as well which you should explore. Finally we come to the theme() function in ggplot2. theme() has many options to customize any type of graph in R. For this basic tutorial, the x values (species) have been rotated so that they are legible compared to our first graph. Congratualtions, you have made your first bar graph in R! . There is a similar ggplot() function in R called geom_col. In geom_col, you can specify what you want the y axis to be, whereas geom_bar is only a count. Want more information on how to customize your graph? The Hadley Wickam book called R for Data Science is a fantastic place to start, and best of all it’s free! . ",
    "url": "/Presentation/Figures/bar_graphs.html#r",
    "relUrl": "/Presentation/Figures/bar_graphs.html#r"
  },"105": {
    "doc": "Bar Graphs",
    "title": "Stata",
    "content": "Stata, like R, also has pre-installed datasets available for use. To find them, click on ‘file’, then click on ‘Example Datasets’ which will open up a new window. Under ‘Description’ click on the link for ‘Example datasets installed with Stata’ which will bring up a list of datasets to use for examples. For the purposes of this demonstration we will use the 'bplong.dta' option. To load it into stata, click ‘use’ and it will appear in Stata. This is fictionalized blood pressure data. In your variables column you should have five variables (patient, sex, agegrp, when, bp). Let’s make a bar chart that looks at the patients within our dataset by gender and age. To make a bar chart type into your stata command console: . graph bar, over(sex) over(agegrp) . and the following output should appear in another window. Congratulations, you’ve made your first bar chart in Stata! We can now visually see the make-up of our dataset by gender and age. We might want to change the axis labels or give this a title. To do so type the following in your command window: . graph bar, over(sex) over(agegrp) title(Our Graph) ytitle(Percent) . and the following graph shoud appear . Notice we gave our graph a title and capitalized the y axis. Lets add some color next. To do so type . graph bar, over(sex) over(agegrp) title(Our Graph) ytitle(Percent) bar(1, fcolor(red)) bar(2, fcolor(blue)) . and the following graph should appear . Our bars are now red with a blue outline. Pretty neat! There are many sources of Stata help on the internet and many different way to customize your bar graphs. There is an official Stata support page that can answer queries regarding Stata. ",
    "url": "/Presentation/Figures/bar_graphs.html#stata",
    "relUrl": "/Presentation/Figures/bar_graphs.html#stata"
  },"106": {
    "doc": "Bar Graphs",
    "title": "Bar Graphs",
    "content": " ",
    "url": "/Presentation/Figures/bar_graphs.html",
    "relUrl": "/Presentation/Figures/bar_graphs.html"
  },"107": {
    "doc": "Boosted Regression Trees",
    "title": "Introduction",
    "content": "Boosting is a numerical optimization technique for minimizing the loss function by adding, at each step, a new tree that best reduces (steps down the gradient of) the loss function. For Boosted Regression Trees (BRT), the first regression tree is the one that, for the selected tree size, maximally reduces the loss function. ",
    "url": "/Machine_Learning/boosted_regression_trees.html#introduction",
    "relUrl": "/Machine_Learning/boosted_regression_trees.html#introduction"
  },"108": {
    "doc": "Boosted Regression Trees",
    "title": "Keep in Mind",
    "content": "The Boosted Trees Model is a type of additive model that makes predictions by combining decisions from a sequence of base models. More formally we can write this class of models as: . \\[g(x) = f_0(x)+f_1(x)+f_2(x)+...\\] where the final classifier \\(g\\) is the sum of simple base classifiers \\(f_i\\). For the boosted trees model, each base classifier is a simple decision tree. This broad technique of using multiple models to obtain better predictive performance is called model ensembling. Random forests improve upon bagged trees by decorrelating the trees. In order to decorrelate its trees, a random forest only considers a random subset of predictors when making each split (for each tree). This is compared to boosted trees, which can pass information from one to the other. We add each new tree to our model (and update our residuals). Trees are typically small—slowly improving where it struggles. | Check here for more help. | . ",
    "url": "/Machine_Learning/boosted_regression_trees.html#keep-in-mind",
    "relUrl": "/Machine_Learning/boosted_regression_trees.html#keep-in-mind"
  },"109": {
    "doc": "Boosted Regression Trees",
    "title": "Also Consider",
    "content": ". | There are non-boosted approaches to decision trees, which can be found at Decision Trees and Random Forest. | . ",
    "url": "/Machine_Learning/boosted_regression_trees.html#also-consider",
    "relUrl": "/Machine_Learning/boosted_regression_trees.html#also-consider"
  },"110": {
    "doc": "Boosted Regression Trees",
    "title": "Implementations",
    "content": " ",
    "url": "/Machine_Learning/boosted_regression_trees.html#implementations",
    "relUrl": "/Machine_Learning/boosted_regression_trees.html#implementations"
  },"111": {
    "doc": "Boosted Regression Trees",
    "title": "Python",
    "content": "There are several packages that can be used to estimate boosted regression trees but sklearn provides a function GradientBoostingRegressor that is perhaps the most user-friendly. # Install scikit-learn using conda or pip if you don't already have it installed from sklearn.datasets import make_regression from sklearn.ensemble import GradientBoostingRegressor from sklearn.model_selection import train_test_split # Generate some synthetic data X, y = make_regression() # Split the synthetic data into train and test arrays X_train, X_test, y_train, y_test = train_test_split(X, y) # The number of trees is set by n_estimators; there are many other options that # you should experiment with. Typically the defaults will be sensible but are # unlikely to be perfect for your use case. Let's create the empty model: reg = GradientBoostingRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, min_samples_split=3) # Fit the model reg.fit(X_train, y_train) # Predict the value of the first test case reg.predict(X_test[:1]) # R^2 score for the model (on the test data) reg.score(X_test, y_test) . ",
    "url": "/Machine_Learning/boosted_regression_trees.html#python",
    "relUrl": "/Machine_Learning/boosted_regression_trees.html#python"
  },"112": {
    "doc": "Boosted Regression Trees",
    "title": "R",
    "content": "Boosted trees can be produced using the gbm package . Boosting has three tuning parameters. | The number of trees B (important to prevent overfitting) | The shrinkage parameter lambda (controls boosting’s learning rate . | often 0.01 or 0.001) | . | The number of splits in each tree (the tree’s complexity) | . data from:https://www.kaggle.com/kondla/carinsurance . # Load necessary packages and set the seed library(pacman) p_load(tidyverse,janitor, caret, glmnet, magrittr, dummies, janitor, rpart.plot, e1071, dplyr, caTools, naniar, forcats, ggplot2, MASS,creshape, pROC,ROCR,readr, gbm) set.seed(101) # Load in data carInsurance_train &lt;- read_csv(\"https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Machine_Learning/Data/boosted_regression_trees/carInsurance_train.csv\") summary(carInsurance_train) # Produce a training and a testing subset of the data sample = sample.split(carInsurance_train$Id, SplitRatio = .8) train = subset(carInsurance_train, sample == TRUE) test = subset(carInsurance_train, sample == FALSE) total &lt;- rbind(train ,test) gg_miss_upset(total) . Step 1: Produce dummies as appropriate . total$CallStart&lt;-as.character(total$CallStart) total$CallStart&lt;-strptime(total$CallStart,format=\" %H:%M:%S\") total$CallEnd&lt;-as.character(total$CallEnd) total$CallEnd&lt;-strptime(total$CallEnd,format=\" %H:%M:%S\") total$averagetimecall&lt;-as.numeric(as.POSIXct(total$CallEnd)-as.POSIXct(total$CallStart),units=\"secs\") time&lt;-mean(total$averagetimecall,na.rm = TRUE) . Produce dummy variables as appropriate . total_df &lt;- dummy.data.frame(total %&gt;% dplyr::select(-CallStart, -CallEnd, -Id, -Outcome)) summary(total_df) . Fill in missing values . total_df$Job[is.na(total_df$Job)] &lt;- \"management\" total_df$Education [is.na(total_df$Education)] &lt;- \"secondary\" total_df$Marital[is.na(total_df$Marital)] &lt;-\"married\" total_df$Communication[is.na(total_df$Communication)] &lt;- \"cellular\" total_df$LastContactMonth[is.na(total_df$LastContactMonth)] &lt;- \"may\" . Step 2: Preprocess data with median imputation and a central scaling . clean_new &lt;- preProcess( x = total_df %&gt;% dplyr::select(-CarInsurance) %&gt;% as.matrix(), method = c('medianImpute') ) %&gt;% predict(total_df) . Step 3: Divide the data into testing and training data . trainclean &lt;- head(clean_new, 3200) %&gt;% as.data.frame() testclean &lt;- tail(clean_new, 800) %&gt;% as.data.frame() summary(trainclean) . Step 4: Parameters . gbm needs the three standard parameters of boosted trees—plus one more: . | n.trees, the number of trees | interaction.depth, trees’ depth (max. splits from top) | shrinkage, the learning rate | n.minobsinnode, minimum observations in a terminal node | . Step 5: Train the boosted regression tree . Notice that trControl is being set to select parameters using five-fold cross-validation (\"cv\"). carinsurance_boost = train( factor(CarInsurance)~., data = trainclean, method = \"gbm\", trControl = trainControl( method = \"cv\", number = 5 ), tuneGrid = expand.grid( \"n.trees\" = seq(25, 200, by = 25), \"interaction.depth\" = 1:3, \"shrinkage\" = c(0.1, 0.01, 0.001), \"n.minobsinnode\" = 5) ) . ",
    "url": "/Machine_Learning/boosted_regression_trees.html#r",
    "relUrl": "/Machine_Learning/boosted_regression_trees.html#r"
  },"113": {
    "doc": "Boosted Regression Trees",
    "title": "Boosted Regression Trees",
    "content": " ",
    "url": "/Machine_Learning/boosted_regression_trees.html",
    "relUrl": "/Machine_Learning/boosted_regression_trees.html"
  },"114": {
    "doc": "Bootstrap Standard Errors",
    "title": "Bootstrap Standard Errors",
    "content": "Boostrapping is a statistical method that uses random sampling with replacement to determine the sampling variation of an estimate. If you have a data set of size \\(N\\), then (in its simplest form) a “bootstrap sample” is a data set that randomly selects \\(N\\) rows from the original data, perhaps taking the same row multiple times. For more information, see Wikipedia. Bootstrap is commonly used to calculate standard errors. If you produce many bootstrap samples and calculate a statistic in each of them, then under certain conditions, the distribution of that statistic across the bootstrap samples is the sampling distribution of that statistic. So the standard deviation of the statistic across bootstrap samples can be used as an estimate of standard error. This approach is generally used in cases where calculating the standard error of a statistic parametrically would be too difficult or impossible. ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/bootstrap_se.html",
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/bootstrap_se.html"
  },"115": {
    "doc": "Bootstrap Standard Errors",
    "title": "Keep in Mind",
    "content": ". | Although it feels entirely data-driven, bootstrap standard errors rely on assumptions just like everything else. It assumes your original model is correctly specified, for example. Basic bootstrapping assumes no heteroskedasticity, and otherwise independent error terms. | Bootstrapping can also be used to calculate other features of the parameter’s sample distribution, like the percentile, not just the standard error. | . ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/bootstrap_se.html#keep-in-mind",
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/bootstrap_se.html#keep-in-mind"
  },"116": {
    "doc": "Bootstrap Standard Errors",
    "title": "Also Consider",
    "content": ". | This page will consider the simplest approach to bootstrapping (the basic resampling of rows), but there are many others, such as cluster (strata) bootstrap, Bayesian bootstrap, and Wild bootstrap. For more information, see Wikipedia. Check the help files of the bootstrap package you’re using to see if they support these approaches. | Bootstrap is relatively straightforward to program yourself: resample, calculate, repeat, and then look at the distribution. If your reason for doing bootstrap is because you want your standard errors to reflect an unusual sampling or data manipulation procedure, for example, you may be best off programming your own routine. | This page contains a general approach to bootstrap, but for some statistical procedures, bootstrap standard errors are common enough that the command itself has an option to produce bootstrap standard errors. If this option is available, it is likely superior. | . ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/bootstrap_se.html#also-consider",
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/bootstrap_se.html#also-consider"
  },"117": {
    "doc": "Bootstrap Standard Errors",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/bootstrap_se.html#implementations",
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/bootstrap_se.html#implementations"
  },"118": {
    "doc": "Bootstrap Standard Errors",
    "title": "R",
    "content": "The sandwich package (link] provides a convenient vcovBS function for obtaining bootstrapped covariance-variance matrices, and thus standard errors, for a wide range of model classes in R. We normally combine this with the coeftest function from the lmtest package, which allows us to substitute in the adjusted (here: bootstrapped) errors into our model, post-estimation. # If necessary # install.packages('sandwich','lmtest') library(sandwich) library(lmtest) # Use in-built mtcars data data(mtcars) # Run a regression with normal (iid) errors m &lt;- lm(hp~mpg + cyl, data = mtcars) # Obtain the boostrapped SEs coeftest(m, vcov = vcovBS(m)) . Another approach to obtaining bootstrapping standard errors in R is to use the boot package (link). This is typcally more hands-on, but gives the user a lot of control over how the bootrapping procedure will execute. # If necessary # install.packages('boot','broom','stargazer') # Load boot library library(boot) # Create function that takes # A dataset and indices as input, and then # performs analysis and returns a parameter of interest regboot &lt;- function(data, indices) { m1 &lt;- lm(hp~mpg + cyl, data = data[indices,]) return(coefficients(m1)) } # Call boot() function using the function we just made with 200 bootstrap samples # Note the option for stratified resampling with \"strata\", in addition to other options # in help(boot) boot_results &lt;- boot(mtcars, regboot, R = 200) # See results boot_results plot(boot_results) # There are lots of diagnostics you can look at at this point, # see https://statweb.stanford.edu/~tibs/sta305files/FoxOnBootingRegInR.pdf # Optional: print regression table with the bootstrap SEs # This uses stargazer, but the method is similar # with other table-making packages, # see /Presentation/export_a_formatted_regression_table.html library(broom) tidy_results &lt;- tidy(boot_results) library(stargazer) m1 &lt;- lm(hp~mpg + cyl, data = mtcars) stargazer(m1, se = list(tidy_results$std.error), type = 'text') . ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/bootstrap_se.html#r",
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/bootstrap_se.html#r"
  },"119": {
    "doc": "Bootstrap Standard Errors",
    "title": "Stata",
    "content": "Many commands in Stata come with a vce(bootstrap) option, which will implement bootstrap standard errors. * Load auto data sysuse auto.dta, clear * Run a regression with bootstrap SEs reg mpg weight length, vce(bootstrap) * see help bootstrap to adjust options like number of samples * or strata reg mpg weight length, vce(bootstrap, reps(200)) * If a command does not support vce(bootstrap), there's a good chance it will * work with a bootstrap: prefix, which works similarly bootstrap, reps(200): reg mpg weight length . ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/bootstrap_se.html#stata",
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/bootstrap_se.html#stata"
  },"120": {
    "doc": "Causal Forest",
    "title": "Causal Forest",
    "content": "Causal forests are a causal inference learning method that are an extension of Random Forests. In random forests, the data is repeatedly split in order to minimize prediction error of an outcome variable. Causal forests are built similarly, except that instead of minimizing prediction error, data is split in order to maximize the difference across splits in the relationship between an outcome variable and a “treatment” variable. This is intended to uncover how treatment effects vary across a sample. For more information, see Explicitly Optimizing on Causal Effects via the Causal Forest. ",
    "url": "/Machine_Learning/causal_forest.html",
    "relUrl": "/Machine_Learning/causal_forest.html"
  },"121": {
    "doc": "Causal Forest",
    "title": "Keep in Mind",
    "content": ". | Causal forests simply uncover heterogeneity in a causal effect, they do not by themselves make the effect causal. A standard causal forest must assume that the assignment to treatment is exogenous, as it might be in a randomized controlled trial. Some extensions of causal forest may allow for covariate adjustment or for instrumental variables. See your causal forest package’s documentation to see if it has an option for ways of identifying the causal effect when treatment is not exogenous such as conditional adjustment or “instrumental forest”. | If using causal forest to estimate confidence intervals for the effects, in addition to the effects itself, it is recommended that you increase the number of trees generated considerably. | . ",
    "url": "/Machine_Learning/causal_forest.html#keep-in-mind",
    "relUrl": "/Machine_Learning/causal_forest.html#keep-in-mind"
  },"122": {
    "doc": "Causal Forest",
    "title": "Also Consider",
    "content": ". | Your intuition for how causal forest works can be based on a thorough understanding of Random Forests, for which materials are much more widely available. | . ",
    "url": "/Machine_Learning/causal_forest.html#also-consider",
    "relUrl": "/Machine_Learning/causal_forest.html#also-consider"
  },"123": {
    "doc": "Causal Forest",
    "title": "Implementations",
    "content": " ",
    "url": "/Machine_Learning/causal_forest.html#implementations",
    "relUrl": "/Machine_Learning/causal_forest.html#implementations"
  },"124": {
    "doc": "Causal Forest",
    "title": "Python",
    "content": "The econml package from Microsoft provides a range of causal machine learning functions, including deep instrumental variables, doubly robust learning, double machine learning, and causal forests. As in the R example below, we will download some crime data and look at the effect of one variable (‘pctymle’, the % of young males, assumed to be exogenous) on another (‘crmrte’, the crime rate). # Use \"pip install econml\" on the command line to install the package import pandas as pd from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeRegressor from econml.ortho_forest import ContinuousTreatmentOrthoForest as CausalForest df = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Crime.csv') # Set the categorical variables: cat_vars = ['year', 'region', 'smsa'] # Transform the categorical variables to dummies and add them back in xf = pd.get_dummies(df[cat_vars]) df = pd.concat([df.drop(cat_vars, axis=1), xf], axis=1) cat_var_dummy_names = list(xf.columns) regressors = ['prbarr', 'prbconv', 'prbpris', 'avgsen', 'polpc', 'density', 'taxpc', 'pctmin', 'wcon'] # Add in the dummy names to the list of regressors regressors = regressors + cat_var_dummy_names # Split into train and test train, test = train_test_split(df, test_size=0.2) # Estimate causal forest estimator = CausalForest(n_trees=100, model_T=DecisionTreeRegressor(), model_Y=DecisionTreeRegressor()) estimator.fit(train['crmrte'], train['pctymle'], train[regressors], inference='blb') effects_train = estimator.effect(train[regressors]) effects_test = estimator.effect(test[regressors]) conf_intrvl = estimator.effect_interval(test[regressors]) . ",
    "url": "/Machine_Learning/causal_forest.html#python",
    "relUrl": "/Machine_Learning/causal_forest.html#python"
  },"125": {
    "doc": "Causal Forest",
    "title": "R",
    "content": "The grf package has a causal_forest function that can be used to estimate causal forests. Additional functions afterwards can estimate, for example, the average_treatment_effect(). See help(package='grf') for more options. # If necessary # install.packages('grf') library(grf) # Get crime data from North Carolina df &lt;- read.csv('https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Crime.csv') # It's not, but let's pretend that \"percentage of young males\" pctymle is exogenous # and see how the effect of it on crmrte varies across the other measured covariates # Make sure the data has no missing values. Here I'm dropping observations # with missing values in any variable, but you can limit the data first to just # variables used in analysis to only drop observations with missing values in those variables df &lt;- df[complete.cases(df),] # Let's use training and holdout data split &lt;- sample(c(FALSE, TRUE), nrow(df), replace = TRUE) df.train &lt;- df[split,] df.hold &lt;- df[!split,] # Isolate the \"treatment\" as a matrix pctymle &lt;- as.matrix(df.train$pctymle) # Isolate the outcome as a matrix crmrte &lt;- as.matrix(df.train$crmrte) # Use model.matrix to get our predictor matrix # We might also consider adding interaction terms X &lt;- model.matrix(lm(crmrte ~ -1 + factor(year) + prbarr + prbconv + prbpris + avgsen + polpc + density + taxpc + factor(region) + factor(smsa) + pctmin + wcon, data = df.train)) # Estimate causal forest cf &lt;- causal_forest(X,crmrte,pctymle) # Get predicted causal effects for each observation effects &lt;- predict(cf)$predictions # And use holdout X's for prediction X.hold &lt;- model.matrix(lm(crmrte ~ -1 + factor(year) + prbarr + prbconv + prbpris + avgsen + polpc + density + taxpc + factor(region) + factor(smsa) + pctmin + wcon, data = df.hold)) # And get effects effects.hold &lt;- predict(cf, X.hold)$predictions # Get standard errors for the holding data predictions - we probably should have set the num.trees # option in causal_forest higher before doing this, perhaps to 5000. SEs &lt;- sqrt(predict(cf, X.hold, estimate.variance = TRUE)$variance.estimates) . ",
    "url": "/Machine_Learning/causal_forest.html#r",
    "relUrl": "/Machine_Learning/causal_forest.html#r"
  },"126": {
    "doc": "Causal Forest",
    "title": "Stata",
    "content": "The MLRtime package allows the causal_forest function in the R grf package to be run from inside of Stata. This does require that R be installed. * If necessary, install MLRtime * net install MLRtime, from(\"https://raw.githubusercontent.com/NickCH-K/MLRtime/master/\") * Then, before use, install R from R-project.org * and run the MLRtimesetup function * MLRtimesetup, go * Start a fresh R session rcall clear * Get crime data from North Carolina import delimited using \"https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Crime.csv\", clear * Turn character variables numeric so we can use them encode region, g(regionn) encode smsa, g(smsan) drop region smsa * It's not, but let's pretend that \"percentage of young males\" pctymle is exogenous * and see how the effect of it on crmrte varies across the other measured covariates * Let's use training and holdout data by sending our holdout data to R with rcall g split = runiform() &gt; .5 preserve * Keep the predictors from the holding data, send it over, so later we can make an X matrix to predict with keep if split == 0 keep year prbarr prbconv prbpris avgsen polpc density taxpc regionn smsan pctmin wcon * R needs that data pre-processed! So using the same variables as in the main model, process the variables fvrevar year prbarr prbconv prbpris avgsen polpc density taxpc i.regionn i.smsan pctmin wcon keep `r(varlist)' * Then send the data to R rcall: df.hold &lt;- st.data() restore * Now go back to just the training data * Run causal_forest, storing the effect predictions for the training data in the \"effects\" variable * the SEs of those effects in effectSE * And the effects and SEs for the holdout data in matrices called effects_hold and effectSE_hold causal_forest crmrte pctymle year prbarr prbconv prbpris avgsen polpc density taxpc i.regionn i.smsan pctmin wcon, pred(effects) varreturn(effectSE = sqrt(predict(CF, X, estimate.variance = TRUE)@@variance.estimates)) return(effects_hold = predict(CF, as.matrix(df.hold))@@predictions; effectSE_hold = sqrt(predict(CF, as.matrix(df.hold), estimate.variance = TRUE)@@variance.estimates)) * Look at the holdout effects predicted di \"`r(effects_hold)'\" . ",
    "url": "/Machine_Learning/causal_forest.html#stata",
    "relUrl": "/Machine_Learning/causal_forest.html#stata"
  },"127": {
    "doc": "Choropleths",
    "title": "Choropleths",
    "content": "Choropleths are maps in which areas are shaded or patterned in proportion to a statistical variable that represents an aggregate summary of a geographic characteristic within each area. For instance, population might be represented by dark green where it is (relatively) high and light green where it is (relatively) low. Choropleths are useful when you want to show differences in variables across areas. ",
    "url": "/Geo-Spatial/choropleths.html",
    "relUrl": "/Geo-Spatial/choropleths.html"
  },"128": {
    "doc": "Choropleths",
    "title": "Keep in Mind",
    "content": ". | Geospatial packages in R and Python tend to have a large number of complex dependencies, which can make installing them painful. Best practice is to install geospatial packages in a new virtual environment. | Think carefully about the units you’re using and whether plotting your data on a map is really informative. Choropleths can all too easily end up simply being plots of population density - and no-one will be surprised to see that areas like Manhattan have a high population! | . ",
    "url": "/Geo-Spatial/choropleths.html#keep-in-mind",
    "relUrl": "/Geo-Spatial/choropleths.html#keep-in-mind"
  },"129": {
    "doc": "Choropleths",
    "title": "Implementations",
    "content": " ",
    "url": "/Geo-Spatial/choropleths.html#implementations",
    "relUrl": "/Geo-Spatial/choropleths.html#implementations"
  },"130": {
    "doc": "Choropleths",
    "title": "Python",
    "content": "The geopandas package is the easiest way to start making choropleths in Python. For plotting more sophisticated maps, there’s geoplot. In the example below, we’ll see three ways of plotting data on GDP per capita by geography. The first uses geopandas built-in .plot method. The second combines this with the matplotlib package to create a more attractive looking chart. The third example uses geoplot to create a cartogram in which the area of each country on the map gets shrunk according to how small its GDP per capita is. # Geospatial packages tend to have many elaborate dependencies. The quickest # way to get going is to use a clean virtual environment and then # 'conda install geopandas' followed by # 'conda install -c conda-forge descartes' import matplotlib.pyplot as plt import geopandas as gpd world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres')) world = world[(world.pop_est &gt; 0) &amp; (world.name != \"Antarctica\")] world['gdp_per_cap'] = 1.0e6 * world.gdp_md_est / world.pop_est # Simple choropleth world.plot(column='gdp_per_cap') # Much better looking choropleth plt.style.use('seaborn-paper') fig, ax = plt.subplots(1, 1) world.plot(column='gdp_per_cap', ax=ax, cmap='plasma', legend=True, vmin=0., legend_kwds={'label': \"GDP per capita (USD)\", 'orientation': \"horizontal\"}) plt.axis('off') # Now let's try a cartogram # If you don't have it already, geoplot can be installed by runnning # 'conda install geoplot -c conda-forge' on the command line. import geoplot as gplt ax = gplt.cartogram( world, scale='gdp_per_cap', hue='gdp_per_cap', cmap='plasma', linewidth=0.5, figsize=(8, 12) ) gplt.polyplot(world, facecolor='lightgray', edgecolor='None', ax=ax) plt.title(\"GDP per capita (USD)\") plt.show() . ",
    "url": "/Geo-Spatial/choropleths.html#python",
    "relUrl": "/Geo-Spatial/choropleths.html#python"
  },"131": {
    "doc": "Cluster-Robust Standard Errors",
    "title": "Cluster-Robust Standard Errors (a.k.a. Clustered Standard Errors)",
    "content": "Data is considered to be clustered when there are subsamples within the data that are related to each other. For example, if you had data on test scores in a school, those scores might be correlated within classroom because classrooms share the same teacher. When error terms are correlated within clusters but independent across clusters, then regular standard errors, which assume independence between all observations, will be incorrect. Cluster-robust standard errors are designed to allow for correlation between observations within cluster. For more information, see A Practitioner’s Guide to Cluster-Robust Inference. ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html#cluster-robust-standard-errors-aka-clustered-standard-errors",
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html#cluster-robust-standard-errors-aka-clustered-standard-errors"
  },"132": {
    "doc": "Cluster-Robust Standard Errors",
    "title": "Keep in Mind",
    "content": ". | Just because there are likely to be clusters in your data is not necessarily a good justification for using cluster-robust inference. Generally, clustering is advised only if either sampling or treatment assignment is performed at the level of the clusters. See Abadie, Athey, Imbens, &amp; Wooldridge (2017), or this simple summary of the paper. | There are multiple kinds of cluster-robust standard errors, for example CR0, CR1, and CR2. Check in to the kind available to you in the commands you’re using. | . ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html#keep-in-mind",
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html#keep-in-mind"
  },"133": {
    "doc": "Cluster-Robust Standard Errors",
    "title": "Also Consider",
    "content": ". | Cluster Bootstrap Standard Errors, which are another way of performing cluster-robust inference that will work even outside of a standard regression context. | . ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html#also-consider",
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html#also-consider"
  },"134": {
    "doc": "Cluster-Robust Standard Errors",
    "title": "Implementations",
    "content": "Note: Clustering of standard errors is especially common in panel models, such as linear fixed effects. For this reason, software routines for these particular models typically offer built-in support for (multiway) clustering. The implementation pages for these models should be hyperlinked in the relevant places below. Here, we instead concentrate on providing implementation guidelines for clustering in general. ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html#implementations",
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html#implementations"
  },"135": {
    "doc": "Cluster-Robust Standard Errors",
    "title": "Julia",
    "content": "For cluster-robust estimation of (high-dimensional) fixed effect models in Julia, see here. ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html#julia",
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html#julia"
  },"136": {
    "doc": "Cluster-Robust Standard Errors",
    "title": "R",
    "content": "For cluster-robust estimation of (high-dimensional) fixed effect models in R, see here. Note that these methods can easily be re-purposed to run and cluster standard errors of non-panel models; just omit the fixed-effects in the model call. But for this page we’ll focus on some additional methods. Cluster-robust standard errors for many different kinds of regression objects in R can be obtained using the vcovCL or vcovBS functions from the sandwich package (link). To perform statistical inference, we combine these with the coeftest function from the lmtest package. This approach allows users to adjust the standard errors for a model “on-the-fly” (i.e. post-estimation) and is thus very flexible. # If necessary, install lmtest, sandwich, and estimatr # install.packages(c('lmtest','sandwich','estimatr')) # Read in data from the College Scorecard df &lt;- read.csv('https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Model_Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv') # Create a regression model with normal (iid) errors my_model &lt;- lm(repay_rate ~ earnings_med + state_abbr, data = df) # Swap out cluster-robust errors post-estimation with coeftest::lmtest and sandwich::vcovCL library(lmtest) library(sandwich) coeftest(my_model, vcov = vcovCL(my_model, cluster = ~inst_name)) . Alternately, users can specify clustered standard errors directly in the model call using the lm_robust function from the estimatr package (link). This latter approach is very similar to how errors are clustered in Stata, for example. # Alternately, use estimator::lm_robust to specify clustered SEs in the original model call. # Standard error types are referred to as CR0, CR1 (\"stata\"), CR2 here. # Here, CR2 is the default library(estimatr) my_model2 &lt;- lm_robust(repay_rate ~ earnings_med + state_abbr, data = df, clusters = inst_name, se_type = \"stata\") summary(my_model2) . ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html#r",
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html#r"
  },"137": {
    "doc": "Cluster-Robust Standard Errors",
    "title": "Stata",
    "content": "Stata has clustered standard errors built into most regression commands, and they generally work the same way for all commands. * Load in College Scorecard data import delimited \"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Model_Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv\", clear * The missings are written as \"NA\", let's turn this numeric destring earnings_med repay_rate, replace force * If we want to cluster on a variable or include it as a factor it must not be a string encode inst_name, g(inst_name_encoded) encode state_abbr, g(state_encoded) * Just add vce(cluster) to the options of the regression * This will give you CR1 regress repay_rate earnings_med i.state_encoded, vce(cluster inst_name_encoded) . ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html#stata",
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html#stata"
  },"138": {
    "doc": "Cluster-Robust Standard Errors",
    "title": "Cluster-Robust Standard Errors",
    "content": " ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html",
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/clustered_se.html"
  },"139": {
    "doc": "Collapse a Data Set",
    "title": "Collapse a Data Set",
    "content": "The observation level of a data set is the set of case-identifying variables which, in combination, uniquely identify every row of the data set. For example, in the data set . | I | J | X | . | 1 | 1 | 3 | . | 1 | 2 | 3.5 | . | 2 | 1 | 2 | . | 2 | 2 | 4.5 | . the variables \\(I\\) and \\(J\\) uniquely identify rows. The first row has \\(I = 1\\) and \\(J = 1\\), and there is no other row with that combination. We could also say that \\(X\\) uniquely identifies rows, but in this example \\(X\\) is not a case-identifying variable, it’s actual data. It is common to want to collapse a data set from one level to another, coarser level. For example, perhaps instead of one row per combination of \\(I\\) and \\(J\\), we simply want one row per \\(I\\), perhaps with the average \\(X\\) across all \\(I\\) observations. This would result in: . | I | X | . | 1 | 3.25 | . | 2 | 3.25 | . This can be one useful way to produce summary statistics, but can also be used to rid the data of unnecessary or unusable detail, or to change one data set to match the observation level of another. ",
    "url": "/Data_Manipulation/collapse_a_data_set.html",
    "relUrl": "/Data_Manipulation/collapse_a_data_set.html"
  },"140": {
    "doc": "Collapse a Data Set",
    "title": "Keep in Mind",
    "content": ". | Collapsing a data set almost by definition requires losing some information. Make sure that you actually want to lose this information, rather than, for example, doing a horizontal merge, which can match data sets with different observation levels without losing information. | Make sure that, for each variable you plan to retain in your new, collapsed data, you know the correct procedure that should be used to figure out the new, summarized value. Should the collapsed data for variable \\(X\\) use the mean of all the observations you started with? The median? The mode? The first value found in the data? Think through these decisions. | . ",
    "url": "/Data_Manipulation/collapse_a_data_set.html#keep-in-mind",
    "relUrl": "/Data_Manipulation/collapse_a_data_set.html#keep-in-mind"
  },"141": {
    "doc": "Collapse a Data Set",
    "title": "Also Consider",
    "content": ". | For more information about observation levels and how to determine what the current observation level is, see determine the observation level of a data set. | . ",
    "url": "/Data_Manipulation/collapse_a_data_set.html#also-consider",
    "relUrl": "/Data_Manipulation/collapse_a_data_set.html#also-consider"
  },"142": {
    "doc": "Collapse a Data Set",
    "title": "Implementations",
    "content": " ",
    "url": "/Data_Manipulation/collapse_a_data_set.html#implementations",
    "relUrl": "/Data_Manipulation/collapse_a_data_set.html#implementations"
  },"143": {
    "doc": "Collapse a Data Set",
    "title": "Python",
    "content": "As in the R example below, we would like to collapse data on storms so that it is uniquely identified by name, year, month, and day. However, there are sometimes multiple observations within each combination of those so we need to collapse these. But to combine multiple variables requires an aggregation of some kind. So, in code, we groupby the variables that we would like to retain and give a dictionary of functions (of the form 'original column': 'function') to aggregate the other variables by. import pandas as pd # Pull in data on storms storms = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/dplyr/storms.csv') # Find the mean wind, mean pressure, and the first # category value by name, year, month, and day. # To do this, use a groupby, followed by an aggregation. storms_collapsed = (storms .groupby(['name', 'year', 'month', 'day']) .agg({'wind': 'mean', 'pressure': 'mean', 'category': 'first'})) . ",
    "url": "/Data_Manipulation/collapse_a_data_set.html#python",
    "relUrl": "/Data_Manipulation/collapse_a_data_set.html#python"
  },"144": {
    "doc": "Collapse a Data Set",
    "title": "R",
    "content": "# If necessary, install dplyr # install.packages('dplyr') library(dplyr) # Get data on storms from dplyr data(\"storms\") # We would like each storm to be identified by # name, year, month, and day # However, currently, they are also identified by hour, # And even then there are sometimes multiple observations per hour # To construct the collapsed data, we start with the original storms_collapsed &lt;- storms %&gt;% # group by the variables that make the new observation level group_by(name, year, month, day) %&gt;% # And use summarize() to pick the variables to keep, as well as # the functions we want to use to collapse each variable. # Let's get the mean wind and pressure, and the first category value summarize(wind = mean(wind), pressure = mean(pressure), category = first(category)) # Note that if we wanted to collapse every variable in the data with the # same function, we could instead use summarize_all() . ",
    "url": "/Data_Manipulation/collapse_a_data_set.html#r",
    "relUrl": "/Data_Manipulation/collapse_a_data_set.html#r"
  },"145": {
    "doc": "Collapse a Data Set",
    "title": "Stata",
    "content": "** Load surface.dta, which contains temperature recordings in different locations sysuse surface.dta, clear * Currently, there is one observation per latitude per longitude per date * I would like this to just be one observation per latitude/longitude * So I construct a collapse command. * I take my new target observation level and put it in by() * and then take each variable I'd like to keep around and tell * Stata what function to use to create the new collapsed value, here (mean) collapse (mean) temperature, by(latitude longitude) . ",
    "url": "/Data_Manipulation/collapse_a_data_set.html#stata",
    "relUrl": "/Data_Manipulation/collapse_a_data_set.html#stata"
  },"146": {
    "doc": "Horizontal Combination (Deterministic)",
    "title": "Combining Datasets: Horizontal Combination (Deterministic)",
    "content": "A deterministic merge is when there is some variable(s) that uniquely and dependably identifies individual units (be it people, firms, teams, etc.) across datasets. For example, we might have two datasets containing information about the same set of people, one with their financial information, the other with their educational information. To analyze the relationship between the education and financial measures, we need them in the same dataset and so would want to combine them. If both datasets had a unique identification field for each person, such as a social security number or other national id, we could use this to match the records, so that all information from the same person appeared on the same line. Because we expect such identifiers to be unique to an individual (unlike many names, such as John Smith) and appear exactly the same in each dataset, we can use just this field to do the match, and don’t anticipate in ambiguity in determining which records match to each other. Thus, it is a deterministic merge. ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#combining-datasets-horizontal-combination-deterministic",
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#combining-datasets-horizontal-combination-deterministic"
  },"147": {
    "doc": "Horizontal Combination (Deterministic)",
    "title": "Keep in mind",
    "content": ". | For any number of reasons, one or both of the datasets may have more than one observation per unit or individual. That may be for a good reason – such as havinng multiple test scores for the same student because they took exams at different points in time – or it may be redundant information. Understanding the structure of your data is key before embarking on a deterministic merge. | It is a good idea to have a clear sense of how much overlap you anticipate across your datasets. It is important to examine the results of your merge and see if it matches the amount the overlap you expected. Subtle differences in a matching variable (e.g. if leading zeroes are present in an ID variable for one variable but not another) can be a source of major headaches for your analysis if not caught early. If something looks weird in your results later in the project, trouble with a merge is a common cause. So check your merge results early and often. | . ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#keep-in-mind",
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#keep-in-mind"
  },"148": {
    "doc": "Horizontal Combination (Deterministic)",
    "title": "Also Consider",
    "content": ". | Determine the observation level of a data set. | . ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#also-consider",
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#also-consider"
  },"149": {
    "doc": "Horizontal Combination (Deterministic)",
    "title": "Implementations",
    "content": " ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#implementations",
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#implementations"
  },"150": {
    "doc": "Horizontal Combination (Deterministic)",
    "title": "Python",
    "content": "There are three main ways to join datasets horizontally in python using the merge function in pandas: one-to-one joins (e.g. two DataFrames joined on unique indexes), many-to-one joins (e.g. joining a unique index to one or more columns in a different DataFrame), and many-to-many joins (joining columns on columns). The column(s) to use as keys for the merge are specified with the on= keyword argument. The merges are different depending on if the merge is inner (use only those keys in both DataFrames), outer (use the cartesian product of all keys), left (use only keys in the left DataFrame), or right (use only keys in the right DataFrame). Outer joins will include entries for all possible combinations of columns. Further details can be found in the pandas documentation. import pandas as pd gdp_2018 = pd.DataFrame({'country': ['UK', 'USA', 'France'], 'currency': ['GBP', 'USD', 'EUR'], 'gdp_trillions': [2.1, 20.58, 2.78]}) dollar_value_2018 = pd.DataFrame({'currency': ['EUR', 'GBP', 'YEN', 'USD'], 'in_dollars': [1.104, 1.256, .00926, 1]}) # Perform a left merge, which discards 'YEN' GDPandExchange = pd.merge(gdp_2018, dollar_value_2018, how='left', on='currency') . ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#python",
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#python"
  },"151": {
    "doc": "Horizontal Combination (Deterministic)",
    "title": "R",
    "content": "There are several ways to combine data sets horizontally in R, including base-R merge and several different approaches in the data.table package. We will be using the join functions in the dplyr package. # If necessary, install dplyr # install.packages('dplyr') library(dplyr) # This data set contains information on GDP in local currency GDP2018 &lt;- data.frame(Country = c(\"UK\", \"USA\", \"France\"), Currency = c(\"Pound\", \"Dollar\", \"Euro\"), GDPTrillions = c(2.1, 20.58, 2.78)) # This data set contains dollar exchange rates DollarValue2018 &lt;- data.frame(Currency = c(\"Euro\", \"Pound\", \"Yen\", \"Dollar\"), InDollars = c(1.104, 1.256, .00926, 1)) . Next we want to join together GDP2018 and DollarValue2018 so we can convert all the GDPs to dollars and compare them. There are three kinds of observations we could get - observations in GDP2018 but not DollarValue2018, observations in DollarValue2018 but not GDP2018, and observations in both. Use help(join) to pick the variant of join that keeps the observations we want. The “Yen” observation won’t have a match, and we don’t need to keep it. So let’s do a left_join and list GDP2018 first, so it keeps matched observations, plus any observations only in GDP2018. GDPandExchange &lt;- left_join(GDP2018, DollarValue2018) . The join function will automatically detect that the Currency variable is shared in both data sets and use it to join them. Generally, you will want to be sure that the set of variables you are joining by uniquely identifies observations in at least one of the data sets you are joining. If you’re not sure whether that’s true, see Determine the observation level of a data set, or run join through the safe_join from the pmdplyr package. ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#r",
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#r"
  },"152": {
    "doc": "Horizontal Combination (Deterministic)",
    "title": "Stata",
    "content": "A Quick Prelude About “Master” And “Using” Datasets . When merging two datasets together, there are two relevant datasets to consider. The first is the one currently in Stata’s memory, the other is whatever dataset (not currently loaded into Stata) that you would like to merge onto the dataset in memory. For ease of reference, Stata calls the dataset in memory the “master” dataset and the other file the “using” dataset. When you see the syntax of the merge command, the reason for calling it the “using” dataset will become clear. In Stata, there are 3 types of deterministic merges: . 1-to-1 . A one-to-one merge expects there to be no more than one row in each dataset to have a matched pair in the other dataset. If there is more than one observation with the same identifying variable(s) in either the master or using datasets when attempting to do a one-to-one merge, Stata will throw an error. (Note: you can check to see if there is more than one observation per identifying variable by using the “duplicates report” command, or the Gtools variant for especially large datasets, called “gduplicates report.”) . *Import Data with Car Sizes and Merge it to Data on Car Prices using the ID variable “Make” webuse autosize.dta, clear merge 1:1 make using http://www.stata-press.com/data/r14/autoexpense.dta . Note that the syntax specifies “make” as the identifying variable after the merge type (1:1) and before the using statement (thus why we call the data not in memory the “using” data. The result of this merge shows 5 successful matches and one observation from the master dataset that did not have a match. | Result | # of obs. | _merge value | . | not matched from master | 1 | (_merge==1) | . | not matched from using | 0 | (_merge==2) | . |   |   |   | . | matched | 5 | (_merge==3) | . Note that Stata creates a new variable (_merge) during the merge that stores the merge status of each observation, where a value of 1 means that the observation was only found in the master dataset, 2 means it was found only in the using dataset, and 3 means it was found in both and successfully merged. Many-to-1 . A many-to-one merge occurs when the master dataset contains multiple observations of the same unit or individual (say, multiple test scores for the same student), while the using dataset has only one observation per unit or individual (say, the age of each student). Here is the syntax for a many-to-1 merge. *Import Data with Car Sizes and Merge it to Data on Car Prices using the ID variable “Make” webuse sforce.dta, clear merge m:1 region using http://www.stata-press.com/data/r14/dollars.dta . Note that in this case that the syntax changes from merge 1:1 to merge m:1 where m stands for many. In this case, the identifying variable is “region.” . 1-to-Many . A one-to-many merge is the opposite of a many to one merge, with multiple observations for the same unit or individual in the using rather than the master data. The only different in the syntax is that it becomes merge 1:m rather than merge m:1. Many-to-Many . A many-to-many merge is intended for use when there are multiple observations for each combination of the set of merging variables in both master and using data. However, merge m:m has strange behavior that is effectively never what you want, and it is not recommended. ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#stata",
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html#stata"
  },"153": {
    "doc": "Horizontal Combination (Deterministic)",
    "title": "Horizontal Combination (Deterministic)",
    "content": " ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html",
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html"
  },"154": {
    "doc": "Combining Datasets",
    "title": "Combining Datasets Overview",
    "content": "There are two main ways to combine data: vertically and horizontally. That is, you can want to combine observations (adding new variables) or combine variables (adding new observations). This is perhaps easiest to show visually: . Individual Name Info |Name| ID |–|–|John Smith|A63240|Desiree Thomas|B78242| . Individual Age Info |ID | Age |–|–|B78242|22|A63240|27| . In the case above, we would like to combine two datasets, the Individual Name Info and the Individual Date Info, that have different information about the same people, who are identified by the ID variable. The result from the merge would be to have a new dataset with more columns than the original datasets because it contains all of the information for each individual from both of the original datasets. Here we have to combine the files according to the ID variable, placing the information from observations with the same ID on the same row in the combined dataset. Alternatively, the below example has two datasets that collect the same information about different people. We would like to combine these datasets vertically, with the result containing more rows than the original dataset, because it contains all of the people that are present in each of the original datasets. Here we combine the files based on the name or position of the columns in the dataset. | Name | ID | Age | . | John Smith | A63240 | 22 | . | Desiree Thomas | B78242 | 27 | . | Name | ID | Age | . | Teresa Suarez | Y34208 | 19 | . | Donald Akliberti | B72197 | 34 | . These ways of combining data are referred to by different names across different programming languages, but will largely be referred to by one common set of terms (used by Stata and Python’s Pandas): merge for horizontal combination and append for for vertical combination. ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_overview.html#combining-datasets-overview",
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_overview.html#combining-datasets-overview"
  },"155": {
    "doc": "Combining Datasets",
    "title": "Combining Datasets",
    "content": " ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_overview.html",
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_overview.html"
  },"156": {
    "doc": "Vertical Combination",
    "title": "Combining Datasets: Vertical Combination",
    "content": "When combining two datasets that collect the same information about different people, they get combined vertically because they have variables in common but different observations. The result of this combination will more rows than the original dataset because it contains all of the people that are present in each of the original datasets. Here we combine the files based on the name or position of the columns in the dataset. It is a “vertical” combination in the sense that one set of observations gets added to the bottom of the other set of observations. ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html#combining-datasets-vertical-combination",
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html#combining-datasets-vertical-combination"
  },"157": {
    "doc": "Vertical Combination",
    "title": "Keep in Mind",
    "content": ". | Vertical combinations require datasets to have variables in common to be of much use. That said, it may not be necessary for the two datasets to have exactly the same variables. Be aware of how your statistical package handles observations for a variable that is in one dataset but not another (e.g. are such observations set to missing?). | It may be the case that the datasets you are combining have the same variables but those variables are stored differently (numeric vs. string storage types). Be aware of how the variables are stored across datasets and how your statistical package handles attempts to combine the same variable with different storage types (e.g. Stata throws an error and will now allow the combination, unless the “, force” option is specified.) | . ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html#keep-in-mind",
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html#keep-in-mind"
  },"158": {
    "doc": "Vertical Combination",
    "title": "Implementations",
    "content": " ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html#implementations",
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html#implementations"
  },"159": {
    "doc": "Vertical Combination",
    "title": "Python",
    "content": "pandas is by far the most widely-used library for data manipulation in python. The concat function concatenates datasets vertically and combines datasets even if they don’t contain the exact same set of variables. It’s also possible to concatenate dataframes horizontally by passing the function the keyword argument axis=1. import pandas as pd # Load California Population data from the internet df_ca = pd.read_stata('http://www.stata-press.com/data/r14/capop.dta') df_il = pd.read_stata('http://www.stata-press.com/data/r14/ilpop.dta') # Concatenate a list of the dataframes (works on any number of dataframes) df = pd.concat([df_ca, df_il]) . ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html#python",
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html#python"
  },"160": {
    "doc": "Vertical Combination",
    "title": "R",
    "content": "There are several ways to vertically combine data sets in R, including rbind. We will use the dplyr package function bind_rows, which allows the two data sets to combine even if they don’t contain the exact same set of variables. # If necessary, install dplyr # install.packages('dplyr') library(dplyr) # Load in mtcars data data(mtcars) # Split it in two, so we can combine them back together mtcars1 &lt;- mtcars[1:10,] mtcars2 &lt;- mtcars[11:32,] # Use bind_rows to vertically combine the data sets mtcarswhole &lt;- bind_rows(mtcars1, mtcars2) . ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html#r",
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html#r"
  },"161": {
    "doc": "Vertical Combination",
    "title": "Stata",
    "content": "* Load California Population data webuse http://www.stata-press.com/data/r14/capop.dta // Import data from the web append using http://www.stata-press.com/data/r14/ilpop.dta // Merge on Illinois population data from the web . You can also append multiple datasets at once, by simply listing both datasets separated by a space: . * Load California Population data * Import data from the web webuse http://www.stata-press.com/data/r14/capop.dta * Merge on Illinois and Texas population data from the web append using http://www.stata-press.com/data/r14/ilpop.dta http://www.stata-press.com/data/r14/txpop.dta . Note that, if there are columns in one but not the other of the datasets, Stata will still append the two datasets, but observations from the dataset that did not contain those columns will have their values for that variable set to missing. * Load Even Number Data webuse odd.dta, clear append using http://www.stata-press.com/data/r14/even.dta . ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html#stata",
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html#stata"
  },"162": {
    "doc": "Vertical Combination",
    "title": "Vertical Combination",
    "content": " ",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html",
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html"
  },"163": {
    "doc": "Create a Conda Package (Python)",
    "title": "Create a Conda Package (Python)",
    "content": "Warning This is a page aimed at git experts at the moment. We welcome suggestions for pushing its difficulty down. Sometimes there are packages that are available in the Python ecosystem generally but which have not yet been incorporated into the conda-forge package repository for Anaconda. This page shows you the correct way to install that package into your environment in the interim and how to get that package added to the conda-forge repository. ",
    "url": "/Other/create_a_conda_package.html",
    "relUrl": "/Other/create_a_conda_package.html"
  },"164": {
    "doc": "Create a Conda Package (Python)",
    "title": "Installing the package locally",
    "content": "First, create a temporary directory where we’ll do all of our work. In the following snippet we call that directory TMP_DIR. Second, identify the package you’d like to install that is currently available via pip. In what follows, we’ll use linearmodels as an example. PACKAGE=linearmodels TMP_DIR=\"~/tmp/{PACKAGE}_build\" # Create the temporary directory mkdir -p $TMP_DIR &amp;&amp; cd $TMP_DIR # Conda boilerplate conda skeleton pypi $PACKAGE conda build -c conda-forge $PACKAGE # Install the package conda install --use-local -c conda-forge linearmodels . ",
    "url": "/Other/create_a_conda_package.html#installing-the-package-locally",
    "relUrl": "/Other/create_a_conda_package.html#installing-the-package-locally"
  },"165": {
    "doc": "Create a Conda Package (Python)",
    "title": "Setting up an Anaconda feedstock for your package",
    "content": "The above solves the problem for you locally, but not for the broader community! In this section, we’ll describe generic instructions for creating an Anaconda feedstock for a Python 3 package. There are detailed instructions here, but we found them a bit confusing for relatively simple packages. First, an overview of the steps: . | Fork the conda-forge/staged-recipes repo on GitHub. | Create the skeleton recipe for your package. | Edit the skeleton recipe. | Create a PR for your recipe. | Work with conda-forge maintainers until your recipe is merged. | . Forking the conda-forge/staged-recipes repo . On GitHub, go to conda-forge/staged-recipes and fork the repository. If you already have a fork, I recommend deleting your fork and reforking. This repository is highly volatile. Creating the skeleton recipe . As above, we’ll assume you’re working with the linearmodels package. We’ll also assume you’re working in REPO_DIR as defined below. You’ll need to run the below scripts. REPO_DIR=~/repo GITHUB_USER=khwilson PACKAGE=linearmodels LOCAL_REPO_DIR=\"${REPO_DIR}/staged-recipes\" # Make sure your REPO_DIR exists mkdir -p $REPO_DIR &amp;&amp; cd $REPO_DIR # Pull your fork git clone \"https://github.com/${GITHUB_USER}/staged-recipes.git\" cd $LOCAL_REPO_DIR # Create the skeleton recipe cd \"recipes\" conda skeleton pypi $PACKAGE . You should now see a file in “${PACKAGE}/meta.yaml”. This is your skeleton recipe. Edit the recipe . You’ll need to make several changes to the skeleton. These can be divided into: . | Make the recipe py3k only. | If your recipe involves Cython, adding some build requirements. | Getting the license setup. | Other “about” metadata. | Add your name to the list of maintainers. | . Make the recipe py3k only . In the build section, add a key skip: True # [py2k] . If your recipe involves Cython . In the requirements section: . | Add a build section that looks like this: ```yaml | . build: . | {{ compiler(‘c’) }} | {{ compiler(‘cxx’) }} | . * In the `host` section, change the line involving `pip` to `pip &gt;=10`. * In the `run` section, remove any references to Cython and pip. #### Getting the license setup This is probably the most annoying part. Find your packages online home. Copy the LICENSE file to your recipe's directory. Then in the `about` section, add the line `license_file: NAME_OF_LICENSE_FILE`, where `NAME_OF_LICENSE_FILE` is the file you saved the license to. Also, you'll need to describe the license by type. Sometimes, conda can figure this out for you. However, sometimes you'll need to figure it out. To do so, you can typically dump large parts of the text into Google and it will tell you the name. In the worst case, you may need to physically search [this page](https://opensource.org/licenses/alphabetical). Once you've found the license, add `license: SPDX_SHORT_CODE` to the `about` block. N.B. You may also need to add a `license_family` to the `about` block. However, the errors should tell you how to do this. :-) #### Other `about` metatdata There are some other stubs in the `about` section that conda should have setup for you, specifically around docs. If you can find these online, then add them here. #### Add your name to the list of maintainers Finally, at the bottom of the `meta.yaml` file, you will need a section that looks like: ```yaml extra: recipe-maintainers: - khwilson . Here you should obviously replace khwilson with your own GitHub username. This completes the main part of recipe editing. Create a PR for your recipe . Before creating a PR for your recipe, you probably want to test your recipe locally. To do so, you’ll need to have docker installed. Then run the following: . cd $LOCAL_REPO_DIR # Remove extra recipes rm -rf recipes/example recipes/spyrmsd . Then in the .circleci/build_steps.sh file, comment out the line that starts git ls-tree --name-only master -- .. Then you should be able to run ./.circleci/run_docker_build.sh. You’ll probably see some errors which you’ll need to fix. Once these errors are sorted out, you can push your recipe to GitHub and create a PR. Make sure to name the PR something memorable, e.g., Adding linearmodels recipe. ",
    "url": "/Other/create_a_conda_package.html#setting-up-an-anaconda-feedstock-for-your-package",
    "relUrl": "/Other/create_a_conda_package.html#setting-up-an-anaconda-feedstock-for-your-package"
  },"166": {
    "doc": "Creating Dummy Variables",
    "title": "Introduction",
    "content": "Creating a dummy variable can be just like creating any other variable but dummy variables can only take the value of 0 or 1 (or false or true). This gives us even more options in how we decide to add dummies. Dummy variables are often used as a way of including categorical variables in a model. ",
    "url": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#introduction",
    "relUrl": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#introduction"
  },"167": {
    "doc": "Creating Dummy Variables",
    "title": "Keep in Mind",
    "content": "Factor class vectors are automatically treated as dummies in regression models in R (Stata and SW languages have similar capabilities). In order to transform a categorical vector to a factor class you can simply use factor() on the variable in regression in R, or i. in Stata. This means you don’t have to create a different dummy vector for every value. If you are interested in looking behind the scenes you can use model.matrix() to see how R is creating dummies from these factor class variables. Note: model.matrix() creates a separate dummy column for all values in the vector. This is called one-hot encoding and, if you aren’t careful, can lead to the dummy variable trap if an intercept is also included in the regression. The dummy variable trap arises because of perfect multicollinearity between the intercept term and the dummy variables (which row-wise all add up to 1). So one of the columns needs to be dropped from the regression in order for it to run. Typically, the first variable is the one which is dropped and effectively absorbed into the intercept term. If this happens then all the dummy estimates will be in reference to the dropped dummy. ",
    "url": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#keep-in-mind",
    "relUrl": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#keep-in-mind"
  },"168": {
    "doc": "Creating Dummy Variables",
    "title": "Implementations",
    "content": " ",
    "url": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#implementations",
    "relUrl": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#implementations"
  },"169": {
    "doc": "Creating Dummy Variables",
    "title": "Python",
    "content": "Several python libraries have functions to turn categorical variables into dummies, including pandas, scikit-learn (where it is called OneHotEncoder), and statsmodels (where it is called categorical). This example uses pandas get_dummies function. import pandas as pd # Create a dataframe df = pd.DataFrame({'colors': ['red', 'green', 'blue', 'red', 'blue'], 'numbers': [5, 13, 1, 7, 5]}) # Replace the colors column with a dummy column for each color df = pd.get_dummies(df, columns=['colors']) . ",
    "url": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#python",
    "relUrl": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#python"
  },"170": {
    "doc": "Creating Dummy Variables",
    "title": "R",
    "content": "Turning a categorical variable into a set of dummies . data(iris) # To retain the column of dummies for the first # categorical value we remove the intercept model.matrix(~-1+Species, data=iris) # Then we can add the dummies to the original data iris &lt;- cbind(iris, model.matrix(~-1+Species, data=iris)) # Of course, in a regression we can skip this process summary(lm(Sepal.Length ~ Petal.Length + Species, data = iris)) . If you are only creating one dummy at a time rather than a set from a factor variable, creating a dummy variable doesn’t have to be any different than creating any other variable. Below are several ways to create a new variable in R. dplyr::mutate . Let’s say that we want our dummy to indicate if variable_1 &gt; variable_2. To do this we can use mutate: . # If necessary, install dplyr # install.packages('dplyr') library(dplyr) data(iris) # The below takes existing data (iris) and adds # a new variable (Long.Petal) based on existing variables # (Petal.Length and Petal.Width) and saves the result as # mutated_data. # Note: new variables do not have to be based on old # variables mutated_data = iris %&gt;% mutate(Long.Petal = Petal.Length &gt; Petal.Width) . This will create a new column of logical (TRUE/FALSE) variables. This works just fine for most uses of dummy variables. However if you need the variables to be 1s and 0s you can now take . mutated_data &lt;- mutated_data %&gt;% mutate(Long.Petal = Long.Petal*1) . You could also nest that operation inside the original creation of new_dummy like so: . mutated_data = iris %&gt;% mutate(Long.Petal = (Petal.Length &gt; Petal.Width)*1) . Base R . #the following creates a 5 x 2 data frame letters = c(\"a\",\"b\",\"c\", \"d\", \"e\") numbers = c(1,2,3,4,5) df = data.frame(letters,numbers) . Now I’ll show several different ways to create a dummy indicating if the numbers variable is odd. df$dummy = df$numbers%%2 df$dummy = ifelse(df$numbers%%2==1,1,0) df$dummy = df$numbers%%2==1 # the last one created a logical outcome to convert to numerical we can either df$dummy = df$dummy * 1 # or df$dummy = (df$numbers%%2==1) *1 . ",
    "url": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#r",
    "relUrl": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#r"
  },"171": {
    "doc": "Creating Dummy Variables",
    "title": "MATLAB",
    "content": "Categorical to Dummy . The equivalent of model.matrix() in MATLAB is dummyvar which creates columns of one-hot encoded dummies from categorical variables. The following example is taken from MathWorks documentation. Colors = {'Red';'Blue';'Green';'Red';'Green';'Blue'}; Colors = categorical(Colors); D = dummyvar(Colors) . Other Dummies . In MATLAB you can store variables as columns in arrays. If you know you are going to add columns multiple times to the same array it is best practice to pre-allocate the final size of the array for computational efficiency. If you do this you can simply select the column you are designating for your dummy variable and story the dummys in that column. arr = [1,2,3;5,2,6;1,8,3]; dum = sum(data(:,:),2) &lt;10; data = horzcat(arr,dum); . In the above script I make a 3 by 3 array, then create a 3 x 1 array of dummy variables indicating if the sum of the rows are less than 10. Then I horizontally concatenate the arrays together. I should note that in MATLAB logicals are automatically stored as 1s and 0s instead of T/F like in R. ",
    "url": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#matlab",
    "relUrl": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#matlab"
  },"172": {
    "doc": "Creating Dummy Variables",
    "title": "Stata",
    "content": "In Stata, if we have a categorical variable stored as a number, we can use i. to turn it into a set of dummies, or include it directly in a regression. sysuse auto.dta, clear * Let's get the brand of the car g brand = word(make,1) * Turn it into a numerically coded categorical encode brand, g(brand_n) * include in a regression regress mpg weight i.brand_n * Or create a set of dummies * specifying the prefix so it's easy to refer to * Note this actually does not require * numeric encoding xi i.brand, pre(b_) regress mpg weight b_* * Create a logical variable gen highmpg = mpg &gt; 30 . ",
    "url": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#stata",
    "relUrl": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html#stata"
  },"173": {
    "doc": "Creating Dummy Variables",
    "title": "Creating Dummy Variables",
    "content": " ",
    "url": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html",
    "relUrl": "/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html"
  },"174": {
    "doc": "Creating Time Series Dataset",
    "title": "Introduction",
    "content": "Time-series estimators are, by definition, a function of the temporal ordering of the observations in the estimation sample. So a number of programmed time-series econometric routines can only be used if the software is instructed ahead of time that it is working with a time-series dataset. ",
    "url": "/Time_Series/creating_time_series_dataset.html#introduction",
    "relUrl": "/Time_Series/creating_time_series_dataset.html#introduction"
  },"175": {
    "doc": "Creating Time Series Dataset",
    "title": "Keep in Mind",
    "content": ". | Date data can be notoriously difficult to work with. Be sure before declaring your data set as a time series that your date variable has been imported properly. | As an example, we will use data on U.S. quarterly real Gross Domestic Product (GDP). To get an Excel spreadsheet holding the GDP data, go to the Saint Louis Federal Reserve Bank FRED website. | . ",
    "url": "/Time_Series/creating_time_series_dataset.html#keep-in-mind",
    "relUrl": "/Time_Series/creating_time_series_dataset.html#keep-in-mind"
  },"176": {
    "doc": "Creating Time Series Dataset",
    "title": "Implementations",
    "content": " ",
    "url": "/Time_Series/creating_time_series_dataset.html#implementations",
    "relUrl": "/Time_Series/creating_time_series_dataset.html#implementations"
  },"177": {
    "doc": "Creating Time Series Dataset",
    "title": "Python",
    "content": "pandas supports time series data. Here is an example which downloads quarterly data, casts the date column (read in as an object series) as a datetime series, and creates a year-quarter column. import pandas as pd # Read in data gdp = pd.read_csv(\"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv\") # Convert date column to be of data type datetime64 gdp['DATE'] = pd.to_datetime(gdp['DATE']) # Create a column with quarter-year combinations gdp['yr-qtr'] = gdp['DATE'].apply(lambda x: str(x.year) + '-' + str(x.quarter)) . ",
    "url": "/Time_Series/creating_time_series_dataset.html#python",
    "relUrl": "/Time_Series/creating_time_series_dataset.html#python"
  },"178": {
    "doc": "Creating Time Series Dataset",
    "title": "R",
    "content": "There are many different kinds of time series data set objects in R. Instead of R-based time series objects such as ts, zoo and xts, here we will use tsibble, will preserves time indices as the essential data column and makes heterogeneous data structures possible. The tsibble package extends the tidyverse to temporal data and built on top of the tibble, and so is a data- and model-oriented object. For more detail information for using tsibble such as key and index, check the tsibble page and the Introduction to tsibble. STEP 1) Load necessary packages . # If necessary # install.packages(c('here','tsibble','tidyverse')) library(here) library(tsibble) library(tidyverse) . STEP 2) Import data into R. gdp &lt;- read.csv(\"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv\") # read.csv() has read in our date variable as a factor. We need a date! gdp$DATE &lt;- as.Date(gdp$DATE) # If it were a little less well-behaved than this, we could use the lubridate package to fix it. STEP 3) Convert a date variable formats to quarter . gdp_ts &lt;- as_tsibble(gdp, index = DATE, regular = FALSE) %&gt;% index_by(qtr = ~ yearquarter(.)) . By applying yearmonth() to the index variable (referred to as .), it creates new variable named qtr with a quarter interval which corresponds to the year-quarter for the original variable DATE. Since the tsibble handles regularly-spaced temporal data whereas our data (GDPC1) has an irregular time interval (since it’s not the exact same number of days between quarters every time), we set the option regular = FALSE. Now, we have a quarterly time-series dataset with the new variable date. References for more information: . | If you want to learn how to build various types of time-series forecasting models, Forecasting: Principles and Practice provides very useful information to deal with time-series data in R. | If you need more detail information on tssible, visit the tsibble page or tsibble on RDRR.io. | The fable packages provides a collection of commonly used univariate and multivariate time-series forecasting models. For more information, visit fable. | . ",
    "url": "/Time_Series/creating_time_series_dataset.html#r",
    "relUrl": "/Time_Series/creating_time_series_dataset.html#r"
  },"179": {
    "doc": "Creating Time Series Dataset",
    "title": "Stata",
    "content": "STEP 1) Import Data to Stata . import delimited \"https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv\", clear . STEP 2) Generate the new date variable . generate date_index = tq(1947q1) + _n-1 . The function tq() converts a date variable for each of the above formats to an integer value (starting point of our data is 1947q1). _n is a Stata command gives the index number of the current row. STEP 3) Index the new variable format as quarter . format date_index %tq . This command will format date_index as a vector of quarterly dates which corresponds to our original date variable observation date. STEP 4) Convert a variable into time-series data . tsset date_index . Now, we have a quarterly Stata time-series dataset. Any data you add to this file in the future will be interpreted as time-series data. ",
    "url": "/Time_Series/creating_time_series_dataset.html#stata",
    "relUrl": "/Time_Series/creating_time_series_dataset.html#stata"
  },"180": {
    "doc": "Creating Time Series Dataset",
    "title": "Creating Time Series Dataset",
    "content": " ",
    "url": "/Time_Series/creating_time_series_dataset.html",
    "relUrl": "/Time_Series/creating_time_series_dataset.html"
  },"181": {
    "doc": "Data Manipulation",
    "title": "Data Manipulation Techniques",
    "content": " ",
    "url": "/Data_Manipulation/data_manipulation.html#data-manipulation-techniques",
    "relUrl": "/Data_Manipulation/data_manipulation.html#data-manipulation-techniques"
  },"182": {
    "doc": "Data Manipulation",
    "title": "Data Manipulation",
    "content": " ",
    "url": "/Data_Manipulation/data_manipulation.html",
    "relUrl": "/Data_Manipulation/data_manipulation.html"
  },"183": {
    "doc": "Decision Trees",
    "title": "Keep in Mind",
    "content": ". | While decision trees are easy to interpret and understand, they often underpreform relative to other machine learning methodologies. | Even though they may not offer the best predictions, decision trees excel at identifying key variables in the data. | . ",
    "url": "/Machine_Learning/decision_trees.html#keep-in-mind",
    "relUrl": "/Machine_Learning/decision_trees.html#keep-in-mind"
  },"184": {
    "doc": "Decision Trees",
    "title": "Also Consider",
    "content": ". | Decision trees are the basis for all tree-based methodologies. More robust methods, such as Random Forests, are a collection of decision trees that aggregate their decisions into a single prediction. These forests are often more useful for predictive modeling. | . ",
    "url": "/Machine_Learning/decision_trees.html#also-consider",
    "relUrl": "/Machine_Learning/decision_trees.html#also-consider"
  },"185": {
    "doc": "Decision Trees",
    "title": "Implementations",
    "content": " ",
    "url": "/Machine_Learning/decision_trees.html#implementations",
    "relUrl": "/Machine_Learning/decision_trees.html#implementations"
  },"186": {
    "doc": "Decision Trees",
    "title": "Python",
    "content": "The easiest way to get started with decision trees in Python is to use the scikit-learn package. In the example below, we’ll use data on the passengers of the Titanic to build a classification tree that predicts whether passengers survived or not (binary outcome) based on properties such as passenger age, gender as recorded in the data, and class of cabin. As ever with machine learning, it’s essential that an out-of-sample set, also known as a test set, is retained and used to score the final model. # Install sklearn and pandas using pip or conda, if you don't have them already. # Note that the 'f-strings' used in the print statements below are only available in Python&gt;=3.6. from sklearn import tree from sklearn.model_selection import train_test_split from sklearn.metrics import plot_confusion_matrix import pandas as pd titanic = pd.read_csv(\"https://raw.githubusercontent.com/Evanmj7/Decision-Trees/master/titanic.csv\", index_col=0) # Let's ensure the columns we want to treat as continuous are indeed continuous by using pd.to_numeric # The errors = 'coerce' keyword argument will force any values that cannot be # cast into continuous variables to become NaNs. continuous_cols = ['age', 'fare'] for col in continuous_cols: titanic[col] = pd.to_numeric(titanic[col], errors='coerce') # Set categorical cols &amp; convert to dummies cat_cols = ['sex', 'pclass'] for col in cat_cols: titanic[col] = titanic[col].astype('category').cat.codes # Clean the dataframe. An alternative would be to retain some rows with missing values by giving # a special value to nan for each column, eg by imputing some values, but one should be careful not to # use information from the test set to impute values in the training set if doing this. Strictly speaking, # we shouldn't be dropping the nans from the test set here (as we pretend we don't know what's in it) - but # for the sake of simplicity, we will. titanic = titanic.dropna() # Create list of regressors regressors = continuous_cols + cat_cols # Predicted var y_var = ['survived'] # Create a test (25% of data) and train set train, test = train_test_split(titanic, test_size=0.25) # Now let's create an empty decision tree to solve the classification problem: clf = tree.DecisionTreeClassifier(max_depth=10, min_samples_split=5, ccp_alpha=0.01) # The last option, ccp_alpha, prunes low-value complexity from the tree to help # avoid overfitting. # Fit the tree with the data clf.fit(train[regressors], train[y_var]) # Let's take a look at the tree: tree.plot_tree(clf) # How does it perform on the train and test data? train_accuracy = round(clf.score(train[regressors], train[y_var]), 4) print(f'Accuracy on train set is {train_accuracy}') test_accuracy = round(clf.score(test[regressors], test[y_var]), 4) print(f'Accuracy on test set is {test_accuracy}') # Show the confusion matrix plot_confusion_matrix(clf, test[regressors], test[y_var]) # Although it won't be the same from run to run, this model scored around 80% # out of sample, and has slightly more false positives than false negatives. ",
    "url": "/Machine_Learning/decision_trees.html#python",
    "relUrl": "/Machine_Learning/decision_trees.html#python"
  },"187": {
    "doc": "Decision Trees",
    "title": "R",
    "content": "# Load packages # install.packages(\"pacman\") ## already installed library(pacman) p_load(rpart,rpart.plot,caret,rattle) # We will utilize data regarding passengers on their survival. We have multiple pieces of information on every passenger, including passenger age, sex, cabin number, and class. # Our goal is to build a decision tree that can predict whether or not passengers survived the wreck, making it a classification tree. These same methodologies can be used and applied to a regression tree framework. # Read in the data titanic &lt;- read.csv(\"https://raw.githubusercontent.com/Evanmj7/Decision-Trees/master/titanic.csv\") # Set a seed for reproducability set.seed(1234) # The data is clean for the most part, but some variables have been read in as factors instead of numeric variables, so we can fix that with the following code. titanic$age &lt;- as.numeric(titanic$age) titanic$fare &lt;- as.numeric(titanic$fare) # As with all machine learning methodologies, we want to create a test and a training dataset # Take a random sample of the data, here we have chosen to use 75% for training and 25% for validation samp_size &lt;- floor(0.75*nrow(titanic)) train_index &lt;- sample(seq_len(nrow(titanic)),size=samp_size,replace=FALSE) train &lt;- titanic[train_index, ] test &lt;- titanic[-train_index, ] # Now that we have our test and train datasets, we can build our trees. Here, we will use the package \"rpart\". Other packages, such as \"ranger\" are also viable options. # Here we can pick some variables we think would be good, the tree will decide which ones are best. Some data we have isn't useful, such as an individual's name or the random ID we assigned passengers, so there is no need to include them. basic_tree &lt;- rpart( survived ~ pclass + sex + age + fare + embarked, # our formula data=train, method = \"class\", # tell the model we are doing classification minsplit=2, # set a minimum number of splits cp=.02 # set an optional penalty rate. It is often useful to try out many different ones, use the caret package to test many at once ) basic_tree # plot it using the packages we loaded above fancyRpartPlot(basic_tree,caption=\"Basic Decision Tree\") # This plot gives a very intuitive visual representation on what is going on behind the scenes. # Now we should predict using the test data we left out! predictions &lt;- predict(basic_tree,newdata=test,type=\"class\") # Make the numeric responses as well as the variables that we are testing on into factors predictions &lt;- as.factor(predictions) test$survived &lt;- as.factor(test$survived) # Create a confusion matrix which tells us how well we did. confusionMatrix(predictions,test$survived) # This particular model got ~80% accuracy. This varies each time if you do not set a seed. Much better than a coin toss, but not great. With some additional tuning a decision tree can be much more accurate! Try it for yourself by changing the factors that go into the prediction and the penalty rates. ",
    "url": "/Machine_Learning/decision_trees.html#r",
    "relUrl": "/Machine_Learning/decision_trees.html#r"
  },"188": {
    "doc": "Decision Trees",
    "title": "Decision Trees",
    "content": "Decision trees are among the most common and useful machine learning methodologies. While they are a relatively simple method, they are incredibly easy to understand and implement for both classification and regression problems. A decision tree “grows” by creating a cutoff point (often called a split) at a single point in the data that maximizes accuracy. The tree’s prediction is then based on the mean of the region that results from the input data. For both regression and classification trees, it is important to optimize the number of splits that we allow the tree to make. If there is no limit, the trees would be able to create as many splits as the data will allow. This would mean the tree could perfectly “predict” every value from the training dataset, but would perform terribly out of sample (i.e., overfit the data). As such, it is important to keep a reasonable limit on the number of splits. This is achieved by creating a penalty that the algorithm has to pay in order to perform another split. If the increase in accuracy is worth more than the penalty, it will make the split. For regression trees, the decision to split along a continuum of values is often made by minimizing the residual sum of squares: . \\[minimize \\sum(y-prediction)^2\\] This should be highly reminiscent of ordinary least squares. Where this differs is in the number of splits created, the binary nature of the splits, and its nonlinear nature. The methodology behind classificiation is very similar, except the splits are decided by minimizing purity, such as the Gini index: . \\[G= 1 - \\sum_{i = 1}^{C} (p_{i})^2\\] The goal here is to create regions with as of classifications as possible, as such, a smaller Gini index implies a more pure region. ",
    "url": "/Machine_Learning/decision_trees.html",
    "relUrl": "/Machine_Learning/decision_trees.html"
  },"189": {
    "doc": "Density Plots",
    "title": "Introduction",
    "content": "A density plot visualises the distribution of data over a continuous interval (or time period). Density Plots are not affected by the number of bins (each bar used in a typical histogram) used, thus, they are better at visualizing the shape of the distribution than a histogram unless the bins in the histogram have a theoretical meaning. ",
    "url": "/Presentation/Figures/density_plots.html#introduction",
    "relUrl": "/Presentation/Figures/density_plots.html#introduction"
  },"190": {
    "doc": "Density Plots",
    "title": "Keep in Mind",
    "content": ". | Notice that the variable on the x-axis should be continuous. Density plots are not designed for use with discrete variables. | . ",
    "url": "/Presentation/Figures/density_plots.html#keep-in-mind",
    "relUrl": "/Presentation/Figures/density_plots.html#keep-in-mind"
  },"191": {
    "doc": "Density Plots",
    "title": "Also Consider",
    "content": "= You might also want to know how to make a histogram or a line graph, click Histogram or Line graph for more information. ",
    "url": "/Presentation/Figures/density_plots.html#also-consider",
    "relUrl": "/Presentation/Figures/density_plots.html#also-consider"
  },"192": {
    "doc": "Density Plots",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Figures/density_plots.html#implementations",
    "relUrl": "/Presentation/Figures/density_plots.html#implementations"
  },"193": {
    "doc": "Density Plots",
    "title": "R",
    "content": "For this R demonstration, we are going to use ggplot2 package to create a density plot. Additionally, we will use the dataset diamonds that is natively available in R. To begin with this R demonstration, make sure that we install and load all the useful packages that we need it. We use the function p_load() in pacman package to help us to install and load the packages at one time. #install and load necessary packages if (!require(\"pacman\")) install.packages(\"pacman\") pacman::p_load(ggplot2,viridis,RColorBrewer,tidyverse,ggthemes,ggpubr,datasets) . Next, in order to make a density plot, we are going to use the ggplot() and geom_density() functions. We will specify price as our x-axis. ggplot(diamonds, aes(x=price))+ geom_density() . We can always change the color of the density plot using the col argument and fill the color inside the density plot using fill argument. Furthermore, we can specify the degree of transparency density fill area using the argument alpha where alpha ranges from 0 to 1. ggplot(diamonds, aes(x=price))+ geom_density(fill=\"lightblue\", col='black', alpha=0.6) . We can also change the type of line of the density plot as well by adding linetype= inside geom_density(). ggplot(diamonds, aes(x=price))+ geom_density(fill=\"lightblue\", col='black', linetype=\"dashed\") . Furthermore, you can also combine both histogram and density plots together. ggplot(diamonds, aes(x=price)) + geom_histogram(aes(y=..density..), colour=\"black\", fill=\"grey45\")+ geom_density(col=\"red\",size=1,linetype='dashed') . What happen if we want to make multiple densities? . For example, we want to make multiple densities plots for price based on the type of cut, all we need to do is adding fill=cut inside aes(). ggplot(data=diamonds, aes(x=price,fill=cut)) + geom_density(adjust=1.5, alpha=.3) . : . ",
    "url": "/Presentation/Figures/density_plots.html#r",
    "relUrl": "/Presentation/Figures/density_plots.html#r"
  },"194": {
    "doc": "Density Plots",
    "title": "Density Plots",
    "content": " ",
    "url": "/Presentation/Figures/density_plots.html",
    "relUrl": "/Presentation/Figures/density_plots.html"
  },"195": {
    "doc": "Desired Nonexistent Pages",
    "title": "Desired Nonexistent Pages",
    "content": "This is a manually-maintained list of pages that have been linked from elsewhere, or that it would be nice to have, but do not exist yet. Feel free to edit with your own wishes! . A page does not have to be listed here for you to add it! These are just the things that we thought of. There’s certainly plenty more in the world of statistical techniques to be added. If you create one of these pages, please remove it from this list. ",
    "url": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html",
    "relUrl": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html"
  },"196": {
    "doc": "Desired Nonexistent Pages",
    "title": "Data Manipulation",
    "content": ". | Aggregating Statistics | . ",
    "url": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#data-manipulation",
    "relUrl": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#data-manipulation"
  },"197": {
    "doc": "Desired Nonexistent Pages",
    "title": "Geo-Spatial",
    "content": ". | Handling Raster Data | Handling Vector Data | Spatial Regression Model | . ",
    "url": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#geo-spatial",
    "relUrl": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#geo-spatial"
  },"198": {
    "doc": "Desired Nonexistent Pages",
    "title": "Model Estimation",
    "content": "OLS . GLS . | Nonlinear Instrumental Variables Estimation | Nonparametric regression | Generalized Method of Moments (GMM) | Tobit | Ordered Probit | Ordered Logit | Conditional Logit | Mixed Logit | Nested Logit | Endogenous Switching Model | Nonparametric Sample Selection Models | . Multilevel Models . | Hierarchical Bayes Conditional Logit | Multilevel Regression with Poststratification | Nonlinear Mixed Effects Models | . Research Design . | Differences in Differences with treatment rollout (use updated methods!) | Propensity Score Matching | Event Study Estimation | Synethic Control Method | . Statistical Inference . | Average Marginal Effects | Marginal Effects at the Mean | Nonlinear hypothesis tests | Treatment Effect Model | . Nonstandard Errors . | Cluster Bootstrap Standard Errors | Jackknife Standard Errors | . ",
    "url": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#model-estimation",
    "relUrl": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#model-estimation"
  },"199": {
    "doc": "Desired Nonexistent Pages",
    "title": "Machine Learning",
    "content": ". | A-B Testing | Artificial Neural Networks | Nearest Neighbors Matching | . ",
    "url": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#machine-learning",
    "relUrl": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#machine-learning"
  },"200": {
    "doc": "Desired Nonexistent Pages",
    "title": "Presentation",
    "content": "Figures . | Marginal Effects Plots for Discrete Variables | Histograms | Graph themes | . Tables . | Summary Statistics by Group | Cross-Tabulations | Correlation Matrix | . ",
    "url": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#presentation",
    "relUrl": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#presentation"
  },"201": {
    "doc": "Desired Nonexistent Pages",
    "title": "Time Series",
    "content": ". | Serial Correlation | Stationarity and Weak Dependence | Granger Causality | Moving Average Model | ARIMA Model | ARIMAX Model | ARCH Model | GARCH Model | TARCH Model | Rolling Regressions | Recursive Regressions | State Dependent Regression | Structural Break (Chow Test) | Dynamic Panel | Ex-Post Forecasting | Ex-Ante Forecasting | . ",
    "url": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#time-series",
    "relUrl": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#time-series"
  },"202": {
    "doc": "Desired Nonexistent Pages",
    "title": "Other",
    "content": ". | Import a Delimited Data File (CSV, TSV) | Import a Fixed-Width Data File | Export Data to a Foreign Format | Set a Working Directory | Power Analysis of Interaction Terms | . ",
    "url": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#other",
    "relUrl": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html#other"
  },"203": {
    "doc": "Determine the Observation Level of a Data Set",
    "title": "Determine the Observation Level of a Data Set",
    "content": "The observation level of a data set is the set of case-identifying variables which, in combination, uniquely identify every row of the data set. For example, in the data set . | I | J | X | . | 1 | 1 | 3 | . | 1 | 2 | 3.5 | . | 2 | 1 | 2 | . | 2 | 2 | 4.5 | . the variables \\(I\\) and \\(J\\) uniquely identify rows. The first row has \\(I = 1\\) and \\(J = 1\\), and there is no other row with that combination. We could also say that \\(X\\) uniquely identifies rows, but in this example \\(X\\) is not a case-identifying variable, it’s actual data. When working with data that has case-identifier variables, like panel data, it’s generally a good idea to know what set of them makes up the observation level of a data set. Otherwise you might perform merges or case-level calculations incorrectly. ",
    "url": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html",
    "relUrl": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html"
  },"204": {
    "doc": "Determine the Observation Level of a Data Set",
    "title": "Keep in Mind",
    "content": ". | As in the above example, it’s easy to uniquely identify rows using continuous data. But the goal is to figure out which case-identifying variables, like an individual’s ID code, or a country code, or a time code, uniquely identify rows. Make sure you only try these variables. | Even if you think you know what the observation level is, it’s good to check. Lots of data is poorly behaved! | . ",
    "url": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html#keep-in-mind",
    "relUrl": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html#keep-in-mind"
  },"205": {
    "doc": "Determine the Observation Level of a Data Set",
    "title": "Also Consider",
    "content": ". | You can collapse a data set to switch from one observation level to another, coarser one. | . ",
    "url": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html#also-consider",
    "relUrl": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html#also-consider"
  },"206": {
    "doc": "Determine the Observation Level of a Data Set",
    "title": "Implementations",
    "content": " ",
    "url": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html#implementations",
    "relUrl": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html#implementations"
  },"207": {
    "doc": "Determine the Observation Level of a Data Set",
    "title": "Python",
    "content": "To check for duplicate rows when using pandas dataframes, you can call duplicated or, to omit the duplicates, drop_duplicates. # Use conda or pip to install pandas if you don't already have it installed import pandas as pd storms = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/dplyr/storms.csv') # Find the duplicates by name, year, month, day, and hour level_variables = ['name', 'year', 'month', 'day', 'hour'] storms[storms.duplicated(subset=level_variables)] # Drop these duplicates, but retain the first occurrence of each storms = storms.drop_duplicates(subset=level_variables, keep='first') . ",
    "url": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html#python",
    "relUrl": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html#python"
  },"208": {
    "doc": "Determine the Observation Level of a Data Set",
    "title": "R",
    "content": "# If necessary, install dplyr # install.packages('dplyr') # We do not need dplyr to detect the observation level # But we will use it to get data, and for our alternate approach library(dplyr) # Get data on storms from dplyr data(\"storms\") # Each storm should be identified by # name, year, month, day, and hour # anyDuplicated will return 0 if there are no duplicate combinations of these # so if we get 0, the variables in c() are our observation level. anyDuplicated(storms[,c('name','year','month','day','hour')]) # We get 2292, telling us that row 2292 is a duplicate (and possibly others!) # We can pick just the rows that are duplicates of other rows for inspection # (note this won't get the first time that duplicate shows up, just the subsequent times) duplicated_rows &lt;- storms[duplicated(storms[,c('name','year','month','day','hour')]),] # Alternately, we can use dplyr storms %&gt;% group_by(name, year, month, day, hour) %&gt;% # Add a variable with the number of times that particular combination shows up mutate(number_duplicates = n()) %&gt;% # Then take that variable out pull(number_duplicates) %&gt;% # And get the maximum of it max() # If the result is 1, then we have found the observation level. If not, we have duplicates. # We can pick out the rows that are duplicated for inspection # by filtering on n(). This approach will give you every time the duplicate appears. duplicated_rows &lt;- storms %&gt;% group_by(name, year, month, day, hour) %&gt;% # Add a variable with the number of times that particular combination shows up filter(n() &gt; 1) . ",
    "url": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html#r",
    "relUrl": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html#r"
  },"209": {
    "doc": "Determine the Observation Level of a Data Set",
    "title": "Stata",
    "content": "* Load surface.dta, which contains temperature recordings in different locations sysuse surface.dta, clear * duplicates report followed by a variable list will show how many times * each combination shows up. * I think there is one observation level for each location, so I'll check that duplicates report latitude longitude * If I am correct, then the only number in the \"Copies\" column will be 1. * But it looks like I was not correct. * duplicates tag will create a binary variable with 1 for all duplicates * so I can examine the problem more closely * (duplicates examples is another option) duplicates tag latitude longitude, g(duplicated_data) * If I want to know not just whether there are duplicates but how many * of each there are for when I look more closely, I can instead do by latitude longitude, sort: g number_of_duplicates_in_this_group = _N . For especially large datasets the Gtools version of the various duplicates commands, gduplicates, is a great option . * Install gtools if necessary * ssc install gtools * Recreate the two duplicates tasks from above gduplicates report latitude longitude gduplicates tag latitude longitude, g(g_duplicated_data) . ",
    "url": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html#stata",
    "relUrl": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html#stata"
  },"210": {
    "doc": "Faceted Graphs",
    "title": "Faceted Graphs",
    "content": "When plotting relationship among variables of interest, one of the useful ways to create visual impact is by way of using facet, which subsets data with faceted variable and creates plots for each of the subset seperately. The result is a panel of subplots, with each subplot depicting the plot for same set of variables. This approach can be especially useful for panel datasets, with the panel variable acting as facet variable and each subplot depicting time series trend of variable of interest. ",
    "url": "/Presentation/Figures/faceted_graphs.html",
    "relUrl": "/Presentation/Figures/faceted_graphs.html"
  },"211": {
    "doc": "Faceted Graphs",
    "title": "Keep in Mind",
    "content": ". | It is important to use a categorical (discrete) variable as a facet variable for creating faceted graphs. | Plotting libraries generally fall into two broad camps: imperative (specify all of the steps to get the desired outcome) or declarative (specify the desired outcome without the steps). Imperative plotting gives more control and some people may find each step clearer to read, but it can also be fiddly and cumbersome, especially with simple plots. Declarative plotting trades away control in favour of tried and tested processes that can quickly produce standardised charts, but the specialised syntax can be a barrier for newcomers. Facets are available in both types, but the code to produce them will look quite different. | . ",
    "url": "/Presentation/Figures/faceted_graphs.html#keep-in-mind",
    "relUrl": "/Presentation/Figures/faceted_graphs.html#keep-in-mind"
  },"212": {
    "doc": "Faceted Graphs",
    "title": "Also Consider",
    "content": ". | It is important to know the basic plotting techniques such as Bar Graphs, Line Graphs and Scatterplot before learning about faceted graphs as the facets are an addition to the underlying plot such as bar graph, line graph, scatterplot etc. | . ",
    "url": "/Presentation/Figures/faceted_graphs.html#also-consider",
    "relUrl": "/Presentation/Figures/faceted_graphs.html#also-consider"
  },"213": {
    "doc": "Faceted Graphs",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Figures/faceted_graphs.html#implementations",
    "relUrl": "/Presentation/Figures/faceted_graphs.html#implementations"
  },"214": {
    "doc": "Faceted Graphs",
    "title": "Python",
    "content": "Python has both imperative and declarative plotting libraries, and many of them have support for faceted graphs. In the example below, we’ll look at three libraries: two declarative, seaborn and plotnine, and one imperative, matplotlib. The imperative library matplotlib is by far the most popular plotting tool in Python, having been used as part of efforts to detect gravitational waves and produce the first image of a black hole. Often, other libraries build upon it as a foundation. For plots that will be shown on the web, the declarative library altair has very good facet support. Because matplotlib is imperative, it takes more effort by the user to produce a simple facet chart. So the first example below will use seaborn, a declarative library that builds on matplotlib. We’ll use it to produce a plot from the Penguins dataset, with a facet for each island. As ever, you may need to conda or pip install the libraries used in the examples. import seaborn as sns # Load the example Penguins dataset df = sns.load_dataset(\"penguins\") # Plot a scatter of bill properties with # columns (facets) given by island and colour # given by the species of Penguin sns.relplot(x=\"bill_depth_mm\", y=\"bill_length_mm\", hue=\"species\", col=\"island\", alpha=.5, palette=\"muted\", data=df) . Results in: . If you have used R for plotting, you might be familiar with the ggplot package. plotnine is another declarative plotting library in Python that is inspired by the API for ggplot in R. from plotnine import * from plotnine.data import mtcars (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')) + geom_point() + stat_smooth(method='lm') + facet_wrap('~gear')) . Results in: . For more complex charts, where you want full control over facet placement, the imperative library matplotlib has a wealth of options. The code for a simple facet plot using synthetic data is: . import matplotlib.pyplot as plt import numpy as np # Some example data to display x = np.linspace(0, 2 * np.pi, 400) y = np.sin(x ** 2) fig, (ax1, ax2) = plt.subplots(2, sharex=True) fig.suptitle('Two sine waves') ax1.plot(x, y) ax2.scatter(x + 1, -y, color='red') . (NB: no figure shown in this case.) Note how everything is specified. While plt.subplots(nrows, ncols, ...) allows for a rectangular facet grid, even more complex facets can be constructed using the mosaic option in matplotlib version 3.3.0+. The arrangment of facets can be specified either through text, as in the example below, or with lists of lists: . import matplotlib.pyplot as plt axd = plt.figure(constrained_layout=True).subplot_mosaic( \"\"\" TTE L.E \"\"\") for k, ax in axd.items(): ax.text(0.5, 0.5, k, ha='center', va='center', fontsize=36, color='darkgrey') . Results in: . ",
    "url": "/Presentation/Figures/faceted_graphs.html#python",
    "relUrl": "/Presentation/Figures/faceted_graphs.html#python"
  },"215": {
    "doc": "Faceted Graphs",
    "title": "R",
    "content": "Implementation of faceted graph in R explained below is taken from online book R for Data Science by Hadley Wickham and Garett Grolemund. The book is also an excellent source for various data visualization techniques in R and learning R in general. We will use tidyverse package available in R for faceted graphs. Tidyverse is actually a meta-package which has various packages, and we will use ggplot2 package for our purpose. This package has a data frame (it is like a table in R), called ‘mpg’ which contains observations collected by the US Environmental Protection Agency on 38 models of car. To create faceted graph, use facet_wrap() option in ggplot. The argument inside the bracket is ~ sign follwed by the categorical variable to be used to create subsets of data. Its use is illustrated in the code given below. #Install package, if not already installed. install.packages(\"tidyverse\") #Load the package library(tidyverse) # Now, we will create faceted graph, with variable 'displ' (a car's engine size) on # x-axis and variable 'hwy (car's fuel efficiency on highway) on y-axis. We will use # `facet_wrap(~class)` option to created faceted graph. The variable 'class' denotes # type of car. We use 'geom_point()` to create a scatterplot. ggplot(data = mpg)+ geom_point(mapping = aes(x = displ, y = hwy))+ facet_wrap(~class) . The above set of code results in the following panel of subplots: . Additionally, one can create faceted graph using two variables with facet_grid(). Inside the bracket, use two variables seperated by ~. The example of the same using ‘mpg’ dataframe and two variables ‘drv’ (whether it’s front wheel, rear wheel or 4wd) and ‘cyl’ (number of cylinders) is given below. ggplot(data = mpg)+ geom_point(mapping = aes(x = displ, y = hwy))+ facet_grid(drv ~ cyl) . The code reults in the follwing panel of subplots: . ",
    "url": "/Presentation/Figures/faceted_graphs.html#r",
    "relUrl": "/Presentation/Figures/faceted_graphs.html#r"
  },"216": {
    "doc": "Faceted Graphs",
    "title": "Stata",
    "content": "In stata, faceted graph can be created by using option by() and mentioning the faceted variable in the bracket. Let’s see an example of the same . Let’s access pre-installed dataset in Stata, called ‘auto.dta’ which has 1978 automobile data. The following code generates scatterplot with ‘length of car’ on x-axis, ‘mileage of car’ on y-axis and variable ‘foreign’ (whether the car is manufactured domestically or imported) used to create subsets of data. sysuse auto is used to load the table ‘auto.dta’. sysuse auto twoway (scatter mpg length), by(foreign) . The code generates the following graph: . ",
    "url": "/Presentation/Figures/faceted_graphs.html#stata",
    "relUrl": "/Presentation/Figures/faceted_graphs.html#stata"
  },"217": {
    "doc": "Fixed Effects in Linear Regression",
    "title": "Fixed Effects in Linear Regression",
    "content": "Fixed effects is a statistical regression model in which the intercept of the regression model is allowed to vary freely across individuals or groups. It is often applied to panel data in order to control for any individual-specific attributes that do not vary across time. For more information, see Wikipedia: Fixed Effects Model. ",
    "url": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html",
    "relUrl": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html"
  },"218": {
    "doc": "Fixed Effects in Linear Regression",
    "title": "Keep in Mind",
    "content": ". | To use individual-level fixed effects, you must observe the same person multiple times (panel data). | In a linear regression context, fixed effects regression is relatively straightforward, and can be thought of as effectively adding a binary control variable for each individual, or subtracting the within-individual mean of each variable (the “within” estimator). However, you may want to apply fixed effects to other models like logit or probit. This is usually possible (depending on the model), but if you just add a set of binary controls or subtract within-individual means, it won’t work very well. Instead, look for a command specifically designed to implement fixed effects for that model. | If you are using fixed effects to estimate the causal effect of a variable \\(X\\), individuals with more variance in \\(X\\) will be weighted more heavily (Gibbons, Serrano, &amp; Urbancic 2019, ungated copy here). You may want to consider weighting your regression by the inverse within-individual variance of \\(X\\). | . ",
    "url": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html#keep-in-mind",
    "relUrl": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html#keep-in-mind"
  },"219": {
    "doc": "Fixed Effects in Linear Regression",
    "title": "Also Consider",
    "content": ". | Instead of fixed effects you may want to use random effects, which requires additional assumptions but is statistically more efficient and also allows the individual effect to be modeled using covariates. See Linear Mixed-Effects Regression | You may want to consider clustering your standard errors at the same level as (some or more of) your fixed effects. | . ",
    "url": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html#also-consider",
    "relUrl": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html#also-consider"
  },"220": {
    "doc": "Fixed Effects in Linear Regression",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html#implementations",
    "relUrl": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html#implementations"
  },"221": {
    "doc": "Fixed Effects in Linear Regression",
    "title": "Julia",
    "content": "Julia provides support for estimating high-dimensional fixed effect models through the FixedEffectModels.jl package (link). Similarly to felm (R) and reghdfe (Stata), the package uses the method of alternating projections to sweep out fixed effects. However, the Julia implementation is typically quite a bit faster than these other two methods. It also offers further performance gains via GPU computation for users with a working CUDA installation (up to an order of magnitude faster for complicated problems). # If necessary, install JuliaFixedEffects.jl and some ancilliary packages for reading in the data # ] add JuliaFixedEffects, CSVFiles, DataFrames # Read in the example CSV and convert to a data frame using CSVFiles, DataFrames df = DataFrame(load(\"https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv\")) # Calculate proportion of graduates working df[!, :prop_working] = df[!, :count_working] ./ (df[!, :count_working ] .+ df[!, :count_not_working]) using JuliaFixedEffects # Regress median earnings on the proportion of working graduates. # We'll control for institution name and year as our fixed effects. # We'll also cluster our standard errors by institution name. reg(df, @formula(earnings_med ~ prop_working + fe(inst_name) + fe(year)), Vcov.cluster(:inst_name)) # Multithread example Threads.nthreads() ## See: https://docs.julialang.org/en/v1.2/manual/parallel-computing/#man-multithreading-1 reg(df, @formula(earnings_med ~ prop_working + fe(inst_name) + fe(year)), Vcov.cluster(:inst_name), method = :lsmr_threads) # GPU example (requires working CUDA installation) reg(df, @formula(earnings_med ~ prop_working + fe(inst_name) + fe(year)), Vcov.cluster(:inst_name), method = :lsmr_gpu) . ",
    "url": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html#julia",
    "relUrl": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html#julia"
  },"222": {
    "doc": "Fixed Effects in Linear Regression",
    "title": "R",
    "content": "There are numerous packages for estimating fixed effect models in R. We will limit our examples here to the two fastest implementations — lfe::felm and fixest::feols — both of which support high-dimensional fixed effects and standard error correction (multiway clustering, etc.). We first demonstrate fixed effects in R using felm from the lfe package (link). lfe::felm uses the Method of Alternating Projections to “sweep out” the fixed effects and avoid estimating them directly. By default, this is automatically done in parallel, using all available cores on a user’s machine to maximize performance. (It is also possible to change this behaviour.) . # If necessary, install lfe # install.packages('lfe') library(lfe) # Read in data from the College Scorecard df &lt;- read.csv('https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv') # Calculate proportion of graduates working df$prop_working &lt;- df$count_working/(df$count_working + df$count_not_working) # A felm formula is constructed as: # outcome ~ # covariates | # fixed effects | # instrumental variables specification | # cluster variables for standard errors # Here let's regress earnings_med on prop_working # with institution name and year as our fixed effects # And clusters for institution name felm_model &lt;- felm(earnings_med ~ prop_working | inst_name + year | 0 | inst_name, data = df) # Look at our results summary(felm_model) . Next, we consider feols from the fixest package (link). The syntax is very similar to lfe::felm and again the estimation will be done in parallel by default. However, rather than the method of alternating projections, fixest::feols uses a concentrated maximum likelihood method to efficiently estimate models with an arbitrary number of fixed effects. Current benchmarks suggest that this can yield significant speed gains, especially for large problems. For the below example, we’ll continue with the same College Scorecard dataset already loaded into memory. # If necessary, install fixest # install.packages('fixest') library(fixest) # Run the same regression as before feols_model &lt;- feols(earnings_med ~ prop_working | inst_name + year, data = df) # Look at our results # Standard errors are automatically clustered at the inst_name level summary(feols_model) # It is also possible to specify additional or different clustering of errors summary(feols_model, se = \"twoway\") summary(feols_model, cluster = c(\"inst_name\", \"year\")) ## same as the above . As noted above, there are numerous other ways to implement fixed effect models in R. Users may also wish to look at the plm, lme4, and estimatr packages among others. For example, the latter’s estimatr::lm_robust function provides syntax that may be more familar syntax to new R users who are coming over from Stata. Note, however, that it will be less efficient for complicated models. ",
    "url": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html#r",
    "relUrl": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html#r"
  },"223": {
    "doc": "Fixed Effects in Linear Regression",
    "title": "Stata",
    "content": "We will estimate fixed effects using Stata in two ways. First, using the built in xtreg command. Second, using the reghdfe package (link), which is more efficient and better handles multiple levels of fixed effects (as well as multiway clustering), but must be downloaded from SSC first. * Load in College Scorecard data import delimited \"https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv\", clear * The missings are written as \"NA\", let's turn this numeric destring count_not_working count_working earnings_med, replace force * Calculate the proportion working g prop_working = count_working/(count_working + count_not_working) * xtset requires that the individual identifier be a numeric variable encode inst_name, g(name_number) * Set the data as panel data with xtset xtset name_number * Use xtreg with the \"fe\" option to run fixed effects * Regressing earnings_med on prop_working * with fixed effects for name_number (implied by fe) * and also year (which we'll add manually with i.year) * and standard errors clustered by name_number xtreg earnings_med prop_working i.year, fe vce(cluster name_number) * Now, let's demonstrate the same regression with reghdfe. * Install the package first if necessary. * ssc install reghdfe * For reghdfe we don't need to xtset the data. Let's undo that xtset, clear * We specify both sets of fixed effects in absorb() reghdfe earnings_med prop_working, absorb(name_number year) vce(cluster inst_name) . ",
    "url": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html#stata",
    "relUrl": "/Model_Estimation/OLS/fixed_effects_in_linear_regression.html#stata"
  },"224": {
    "doc": "Formatting Graph Legends",
    "title": "Formatting Graph Legends",
    "content": "Graph legends are an important part of data representation, as they enable the reader to better under stand the figure infront of them while not cluttering the figure itself. Legends can often be seen next to graphs describing some of aspect of a graph. For more information on graphs see other articles such as: bar graphs, scatter plots and histograms. ",
    "url": "/Presentation/Figures/formatting_graph_legends.html",
    "relUrl": "/Presentation/Figures/formatting_graph_legends.html"
  },"225": {
    "doc": "Formatting Graph Legends",
    "title": "Keep in Mind",
    "content": ". | Title: A legend should be well-titled so you can tell what the different component elements, like any key symbols, colors, scale changes, lines or other compeents mean. | Some more extended legends cal also include very brief information about methods or results, as simplified as possible. | Avoid clutter: It is important to keep these legends simple, an effective legend is there to help the graph stand alone. | Ask if a legend is necessary. You may be able to move labels for things like colors or shapes onto the graph itself where it is more closely tied to the data. Or you may be able to make things self-explanatory with a good graph title. | . ",
    "url": "/Presentation/Figures/formatting_graph_legends.html#keep-in-mind",
    "relUrl": "/Presentation/Figures/formatting_graph_legends.html#keep-in-mind"
  },"226": {
    "doc": "Formatting Graph Legends",
    "title": "Also Consider",
    "content": ". | Before finalizing your legend the graph itself should be completed. Deciding which type of graph is best suited for your data is a whole topic in itself. | Some of the graphical implementations this page contains include are; Bar Graphs, Histograms, and Scatterplot by Group on Shared Axes. | Check out this article for a brief description of the relationships shown for different graph types . | . ",
    "url": "/Presentation/Figures/formatting_graph_legends.html#also-consider",
    "relUrl": "/Presentation/Figures/formatting_graph_legends.html#also-consider"
  },"227": {
    "doc": "Formatting Graph Legends",
    "title": "Graphs legends in R",
    "content": "Here are some of the packages that will be used in this section: . if (!require(\"pacman\")) install.packages(\"pacman\") pacman::p_load(ggplot2) . The dataset used in this article will be the mtcars dataset as it comes with base R. The functions here are ggplot and geom_point from the ggplot2 package. geom_point creates a scatterplot while in ggplot we will assign the axis and the legend. More specifically, in aes() we color the point depending on the number of cylinders. fig1 &lt;- ggplot(mtcars, aes(wt, mpg, colour = factor(cyl))) + geom_point() fig1 . Now we can add another layor by giving each type of transition a different shape through shape in aes(). Additionally, you can augment the labels for both colour and shape with labs(). fig2 &lt;- ggplot(mtcars, aes(wt, mpg, colour = factor(cyl), shape = factor(am) )) + geom_point() fig2 + labs(colour= \"Number of Cylinders\", shape = \"Transmission Type\") . To change the legend position use the theme() modifier in ggplot. From there you can choose top, right, bottom, left, or none (removes the legend). To put the legends inside the plot create column vector of size 2 (the first value refers to the x coordinate. while the second refers to the y) where both elements are between 0 and 1. To ensure that the whole legends is within the graph use the legend.justification to set the corner where you want the legend. fig2 + theme( legend.position = c(.95, .95), legend.justification = c(\"right\", \"top\") ) . There are other cool things you can do to the legend to better customize the visual experience by adding more to the theme modifier such as: . | Changing the margin size with legend.box.margin | The color of the box around the legend with legend.box.background | Changing the font size and color with legend.text | Changing the boxes of the legend key with legend.key fig3 &lt;- fig2 + theme( legend.box.background = element_rect(color=\"red\", size=2), legend.box.margin = margin(116, 6, 6, 6), legend.key = element_rect(fill = \"white\", colour = \"black\"), legend.text = element_text(size = 8, colour = \"red\") ) fig3 . | . Sometimes you may want to remove a legend for the sake of graph readability or to reduce clutter. You can remove a legend by changing its position to “none” within the theme modifier. fig4 &lt;- fig2 + theme(legend.postion=\"none\") fig4 . You can alternately remove legends (or components of legends) with guides . # Here we've removed the color legend, but the shape legend is still there. fig5 &lt;- fig2 + guides(color = FALSE) fig5 # This removes both fig6 &lt;- fig2 + guides(color = FALSE, shape = FALSE) fig6 . ",
    "url": "/Presentation/Figures/formatting_graph_legends.html#graphs-legends-in-r",
    "relUrl": "/Presentation/Figures/formatting_graph_legends.html#graphs-legends-in-r"
  },"228": {
    "doc": "Formatting Graph Legends",
    "title": "Legend Formatting in Stata",
    "content": "In Stata the legend is automatically added when you create a graph with multiple lines. However there may be instances where you would still prefer to have a legend on a single line graph or possibly no legend on your graph in general. For those times just add legend(on) or legend(off). * Let's use the US Life Expectancy data that comes with Stata sysuse uslifeexp.dta, clear line le year, legend(on) . Labelling legends is also straight forward, within the legend function add label(n \"line title\") to augment the titles, where ‘n’ is the nth group. line le_m le_f year, legend(label(1 \"Males\") label(2 \"Females\")) . Legend position can be changed with the position(), col(), and ring() modifiers in the legend() function. Here position refers to where the legend is placed with respect to the center of the graph using clock directions (i.e. 6 is 6 o’clock or below and 3 is 3 o’clock or right). col() is the number of columns the legend will take up, usually you just want to set the to be 1 to prevent the graph and legend fighting over space. ring() refers to the distance away from the center of the graph. line le_m le_f year, legend(pos(3) col(1) lab(1 \"Males\") lab(2 \"Females\") stack) . Then you can add some color to the legend with region() and a title with subtitle(): . line le_m le_f year, legend(pos(5) ring(0) col(1) lab(1 \"Males\") lab(2 \"Females\") region(fcolor(gs15))) legend(subtitle(\"Legend\")) . In the case where you are dealing with a two-way or a combination of graphs, augmenting the legend is exactly like above only now you must specify the line which you want to augment. Take this example from the stata manual on two-way graphs: . line le_wm year, yaxis(1 2) xaxis(1 2) /// || line le_bm year /// || line diff year /// || lfit diff year /// ||, /// ylabel(0(5)20, axis(2) gmin angle(horizontal)) /// ylabel(0 20(10)80, gmax angle(horizontal)) /// ytitle(\"\", axis(2)) /// xlabel(1918, axis(2)) xtitle(\"\", axis(2)) /// ylabel(, axis(2) grid) /// ytitle(\"Life expectancy at birth (years)\") /// title(\"White and black life expectancy\") /// subtitle(\"USA, 1900-1999\") /// note(\"Source: National Vital Statistics, Vol 50, No. 6\" /// \"(1918 dip caused by 1918 Influenza Pandemic)\") /// legend(label(1 \"White males\") label(2 \"Black males\")) /// legend(col(1) pos(3)) . Notice within the label() command nested within legend() you must specify which part of the graph you’re labeling first (the number denotes its order i.e. the first graph is 1) then the new label with a space between both items. Here they change the legend label for the first 2 lines. In regards to legend positioning, the same rules discussed above apply. #Sources Stata’s manual on two-way graphs: https://www.stata.com/manuals13/g-2graphtwowayline.pdf Stata’s manual on legends: https://www.stata.com/manuals13/g-3legend_options.pdf . ",
    "url": "/Presentation/Figures/formatting_graph_legends.html#legend-formatting-in-stata",
    "relUrl": "/Presentation/Figures/formatting_graph_legends.html#legend-formatting-in-stata"
  },"229": {
    "doc": "Geocoding",
    "title": "Geocoding",
    "content": "Geocoding is taking an address (e.g. 1600 Pennsylvania Ave NW, Washington DC 20500) or a name of a place (e.g. The White House) and turning it into a geographic position on the earth’s surface. Commonly, the cooridinate system is longitude and latitude but there are other potential coordinate systems that can be used. There are many different types of locations one can geocode including: . | Cities | Landmarks | Geographic Locations * Mountains * Rivers | Addresses * Street Intersections * House Numbers with street names * Postal Codes | . There are multiple ways to geocode. For instance, you could find the corrdinates of the Empire State building by flying to New York, riding an elevator to the top of the building, and then using your GPS to get the latitiude and longitude of where you were standing. A much more efficient way of geocoding is through interpolation. Interpolation uses other known geocoded locations to estimate the coordinates of the data that you wish to geocode. A computer uses an algorithm and the closest known geocodes to conduct this interpolation. However, the farther the “closest” known geocodes are to the data you are trying to geocode the less accurate the geocoding process is. smartystreets, a geocode platform, has a good explanation of this. Additionally, Reverse Geocoding takes a latitude-longitude pair (or other global coordinates) and converts it into an address or a place. Depending on the data that is available reverse geocoding can be very useful. Similar to regular geocoding, reverse geocoding uses other known reverse geocoded locations to estimate the address of the inputted coordinates. ",
    "url": "/Geo-Spatial/geocoding.html",
    "relUrl": "/Geo-Spatial/geocoding.html"
  },"230": {
    "doc": "Geocoding",
    "title": "The Geocoding Process",
    "content": "Whenever you geocode data there is a 3 step process that is undergone: . | Step 1: Input Data Descriptive or textual data is inputted to yield a desired corresponding spatial data | Step 2: Classification of Input Data Input data is sorted into two groups relative input data and absolute data | Step 3A: Relative Input Data Relative input data is the non-preferred type of data (most geocoding reject relative input data). Relative data are textual descriptions of locations that cannot be converted into precise spatial data on their own. Instead they are dependent on a other reference locations. For example, “across the street from the White House” has to use “the White House” as a reference point and then deduce what “across the street” means. | Step 3B: Absolute Data This is the sweet sweet data that geocoding platforms love. A spatial coordinate (lon, lat) can be defined for this data independently of other reference points. Examples: USPS ZIP codes; complete and partial postal addresses; PO boxes; cities; counties; intersections; and named places | . ",
    "url": "/Geo-Spatial/geocoding.html#the-geocoding-process",
    "relUrl": "/Geo-Spatial/geocoding.html#the-geocoding-process"
  },"231": {
    "doc": "Geocoding",
    "title": "Why is geocoding helpful?",
    "content": "Odds are if you are on this page then you already have a reason to use geocoding, but here is a brief motivation for how geocoding can help with a project. Geocoding is helpful when you want to do spatial work. For example, maybe you have data on voter addresses and want to visualize party allegiance. Perhaps, you are wondering who is affected by a certain watershed. If you are limited to postal addresses without being able to visualize the actual location of those addresses the inference is limited. Commuter habits, crime trends, pandemic evolution, and (fill in your example here) analyses are all improved with geocoding. Thanks, geocoding! . ",
    "url": "/Geo-Spatial/geocoding.html#why-is-geocoding-helpful",
    "relUrl": "/Geo-Spatial/geocoding.html#why-is-geocoding-helpful"
  },"232": {
    "doc": "Geocoding",
    "title": "Geocoding Services",
    "content": "It is important to recognize that there are many different geocoding platforms. There are others but here is a short list of platforms to consider: . | Geocodio | Google’s geocode API service | IPUMS Geomarker | ArcGIS | . When you are deciding which geocode platform to use some important things to keep in mind are pricing structures and other specific features of the platfrom like bulk geocoding and coverage. For example, Geocodio is much more suited to geocode big data sets than Google’s platform. However, Geocodio is only able to geocode within the United States and Cananda whereas Google has international capabilities. Google is better at guessing what location you are trying to geocode (“the White House”) than Geocodio, but Geocodio offers census appends. The pricing sturcture is also nuanced across platforms. Here is a comparison chart provided by Geocodio that gives a flavor of what to consider when deciding which service to use (although you should bear in mind that vendor evaluations may be biased…) Lots to consider! In the end, which platform works best will depend on your preferences and the nature of your project. ",
    "url": "/Geo-Spatial/geocoding.html#geocoding-services",
    "relUrl": "/Geo-Spatial/geocoding.html#geocoding-services"
  },"233": {
    "doc": "Geocoding",
    "title": "Keep in Mind",
    "content": ". | Be attentive to accuracy and accuracy type. Just because something is spit out doesn’t mean you should use/trust it | When you use a service like Geocodio, you need to consider pricing (2,500 free lookups per day) | One size doesn’t necesarily fit all–tailor the geocode platform you choose to your project | . ",
    "url": "/Geo-Spatial/geocoding.html#keep-in-mind",
    "relUrl": "/Geo-Spatial/geocoding.html#keep-in-mind"
  },"234": {
    "doc": "Geocoding",
    "title": "Implementations",
    "content": " ",
    "url": "/Geo-Spatial/geocoding.html#implementations",
    "relUrl": "/Geo-Spatial/geocoding.html#implementations"
  },"235": {
    "doc": "Geocoding",
    "title": "Python",
    "content": "Geopy is a Python package thay provides a front end for a large number of geocoding APIs, including OpenStreetMap, Bing, Google, ArcGIS, and more. Below is an example of using geopy. Users may also want to explore the geocoder Python package. We’ll use the OpenStreetMap API to do the geocoding. It’s important to note that this API has some fair usage conditions including a maximum of 1 request per second, that you provide an informative ‘user agent’ parameter, and that you clearly display attribution (thank you OpenStreetMap!). For bulk geocoding, you may need to pay a fee to a provider. # If you don't have it, install geopy using 'pip install geopy' from geopy.geocoders import Nominatim # Create a geolocator using Open Street Map (aka Nominatim) # Use your own user agent identifier here geolocator = Nominatim(user_agent='LOST_geocoding_page') # Pass an address to retrieve full location information: location = geolocator.geocode('Bank of England') print(location.address) # &gt;&gt; Bank of England, 8AH, Threadneedle Street, Bishopsgate, City of London, # England, EC2R 8AH, United Kingdom print(location.latitude, location.longitude) # &gt;&gt; 51.51413225 -0.08892476721255456 # We can also reverse geocode from a lat and lon: scnd_location = geolocator.reverse(\"51.529969, -0.127688\") print(scnd_location.address) # &gt;&gt; British Library, 96, Euston Road, Bloomsbury, London Borough of Camden, # England, NW1 2DB, United Kingdom . ",
    "url": "/Geo-Spatial/geocoding.html#python",
    "relUrl": "/Geo-Spatial/geocoding.html#python"
  },"236": {
    "doc": "Geocoding",
    "title": "rgeocodio (R + Geocodio)",
    "content": "This example will talk specifically about Geocodio and how to use the Goecodio platform in R studio. Geocodio . Geocodio’s website is very straight forward, but I will briefly walk through the process: . | Start by making an account. This account will allow you to do your geocoding with Geocodio as well as get a Geocodio API which we can use in R studio. | To geocode on the website you can either upload a spreadsheet or copy and paste addresses into the input window. I highly recommend a spreadsheet which takes a specific format . | Geocodio will ask you to make edits if the data you have provided isn’t accurate enough . | Once your data is in satisfactory form Geodio allows you to make appends which allows you to include information pertaining to the addresses you wish to geocode (e.g. what State Legislative District the address is in or Census ACS Demographic information for the addresses you are geocoding) . | Finally Geocodio will geocode your addresses and return a downloadable csv file. The cost and the time of this process depends on the size of your data. For example, 250,000 addresses can be geocoded for $123.75 and will take about an hour to process. For estimates of both cost and time click here . | . ",
    "url": "/Geo-Spatial/geocoding.html#rgeocodio-r--geocodio",
    "relUrl": "/Geo-Spatial/geocoding.html#rgeocodio-r--geocodio"
  },"237": {
    "doc": "Geocoding",
    "title": "Example",
    "content": "rgeocodio allows you to access the Geocodio platform in R studio. Instead of the steps mentioned above you can use the rgeocodio to perform the same functions. In order to install rgeocodio you will need to load the devtools package. Install it if you haven’t already install.packages(\"devtools\"). Once devtools is loaded run:devtools::install_github('hrbrmstr/rgeocodio'). rgeocodio uses an API that you can get from the geocodio website. To get an API visit geocodio’s website. Then save it in your Renviron. To save the API in your Renvrion: . | Open the Renviron by running usethis::edit_r_environ() | Once you are in the Renviron name and save the API you got from Geocodio. Maybe something like: #geocodio_API = 'your api` . | Save your Renviron and then restart your R session just to be sure that the API is saved. | . Now that you have your API saved in R you still need to authorize the API in your R session. Do so by running gio_auth(). # If necessary # install.packages(c('rgeocodio','readxl','tidyverse')) library(rgeocodio) gio_auth(force = F) . A quick note, force makes you set a new geocodio API key for the current environment. In general you will want to run force=F. Lets try a regeocodio example. Say you want to get the coordinates of the White House. You could run: . rgeocodio::gio_geocode('1600 Pennsylvania Ave NW, Washington DC 20500') . Most of these variables are intuitive but I want to spend a few seconds on accuracy and accuracy type which we can learn more about here. | Accuracy: because geocodio is interpolating the output will tell you how confident geocodio is in its estimation. Anything below 0.8 should be considered not accurate enough, but that is up to the user. | Accuracy Type: interpolation uses the closest know geocodes. So if the closest geocodes are, for instance two ends of a street and you are trying to geocode a location somewhere on that street then the accuracy type will be “street.” In this case the accuracy type is “rooftop” which means the buildings on either side of the location were used to interpolate your query. Again, smartystreets has a good explanation of this. | . What if we want to geocode a bunch of addresses at once? To geocode multiple addresses at once we will use gio_batch_geocode. The data that we enter will need to be a character vector of addresses. library(readxl) library(tidyverse) addresses&lt;- c('Yosemite National Park, California', '1600 Pennsylvania Ave NW, Washington DC 20500', '2975 Kincaide St Eugene, Oregon, 97405') gio_batch_geocode(addresses) . You will notice that the output is a list with dataframes of the results embedded. There are a number of ways to extract the relevant data but one approach would be: . addresses&lt;- c('Yosemite National Park, California', '1600 Pennsylvania Ave NW, Washington DC 20500', '2975 Kincaide St Eugene, Oregon, 97405') extract_function&lt;- function(addresses){ data&lt;-gio_batch_geocode(addresses) vector&lt;- (1: length(addresses)) df_function&lt;-function(vector){ df&lt;-data$response_results[vector] df&lt;-df%&gt;%as.data.frame() } geocode_data&lt;-do.call(bind_rows, lapply(vector, df_function)) return(geocode_data) } extract_function(addresses) . Reverse geocoding uses gio_reverse and gio_batch_reverse. For gio_reverse you submit a longitude-latitude pair: . gio_reverse(38.89767, -77.03655) . For gio_batch_reverse we will submit a vector of numeric entries ordered by c(longitude, latitude): . #make a dataset data&lt;-data.frame( lat = c(35.9746000, 32.8793700, 33.8337100, 35.4171240), lon = c(-77.9658000, -96.6303900, -117.8362320, -80.6784760) ) gio_batch_reverse(data) . Notice that the output gives us multiple accuracy types. What about geocoding the rest of the world, chico? . rgeocodio::gio_batch_geocode('523-303, 350 Mokdongdong-ro, Yangcheon-Gu, Seoul, South Korea 07987') . gasp Geocodio only works, from my understanding, in the United States and Canada. We would need to use a different service like Google’s geocoder to do the rest of the world. ",
    "url": "/Geo-Spatial/geocoding.html#example",
    "relUrl": "/Geo-Spatial/geocoding.html#example"
  },"238": {
    "doc": "Get a List of Files",
    "title": "Get a List of Files",
    "content": "When cleaning and compiling data, it is common to need to get a list of files in a directory so that you can loop over them. Often, the goal is to open the files one at a time so you can combine them together in some way. For example, perhaps you have one data file per month, and want to compile them all together into a single data set. ",
    "url": "/Other/get_a_list_of_files.html",
    "relUrl": "/Other/get_a_list_of_files.html"
  },"239": {
    "doc": "Get a List of Files",
    "title": "Keep in Mind",
    "content": ". | Giving your files a consistent naming scheme will often make it easier to get this to work, as many approaches require you to give a template for the file names you want. | Before doing this you will probably find it useful to Set a Working Directory | . ",
    "url": "/Other/get_a_list_of_files.html#keep-in-mind",
    "relUrl": "/Other/get_a_list_of_files.html#keep-in-mind"
  },"240": {
    "doc": "Get a List of Files",
    "title": "Implementations",
    "content": "Note that, because these code examples necessarily refer to files on disk, they might not run properly if copied and pasted. But they can be used as templates. ",
    "url": "/Other/get_a_list_of_files.html#implementations",
    "relUrl": "/Other/get_a_list_of_files.html#implementations"
  },"241": {
    "doc": "Get a List of Files",
    "title": "Python",
    "content": "The glob module finds all pathnames matching a specified pattern and stores them in a list. import glob # Retrieve all csvs in the working directory list_of_files = glob.glob('*.csv') # Retrieve all csvs in the working directory and all sub-directories list_of_files = glob.glob('**/*.csv', recursive=True) . ",
    "url": "/Other/get_a_list_of_files.html#python",
    "relUrl": "/Other/get_a_list_of_files.html#python"
  },"242": {
    "doc": "Get a List of Files",
    "title": "R",
    "content": "The list.files() function can produce a list of files that can be looped over. # Get a list of all .csv files in the Data folder # (which sits inside our working directory) filelist &lt;- list.files('Data','*.csv') # filelist just contains file names now. If we want it to # open them up from the Data folder we must say so filelist &lt;- paste0('Data/',filelist) # Read them all in and then row-bind them together # (assuming they're all the same format and can be rbind-ed) datasets &lt;- lapply(filelist,read.csv) data &lt;- do.call(rbind,datasets) # Or, use the tidyverse with purrr # (assuming they're all the same format and can be rbind-ed) library(tidyverse) data &lt;- filelist %&gt;% map(read_csv) %&gt;% bind_rows() . ",
    "url": "/Other/get_a_list_of_files.html#r",
    "relUrl": "/Other/get_a_list_of_files.html#r"
  },"243": {
    "doc": "Get a List of Files",
    "title": "Stata",
    "content": "The dir function in Stata can be used to produce a list of files, which can be stored in a local (or global), and then looped over. * Get a list of all .xlsx files in the Data folder * (which sits inside our working directory) local filelist: dir \"Data/\" files \"*.xlsx\" * Append them all together * (assuming this is what you want to do) * filelist only contains file names - if we want it to look in * the Data folder, we must say so explicitly local firsttime = 1 foreach f in `filelist' { * import the data import excel using \"Data/`f'\", clear firstrow * Append it to the data we've already imported * Unless this is the first one we opened, in which * case just start a new file if `firsttime' == 0 { append using compiled_data.dta } save compiled_data.dta, replace local firsttime = 0 } . ",
    "url": "/Other/get_a_list_of_files.html#stata",
    "relUrl": "/Other/get_a_list_of_files.html#stata"
  },"244": {
    "doc": "Heteroskedasticity-consistent standard errors",
    "title": "Heteroskedasticity-consistent (HC) standard errors",
    "content": "Heteroskedasticity is when the variance of a model’s error term is related to the predictors in that model. For more information, see Wikipedia: Heteroscedasticity. Many regression models assume homoskedasticity (i.e. constant variance of the error term), especially when calculating standard errors. So in the presence of heteroskedasticity, standard errors will be incorrect. Heteroskedasticity-consistent (HC) standard errors — also called “heteroskedasticity-robust”, or sometimes just “robust” standard errors — are calculated without assuming such homoskedasticity. For more information, see Wikipedia: Heteroscedasticity-consistent standard errors. ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/hc_se.html#heteroskedasticity-consistent-hc-standard-errors",
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/hc_se.html#heteroskedasticity-consistent-hc-standard-errors"
  },"245": {
    "doc": "Heteroskedasticity-consistent standard errors",
    "title": "Keep in Mind",
    "content": ". | Robust standard errors are a common way of dealing with heteroskedasticity. However, they make certain assumptions about the form of that heteroskedasticity which may not be true. You may instead want to use GMM instead. | For nonlinear models like Logit, heteroskedasticity can bias estimates in addition to messing up standard errors. Simply using a robust covariance matrix will not eliminate this bias. Check the documentation of your nonlinear regression command to see whether its robust-error options also adjust for this bias. If not, consider other ways of dealing with heteroskedasticity besides robust errors. | There are multiple kinds of robust standard errors, for example HC1, HC2, and HC3. Check in to the kind available to you in the commands you’re using. | . ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/hc_se.html#keep-in-mind",
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/hc_se.html#keep-in-mind"
  },"246": {
    "doc": "Heteroskedasticity-consistent standard errors",
    "title": "Also Consider",
    "content": ". | Generalized Method of Moments | Cluster-Robust Standard Errors | Bootstrap Standard Errors | Jackknife Standard Errors | . ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/hc_se.html#also-consider",
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/hc_se.html#also-consider"
  },"247": {
    "doc": "Heteroskedasticity-consistent standard errors",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/hc_se.html#implementations",
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/hc_se.html#implementations"
  },"248": {
    "doc": "Heteroskedasticity-consistent standard errors",
    "title": "R",
    "content": "The easiest way to obtain robust standard errors in R is with the estimatr package (link) and its family of lm_robust functions. These will default to “HC2” errors, but users can specify a variety of other options. # If necessary, install estimatr # install.packages(c('estimatr')) library(estimatr) # Get mtcars data # data(mtcars) ## Optional: Will load automatically anyway # Default is \"HC2\". Here we'll specify \"HC3\" just to illustrate. m1 &lt;- lm_robust(mpg ~ cyl + disp + hp, data = mtcars, se_type = \"HC3\") summary(m1) . Alternately, users may consider the vcovHC function from the sandwich package (link), which is very flexible and supports a wide variety of generic regression objects. For inference (t-tests, etc.), use in conjunction with the coeftest function from the lmtest package (link). # If necessary, install lmtest and sandwich # install.packages(c('lmtest','sandwich')) library(sandwich) library(lmtest) # Create a normal regression model (i.e. without robust standard errors) m2 &lt;- lm(mpg ~ cyl + disp + hp, data = mtcars) # Get the robust VCOV matrix using sandwich::vcovHC(). We can pick the kind of robust errors # with the \"type\" argument. Note that, unlike estimatr::lm_robust(), the default this time # is \"HC3\". I'll specify it here anyway just to illustrate. vcovHC(m2, type = \"HC3\") sqrt(diag(vcovHC(m2))) ## HAC SEs # For statistical inference, use together with lmtest::coeftest(). coeftest(m2, vcov = vcovHC(m2)) . ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/hc_se.html#r",
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/hc_se.html#r"
  },"249": {
    "doc": "Heteroskedasticity-consistent standard errors",
    "title": "Stata",
    "content": "Stata has robust standard errors built into most regression commands, and they generally work the same way for all commands. * Load in auto data sysuse auto.dta, clear * Just add robust to the options of the regression * This will give you HC1 regress price mpg gear_ratio foreign, robust * For other kinds of robust standard errors use vce() regress price mpg gear_ratio foreign, vce(hc3) . ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/hc_se.html#stata",
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/hc_se.html#stata"
  },"250": {
    "doc": "Heteroskedasticity-consistent standard errors",
    "title": "Heteroskedasticity-consistent standard errors",
    "content": " ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/hc_se.html",
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/hc_se.html"
  },"251": {
    "doc": "Heatmap Colored Correlation Matrix",
    "title": "Heatmap Colored Correlation Matrix",
    "content": "A correlation matrix shows the correlation between different variables in a matrix setting. However, because these matrices have so many numbers on them, they can be difficult to follow. Heatmap coloring of the matrix, where one color indicates a positive correlation, another indicates a negative correlation, and the shade indicates the strength of correlation, can make these matrices easier for the reader to understand. ",
    "url": "/Presentation/Figures/heatmap_colored_correlation_matrix.html",
    "relUrl": "/Presentation/Figures/heatmap_colored_correlation_matrix.html"
  },"252": {
    "doc": "Heatmap Colored Correlation Matrix",
    "title": "Keep in Mind",
    "content": ". | Even with heatmap coloring, very large correlation matrices can still be difficult to read, as you must pinpoint which variable names go with which cell of the matrix. Consider breaking big correlation matrices up into smaller ones, or limiting the amount of data you’re trying to show in some other way. | . ",
    "url": "/Presentation/Figures/heatmap_colored_correlation_matrix.html#keep-in-mind",
    "relUrl": "/Presentation/Figures/heatmap_colored_correlation_matrix.html#keep-in-mind"
  },"253": {
    "doc": "Heatmap Colored Correlation Matrix",
    "title": "Also Consider",
    "content": ". | You may just want to create a correlation matrix | . ",
    "url": "/Presentation/Figures/heatmap_colored_correlation_matrix.html#also-consider",
    "relUrl": "/Presentation/Figures/heatmap_colored_correlation_matrix.html#also-consider"
  },"254": {
    "doc": "Heatmap Colored Correlation Matrix",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Figures/heatmap_colored_correlation_matrix.html#implementations",
    "relUrl": "/Presentation/Figures/heatmap_colored_correlation_matrix.html#implementations"
  },"255": {
    "doc": "Heatmap Colored Correlation Matrix",
    "title": "Python",
    "content": "We present two ways you can create a heatmap. First, the seaborn package has a great collection of premade plots, one of which is a heatmap we’ll use. The second we’ll only point you to, which is a “by hand” approach that will allow you more customization. For the by hand approach, see this guide. For the seaborn approach, you will need to pip install seaborn or conda install seaborn before continuing. Once you’ve done that, the follow code will produce the below plot. # Ganked from https://seaborn.pydata.org/examples/many_pairwise_correlations.html # Assumes you have run `pip install numpy pandas matplotlib scikit-learn seaborn` # Standard imports import numpy as np import pandas as pd from matplotlib import pyplot as plt # For this example we'll use Seaborn, which has some nice built in plots import seaborn as sns # Grab a data set from scikit-learn from sklearn.datasets import fetch_california_housing data = fetch_california_housing() df = pd.DataFrame( np.c_[data['data'], data['target']], columns=data['feature_names'] + ['target'] ) # Create the correlation matrix corr = df.corr() # Generate a mask for the upper triangle; True = do NOT show mask = np.zeros_like(corr, dtype=np.bool) mask[np.triu_indices_from(mask)] = True # Set up the matplotlib figure f, ax = plt.subplots(figsize=(11, 9)) # Generate a custom diverging colormap cmap = sns.diverging_palette(220, 10, as_cmap=True) # Draw the heatmap with the mask and correct aspect ratio # More details at https://seaborn.pydata.org/generated/seaborn.heatmap.html sns.heatmap( corr, # The data to plot mask=mask, # Mask some cells cmap=cmap, # What colors to plot the heatmap as annot=True, # Should the values be plotted in the cells? vmax=.3, # The maximum value of the legend. All higher vals will be same color vmin=-.3, # The minimum value of the legend. All lower vals will be same color center=0, # The center value of the legend. With divergent cmap, where white is square=True, # Force cells to be square linewidths=.5, # Width of lines that divide cells cbar_kws={\"shrink\": .5} # Extra kwargs for the legend; in this case, shrink by 50% ) # You can save this as a png with # f.savefig('heatmap_colored_correlation_matrix_seaborn_python.png') . ",
    "url": "/Presentation/Figures/heatmap_colored_correlation_matrix.html#python",
    "relUrl": "/Presentation/Figures/heatmap_colored_correlation_matrix.html#python"
  },"256": {
    "doc": "Heatmap Colored Correlation Matrix",
    "title": "R",
    "content": "We will be creating our heatmap in two different ways. First, we will be using the corrplot package, which is tailor-made for the task and is very easy to use. Then, we will be using ggplot2 with geom_tile, which requires much more preprocessing to use, but then provides access to the entirety of the ggplot2 package for customization. First, we will use corrplot: . # Install the corrplot package if necessary # install.packages('corrplot') # Load in the corrplot package library(corrplot) # Load in mtcars data data(mtcars) # Don't use too many variables or it will get messy! mtcars &lt;- mtcars[,c('mpg','cyl','disp','hp','drat','wt','qsec')] # Create a corrgram corrplot(cor(mtcars), # Using the color method for a heatmap method = 'color', # And the lower half only for easier readability type = 'lower', # Omit the 1's along the diagonal to bring variable names closer diag = FALSE, # Add the number on top of the color addCoef.col = 'black' ) . This results in: . Now we will make the graph using ggplot2. We will also make a little use of dplyr and tidyr, and so we’ll load them all as a part of the tidyverse. This example makes use of this guide. # Install the tidyverse if necessary # install.packages('tidyverse') # Load in the tidyverse library(tidyverse) # Load in mtcars data data(mtcars) # Create a correlation matrix. C &lt;- mtcars %&gt;% # Don't use too many variables or it will get messy! # We use dplyr's select() here but there are other ways to limit variables, like [] select(cyl, disp, drat, hp, mpg, qsec, wt) %&gt;% # Correlation matrix cor() # At this point, we can limit the matrix to just its lower half # Note this will give weird results if you didn't select variables in alphabetical order earlier C[upper.tri(C)] &lt;- NA C &lt;- C %&gt;% # Turn it into a data frame as.data.frame() %&gt;% # with a column for the variable names. # We use dplyr's mutate to create this column but it could be made with $ # the . here means \"the data set we're working with\" mutate(Variable = row.names(.)) # Use tidyr's pivot_longer to reshape to long format # There are other ways to reshape too C_Long &lt;- pivot_longer(C, cols = c(mpg, cyl, disp, hp, drat, wt, qsec), # We will want this option for sure if we dropped the # upper half of the triangle earlier values_drop_na = TRUE) %&gt;% # Make both variables into factors mutate(Variable = factor(Variable), name = factor(name)) %&gt;% # Reverse the order of one of the variables so that the x and y variables have # Opposing orders, common for a correlation matrix mutate(Variable = factor(Variable, levels = rev(levels(.$Variable)))) # Now we graph! ggplot(C_Long, # Our x and y axis are Variable and name # And we want to fill each cell with the value aes(x = Variable, y = name, fill = value))+ # geom_tile to draw the graph geom_tile() + # Color the graph as we like # Here our negative correlations are red, positive are blue # gradient2 instead of gradient gives us a \"mid\" color which we can make white scale_fill_gradient2(low = \"red\", high = \"blue\", mid = \"white\", midpoint = 0, limit = c(-1,1), space = \"Lab\", name=\"Pearson\\nCorrelation\") + # Axis names don't make much sense labs(x = NULL, y = NULL) + # We don't need that background theme_minimal() + # If we need more room for variable names at the bottom, rotate them theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1)) + # We want those cells to be square! coord_fixed() + # If you also want the correlations to be written directly on there, add geom_text geom_text(aes(label = round(value,3))) . This results in: . ",
    "url": "/Presentation/Figures/heatmap_colored_correlation_matrix.html#r",
    "relUrl": "/Presentation/Figures/heatmap_colored_correlation_matrix.html#r"
  },"257": {
    "doc": "Heatmap Colored Correlation Matrix",
    "title": "SAS",
    "content": "See this guide. ",
    "url": "/Presentation/Figures/heatmap_colored_correlation_matrix.html#sas",
    "relUrl": "/Presentation/Figures/heatmap_colored_correlation_matrix.html#sas"
  },"258": {
    "doc": "Heatmap Colored Correlation Matrix",
    "title": "Stata",
    "content": "Stata has the installable package corrtable which produces heatmap correlation tables. Handily, it puts the variable labels (or names, if labels aren’t available) along the diagonal where they are easy to read. Note that it does run quite slowly. * Install corrtable if necessary * ssc install corrtable * Get auto data sysuse auto.dta, clear * Make correlation table * The half option just shows the lower triangle and puts variable names on the axis. * The flag1 and howflag1 options tell corrtable to plot positive correlations (r(rho &gt; 0)) * as blue (blue*.1) * and flag2 and howflag2 similarly tell it to plot negative correlations as pink. corrtable price-length, half flag1(r(rho) &gt; 0) howflag1(plotregion(color(blue * 0.1))) flag2(r(rho) &lt; 0) howflag2(plotregion(color(pink*0.1))) . This results in: . ",
    "url": "/Presentation/Figures/heatmap_colored_correlation_matrix.html#stata",
    "relUrl": "/Presentation/Figures/heatmap_colored_correlation_matrix.html#stata"
  },"259": {
    "doc": "Heckman Correction Model",
    "title": "Heckman Correction Model",
    "content": "The Heckman correction for sample selection is a method designed to be used in cases where the model can only be run on a subsample of the data that is not randomly selected. For example, a regression using \\(Wage\\) to predict \\(Hours Worked\\) cannot include people who don’t work, since we don’t observe their wage. The Heckman model views this sample selection process as a form of omitted variable bias. So, it (1) explicitly models the process of selecting into the sample, (2) transforms the predicted probability of being in the sample, and (3) adds a correction term to the model. For more information, see Wikipedia: Heckman correction. ",
    "url": "/Model_Estimation/GLS/heckman_correction_model.html",
    "relUrl": "/Model_Estimation/GLS/heckman_correction_model.html"
  },"260": {
    "doc": "Heckman Correction Model",
    "title": "Keep in Mind",
    "content": ". | Conceptually, the Heckman model uses the regression covariates to predict selection, transforms the prediction, and then includes that transformation in the model. If there are no variables in the selection model that are excluded from the regression model, then the Heckman model is perfectly collinear, and is only statistically identified because the transformation is nonlinear (you may have heard the phrase “identified by nonlinearity” or “identified by the normality assumption”). That’s not ideal! You want to find an exclusion restriction - a variable that predicts selection, but does not belong in the final regression model - to avoid this collinearity. | . ",
    "url": "/Model_Estimation/GLS/heckman_correction_model.html#keep-in-mind",
    "relUrl": "/Model_Estimation/GLS/heckman_correction_model.html#keep-in-mind"
  },"261": {
    "doc": "Heckman Correction Model",
    "title": "Also Consider",
    "content": ". | There are many ways to estimate a Heckman model. Maximum likelihood approaches generally have better statistical properties, but two-stage models are computationally simpler. Often you can look in the options of your Heckman estimator command to select an estimation method. | If your goal is to estimate the effect of a binary treatment by modeling selection into treatment, consider a Treatment Effect Model, or an Endogenous Switching Model which also allows predictors to work differently in different settings. | Standard Heckman models rely heavily on assumptions about the normality of error terms. You may want to consider Nonparametric Sample Selection Models. | . ",
    "url": "/Model_Estimation/GLS/heckman_correction_model.html#also-consider",
    "relUrl": "/Model_Estimation/GLS/heckman_correction_model.html#also-consider"
  },"262": {
    "doc": "Heckman Correction Model",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/GLS/heckman_correction_model.html#implementations",
    "relUrl": "/Model_Estimation/GLS/heckman_correction_model.html#implementations"
  },"263": {
    "doc": "Heckman Correction Model",
    "title": "Gretl",
    "content": "See here for a demonstration. ",
    "url": "/Model_Estimation/GLS/heckman_correction_model.html#gretl",
    "relUrl": "/Model_Estimation/GLS/heckman_correction_model.html#gretl"
  },"264": {
    "doc": "Heckman Correction Model",
    "title": "Python",
    "content": "See here for a demonstration. ",
    "url": "/Model_Estimation/GLS/heckman_correction_model.html#python",
    "relUrl": "/Model_Estimation/GLS/heckman_correction_model.html#python"
  },"265": {
    "doc": "Heckman Correction Model",
    "title": "R",
    "content": "# Install sampleSelection package if necessary # install.packages('sampleSelection') library(sampleSelection) # Get data from Mroz (1987, Econometrica) # which has Panel Study of Income Dynamics data for married women data(\"Mroz87\") # First consider our selection model # We only observe wages for labor force participants (lfp == 1) # So we model that as a function of work experience (linear and squared), # income from the rest of the family, education, and number of kids 5 or younger. # lfp ~ exper + I(exper^2) + faminc + educ + kids5 # Then we model the regression of interest. We're interested in modeling # wage as a function of work experience, education, and whether you're in a city # Here, we don't include family income or number of kids, under the assumption that they # do not belong in a wage model. These are our exclusion restrictions # (note these particular exclusion restrictions might be a little dubious! But hey, this paper's from 1987.) # wage ~ exper + I(exper^2) + educ + city # Put them together in a selection() command heck_model &lt;- selection(lfp ~ exper + I(exper^2) + faminc + educ + kids5, wage ~ exper + I(exper^2) + educ + city, Mroz87) summary(heck_model) . ",
    "url": "/Model_Estimation/GLS/heckman_correction_model.html#r",
    "relUrl": "/Model_Estimation/GLS/heckman_correction_model.html#r"
  },"266": {
    "doc": "Heckman Correction Model",
    "title": "Stata",
    "content": "* Get data from Mroz (1987, Econometrica) * which has Panel Study of Income Dynamics data for married women * (data via the sampleSelection package in R) import delimited \"https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Estimation/Data/Heckman_Correction_Model/Mroz87.csv\", clear * First, consider the regression of interest. * First consider our selection model * We only observe wages for labor force participants (lfp == 1) * So we model that as a function of work experience (linear and squared), * income from the rest of the family, education, and number of kids 5 or younger. * select(lfp = c.exper##c.exper faminc educ kids5) * Then we model the regression of interest. We're interested in modeling * wage as a function of work experience, education, and whether you're in a city * Here, we don't include family income or number of kids, under the assumption that they * do not belong in a wage model. These are our exclusion restrictions * (note these particular exclusion restrictions might be a little dubious! But hey, 1987.) * wage c.exper##c.exper educ city * Now we run our Heckman model! heckman wage c.exper##c.exper educ city, select(lfp = c.exper##c.exper faminc educ kids5) . ",
    "url": "/Model_Estimation/GLS/heckman_correction_model.html#stata",
    "relUrl": "/Model_Estimation/GLS/heckman_correction_model.html#stata"
  },"267": {
    "doc": "Histograms",
    "title": "Histograms",
    "content": "Histograms are an indespensible tool of research across disciplines. They offer a helpful way to represent the distribution of a variable of interest. Specifically, their function is to record how frequently data values fall within pre-specified ranges called “bins.” Such visual representations can help researchers easily detect whether their data are distributed in a skewed or symmetric way, and can help detect outliers. Despite being such a popular tool for scientific research, choosing the bin width (alternatively, number of bins) is ultimately a choice by the researcher. Histograms are intended to convey information about the variable, and choosing the “right” bin size to convey the information helpfully can be something of an art. The relationship between bin width \\(h\\) and the number of bins \\(k\\) is given by: . \\[k = \\frac{ \\max x - \\min x}{h}\\] For this reason, statistical softwares such as R and Stata will often accept either custom bin width specifications, or a number of bins. ",
    "url": "/Presentation/Figures/histograms.html",
    "relUrl": "/Presentation/Figures/histograms.html"
  },"268": {
    "doc": "Histograms",
    "title": "Histogram vs. bar graph",
    "content": "Because histograms represent data frequency using rectangular bars, they might be mistaken for bar graphs at first glance. Whereas bar graphs (sometimes called bar charts) plot values for categorical data, histograms represent the distribution of continuous variables such as income, height, weight, etc. ",
    "url": "/Presentation/Figures/histograms.html#histogram-vs-bar-graph",
    "relUrl": "/Presentation/Figures/histograms.html#histogram-vs-bar-graph"
  },"269": {
    "doc": "Histograms",
    "title": "Implementations",
    "content": "When feeding data to visualise using a histogram, one will notice that both R and Stata will attempt to “guess” what the “best” bin width/number of bins are. These may be overridden by user commands, as we will see. ",
    "url": "/Presentation/Figures/histograms.html#implementations",
    "relUrl": "/Presentation/Figures/histograms.html#implementations"
  },"270": {
    "doc": "Histograms",
    "title": "Python",
    "content": "There are many plotting libraries in Python, including declarative (say what you want) and imperative (build what you want) options. In the example below, we’ll explore several different options for plotting histogram data. By far the quickest way to plot a histogram is to use data analysis package pandas’ built-in bar chart option (which uses plotting library matplotlib). import pandas as pd df = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/PSID.csv\", index_col=0) df['earnings'].plot.hist(); . Note that, by default, the hist function shows counts on the y-axis. We can make this plot a bit more appealing by calling on the customisation features from matplotlib. Calling df['column-name'].plot.hist() returns a matplotlib ax object that accepts further customisation. We will use this and a style to make the plot more appealing, while also switching to show a density rather than counts, setting the number of bins explicitly, and using a logarithmic scale. import matplotlib.pyplot as plt plt.style.use('seaborn') ax = df['earnings'].plot.hist(density=True, log=True, bins=80) ax.set_title('Earnings in the PSID', loc='left') ax.set_ylabel('Density') ax.set_xlabel('Earnings'); . An alternative to the matplotlib-pandas combination is seaborn, a declarative plotting library. Using seaborn, we can quickly compared histograms of different cuts of the data. In the example below, a binary variable that sorts individuals into two groups based on age is added and used as the ‘hue’ of the keyword argument. import seaborn as sns age_cut_off = 45 df[f'Older than {age_cut_off}'] = df['age']&gt;age_cut_off ax = sns.histplot(df, x=\"earnings\", hue=f\"Older than {age_cut_off}\", element=\"step\", stat=\"density\") ax.set_yscale('log') . Finally, let’s look at a different declarative library, plotnine, which is inspired by (and has very similar syntax to) R’s plotting library ggplot. from plotnine import ggplot, aes, geom_histogram ( ggplot(df, aes(x='earnings', y='stat(density)') ) + geom_histogram(bins=80) ) . ",
    "url": "/Presentation/Figures/histograms.html#python",
    "relUrl": "/Presentation/Figures/histograms.html#python"
  },"271": {
    "doc": "Histograms",
    "title": "R",
    "content": "Histograms can be represented using base R, or more elegantly with ggplot. R comes with a built-in states.x77 dataset containing per-capita income in the US states for the year 1974, which we will be using. # loading the data incomes = data.frame( income = state.x77[,'Income']) # first using base R hist(incomes$income) # now using ggplot if(!require(ggplot2)) install.packages('ggplot2') library(ggplot2) ggplot( data = incomes ) + geom_histogram( aes( x = income ) ) # showing how we can adjust number of bins... ggplot( data = incomes ) + geom_histogram( aes( x = income ) , bins = 15 ) # ...or the width of each bin ggplot( data = incomes ) + geom_histogram( aes( x = income ) , binwidth = 500 ) . ",
    "url": "/Presentation/Figures/histograms.html#r",
    "relUrl": "/Presentation/Figures/histograms.html#r"
  },"272": {
    "doc": "Histograms",
    "title": "Stata",
    "content": "To illustrate the basic histogram function in Stata we will use the “auto” dataset. ** loading the data webuse auto * histogram with default bin width * The frequency option puts a count of observations on the y-axis * rather than a proportion histogram mpg, frequency * we can adjust the number of bins... histogram mpg, bin(15) frequency * ...or the bin width hist mpg, width(2) frequency . ",
    "url": "/Presentation/Figures/histograms.html#stata",
    "relUrl": "/Presentation/Figures/histograms.html#stata"
  },"273": {
    "doc": "Import a Foreign Data File",
    "title": "Import a Foreign Data File",
    "content": "Commonly, data will be distributed in a format that is not native to the software that you are using, such as Excel. How can you import it? . This page is specifically about importing data files from formats specific to particular foreign software. For importing standard shared formats, see Import a Delimited Data File (CSV, TSV) or Import a Fixed-Width Data File. ",
    "url": "/Other/import_a_foreign_data_file.html",
    "relUrl": "/Other/import_a_foreign_data_file.html"
  },"274": {
    "doc": "Import a Foreign Data File",
    "title": "Keep in Mind",
    "content": ". | Check your data after it’s imported to make sure it worked properly. Sometimes special characters will have trouble converting, or variable name formats are inconsistent, and so on. It never hurts to check! | . ",
    "url": "/Other/import_a_foreign_data_file.html#keep-in-mind",
    "relUrl": "/Other/import_a_foreign_data_file.html#keep-in-mind"
  },"275": {
    "doc": "Import a Foreign Data File",
    "title": "Also Consider",
    "content": ". | Import a Delimited Data File (CSV, TSV) | Import a Fixed-Width Data File | Export Data to a Foreign Format | . ",
    "url": "/Other/import_a_foreign_data_file.html#also-consider",
    "relUrl": "/Other/import_a_foreign_data_file.html#also-consider"
  },"276": {
    "doc": "Import a Foreign Data File",
    "title": "Implementations",
    "content": "Because there are so many potential foreign formats, these implementations will be more about listing the appropriate commands with example syntax than providing full working examples. Make sure that you fill in the proper filename. The filename should include a filepath, or you should Set a Working Directory. ",
    "url": "/Other/import_a_foreign_data_file.html#implementations",
    "relUrl": "/Other/import_a_foreign_data_file.html#implementations"
  },"277": {
    "doc": "Import a Foreign Data File",
    "title": "R",
    "content": "# Load Excel files with the readxl package # install.packages('readxl') library(readxl) data &lt;- read_excel('filename.xlsx') # Read Stata, SAS, and SPSS files with the haven package # install.packages('haven') library(haven) data &lt;- read_stata('filename.dta') data &lt;- read_spss('filename.sav') # read_sas also supports .sas7bcat, or read_xpt supports transport files data &lt;- read_sas('filename.sas7bdat') # Read lots of other types with the foreign package # install.packages('foreign') library(foreign) data &lt;- read.arff('filename.arff') data &lt;- read.dbf('filename.dbf') data &lt;- read.epiinfo('filename.epiinfo') data &lt;- read.mtb('filename.mtb') data &lt;- read.octave('filename.octave') data &lt;- read.S('filename.S') data &lt;- read.systat('filename.systat') . ",
    "url": "/Other/import_a_foreign_data_file.html#r",
    "relUrl": "/Other/import_a_foreign_data_file.html#r"
  },"278": {
    "doc": "Import a Foreign Data File",
    "title": "Stata",
    "content": "Stata can import foreign files using the File -&gt; Import menu. Alternately, you can use the import command: . import type using filename . where type can be excel, spss, sas, haver, or dbase (import can also be used to download data directly from sources like FRED). ",
    "url": "/Other/import_a_foreign_data_file.html#stata",
    "relUrl": "/Other/import_a_foreign_data_file.html#stata"
  },"279": {
    "doc": "Instrumental Variables",
    "title": "Instrumental Variables",
    "content": "In the regression model . \\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\] where \\(\\epsilon\\) is an error term, the estimated \\(\\hat{\\beta}_1\\) will not give the causal effect of \\(X\\) on \\(Y\\) if \\(X\\) is endogenous - that is, if \\(X\\) is related to \\(\\epsilon\\) and so determined by forces within the model (endogenous). One way to recover the causal effect of \\(X\\) on \\(Y\\) is to use instrumental variables. If there exists a variable \\(Z\\) that is related to \\(X\\) but is completely unrelated to \\(\\epsilon\\) (perhaps after adding some controls), then you can use instrumental variables estimation to isolate only the part of the variation in \\(X\\) that is explained by \\(Z\\). Naturally, then, this part of the variation is unrelated to \\(\\epsilon\\) because \\(Z\\) is unrelated to \\(\\epsilon\\), and you can get the causal effect of that part of \\(X\\). For more information, see Wikipedia: Instrumental variables estimation. ",
    "url": "/Model_Estimation/Research_Design/instrumental_variables.html",
    "relUrl": "/Model_Estimation/Research_Design/instrumental_variables.html"
  },"280": {
    "doc": "Instrumental Variables",
    "title": "Keep in Mind",
    "content": ". | Technically, all the variables in the model except for the dependent variable and the endogenous variables are “instruments”, including controls. However, it is also common to refer to only the excluded instruments (i.e., variables that are only used to predict the endogenous variable, not the dependent variable) as instruments. This page will follow that convention. | For instrumental variables to work, it must be the case that the instrument is only related to the outcome variable through other variables already included in the model like the endogenous variables or the controls. This is called the “validity” assumption and it cannot be verified in the data, only theoretically. Give serious consideration as to whether validity applies to your instrument before using instrumental variables. | You can check for the relevance of your instrument, which is how strongly related it is to your endogenous variable. A rule of thumb is that an joint F-test of the instruments should be at least 10, but this is only a rule of thumb, and imprecise (see Stock and Yogo 2005 for a more precise version of this test). In general, if the instruments are not very strong predictors of the endogenous variables, you should consider whether your analysis fits the assumptions necessary to run a weak-instrument-robust estimation method. See Hahn &amp; Hausman 2003 for an overview. | Instrumental variables estimates a local average treatment effect - in other words, a weighted average of each individual observation’s treatment effect, where the weights are based on the strength of the effect of the instrument on the endogenous variable. Note both that this is not the same thing as an average treatment effect, which is an average of each individual’s treatment effect, which is usually what is desired, and also that if the instrumental variable has effects of different signs for different people (non-monotonicity), then the estimate isn’t really anything of interest. Be sure that monotonicity makes sense in your context before using instrumental variables. | Instrumental variables is a consistent estimator of a causal effect, but it is biased in finite samples. Be wary of using instrumental variables in small samples. | . ",
    "url": "/Model_Estimation/Research_Design/instrumental_variables.html#keep-in-mind",
    "relUrl": "/Model_Estimation/Research_Design/instrumental_variables.html#keep-in-mind"
  },"281": {
    "doc": "Instrumental Variables",
    "title": "Also Consider",
    "content": ". | Instrumental variables methods generally rely on linearity assumptions, and if your dependent or endogenous variables are not continuous, their assumptions may not hold. Consider methods specially designed for nonlinear instrumental variables estimation. | There are many ways to estimate instrumental variables, not just two stage least squares. Different estimators such as GMM or k-class limited-information maximum likelihood estimators perform better or worse depending on heterogeneous treatment effects, heteroskedasticity, and sample size. Many instrumental variables estimation commands allow for multiple different estimation methods, described below. Note that in the just-identified case (where the number of instruments is the same as the number of endogenous variables), several common estimators produce identical results. | . ",
    "url": "/Model_Estimation/Research_Design/instrumental_variables.html#also-consider",
    "relUrl": "/Model_Estimation/Research_Design/instrumental_variables.html#also-consider"
  },"282": {
    "doc": "Instrumental Variables",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/Research_Design/instrumental_variables.html#implementations",
    "relUrl": "/Model_Estimation/Research_Design/instrumental_variables.html#implementations"
  },"283": {
    "doc": "Instrumental Variables",
    "title": "Python",
    "content": "The easiest way to run instrument variables regressions in Python is probably the linearmodels package, although there are other packages available. # Conda install linearmodels, pandas, and numpy, if you don't have them already from linearmodels.iv import IV2SLS import pandas as pd import numpy as np df = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/AER/CigarettesSW.csv', index_col=0) # We will use cigarette taxes as an instrument for cigarette prices # to evaluate the effect of cigarette price on log number of packs smoked # With income per capita as a control # Adjust everything for inflation df['rprice'] = df['price']/df['cpi'] df['rincome'] = df['income']/df['population']/df['cpi'] df['tdiff'] = (df['taxs'] - df['tax'])/df['cpi'] # Specify formula in format of 'y ~ exog + [endog ~ instruments]'. # The '1' on the right-hand side of the formula adds a constant. formula = 'np.log(packs) ~ 1 + np.log(rincome) + [np.log(rprice) ~ tdiff]' # Specify model and data mod = IV2SLS.from_formula(formula, df) # Fit model res = mod.fit() # Show model summary res.summary . ",
    "url": "/Model_Estimation/Research_Design/instrumental_variables.html#python",
    "relUrl": "/Model_Estimation/Research_Design/instrumental_variables.html#python"
  },"284": {
    "doc": "Instrumental Variables",
    "title": "R",
    "content": "There are several ways to run instrumental variables in R. Here we will cover two - AER::ivreg(), which is probably the most common, and lfe::felm(), which is more flexible and powerful. You may also want to consider looking at estimatr::iv_robust, which combines much of the flexibility of lfe::felm() with the simple syntax of AER::ivreg(), although it is not as powerful. # If necessary, install both packages. # install.packages(c('AER','lfe')) # Load AER library(AER) # Load the Cigarettes data from ivreg, following the example data(CigarettesSW) # We will be using cigarette taxes as an instrument for cigarette prices # to evaluate the effect of cigarette price on log number of packs smoked # With income per capita as a control # Adjust everything for inflation CigarettesSW$rprice &lt;- CigarettesSW$price/CigarettesSW$cpi CigarettesSW$rincome &lt;- CigarettesSW$income/CigarettesSW$population/CigarettesSW$cpi CigarettesSW$tdiff &lt;- (CigarettesSW$taxs - CigarettesSW$tax)/CigarettesSW$cpi # The regression formula takes the format # dependent.variable ~ endogenous.variables + controls | instrumental.variables + controls ivmodel &lt;- ivreg(log(packs) ~ log(rprice) + log(rincome) | tdiff + log(rincome), data = CigarettesSW) summary(ivmodel) # Now we will run the same model with lfe::felm library(lfe) # The regression formula takes the format # dependent vairable ~ # controls | # fixed.effects | # (endogenous.variables ~ instruments) | # clusters.for.standard.errors # So if need be it is straightforward to adjust this example to account for # fixed effects and clustering. # Note the 0 indicating no fixed effects ivmodel2 &lt;- felm(log(packs) ~ log(rincome) | 0 | (log(rprice) ~ tdiff), data = CigarettesSW) summary(ivmodel2) # felm can also use several k-class estimation methods; see help(felm) for the full list. # Let's run it with a limited-information maximum likelihood estimator with # the fuller adjustment set to minimize squared error (4). ivmodel3 &lt;- felm(log(packs) ~ log(rincome) | 0 | (log(rprice) ~ tdiff), data = CigarettesSW, kclass = 'liml', fuller = 4) summary(ivmodel3) . ",
    "url": "/Model_Estimation/Research_Design/instrumental_variables.html#r",
    "relUrl": "/Model_Estimation/Research_Design/instrumental_variables.html#r"
  },"285": {
    "doc": "Instrumental Variables",
    "title": "Stata",
    "content": "Instrumental variables estimation in Stata typically uses the built-in ivregress command. This command can be used to implement linear instrumental variables regression using two-stage least squares, GMM, or LIML . * Get Stock and Watson Cigarette data import delimited \"https://vincentarelbundock.github.io/Rdatasets/csv/Zelig/CigarettesSW.csv\", clear * Adjust everything for inflation g rprice = price/cpi g rincome = (income/population)/cpi g tdiff = (taxs - tax)/cpi * And take logs g lpacks = ln(packs) g lrincome = ln(rincome) g lrprice = ln(rprice) * The syntax for the regression is * name_of_estimator dependent_variable controls (endogenous_variable = instruments) * where name_of_estimator can be two stage least squares (2sls), * limited information maximum likelihood (liml, note that ivregress doesn't support k-class estimators), * or generalized method of moments (gmm) * Here we can run two stage least squares ivregress 2sls lpacks rincome (lrprice = tdiff) * Or gmm. ivregress gmm lpacks rincome (lrprice = tdiff) . ",
    "url": "/Model_Estimation/Research_Design/instrumental_variables.html#stata",
    "relUrl": "/Model_Estimation/Research_Design/instrumental_variables.html#stata"
  },"286": {
    "doc": "Interaction Terms and Polynomials",
    "title": "Interaction Terms and Polynomials",
    "content": "Regression models generally assume that the outcome variable is a function of an index, which is a linear function of the independent variables, for example in ordinary least squares: . \\[Y = \\beta_0+\\beta_1X_1+\\beta_2X_2\\] However, if the independent variables have a nonlinear effect on the outcome, the model will be incorrectly specified. This is fine as long as that nonlinearity is modeled by including those nonlinear terms in the index. The two most common ways this occurs is by including interactions or polynomial terms. With an interaction, the effect of one variable varies according to the value of another: . \\[Y = \\beta_0+\\beta_1X_1+\\beta_2X_2 + \\beta_3X_1X_2\\] and with polynomial terms, the effect of one variable one the outcome is allowed to take a non-linear shape: . \\[Y = \\beta_0+\\beta_1X_1+\\beta_2X_2 + \\beta_3X_2^2 + \\beta_4X_2^3\\] ",
    "url": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html",
    "relUrl": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html"
  },"287": {
    "doc": "Interaction Terms and Polynomials",
    "title": "Keep in Mind",
    "content": ". | When you have interaction terms or polynomials, the effect of a variable can no longer be described with a single coefficient, and in some senses the individual coefficients lose meaning without the others. You can understand the effect of a single variable by taking the derivative of the index with respect to that variable. For example, in \\(Y = \\beta_0+\\beta_1X_1+\\beta_2X_2 + \\beta_3X_1X_2\\), the effect of \\(X_2\\) on \\(Y\\) is \\(\\partial Y/\\partial X_2 = \\beta_2 + \\beta_3X_1\\). You must plug in the value of \\(X_1\\) to get the effect of \\(X_2\\). Or in \\(Y = \\beta_0+\\beta_1X_1+\\beta_2X_2 + \\beta_3X_2^2 + \\beta_4X_2^3\\), the effect of \\(X_2\\) is \\(\\partial Y/\\partial X_2 = \\beta_2 + 2\\beta_3X_2 + 3\\beta_4X_2^2\\). You must plug in a value of \\(X_2\\) to get the marginal effect of \\(X_2\\) at that value. | In almost all cases, if you are including an interaction term, you should also include each of the interacted variables on their own. Otherwise, the coefficients become very difficult to interpret. | In almost all cases, if you are including a polynomial, you should include all terms of the polynomial. In other words, include the linear and squared term, not just the squared term. | . ",
    "url": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html#keep-in-mind",
    "relUrl": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html#keep-in-mind"
  },"288": {
    "doc": "Interaction Terms and Polynomials",
    "title": "Also Consider",
    "content": ". | Interaction terms tend to have low statistical power. Consider performing a power analysis of interaction terms before running your analysis. | Polynomials are not the only way to model a nonlinear relationship. You could, for example, run one of many kinds of nonparametric regression. | You may want to get the average marginal effects or the marginal effects at the mean of your variables after running your model. | One common way to display the effects of a model with interactions is to graph them. See marginal effects plots for interactions with continuous variables and Marginal effects plots for interactions with continuous variables | . ",
    "url": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html#also-consider",
    "relUrl": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html#also-consider"
  },"289": {
    "doc": "Interaction Terms and Polynomials",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html#implementations",
    "relUrl": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html#implementations"
  },"290": {
    "doc": "Interaction Terms and Polynomials",
    "title": "Python",
    "content": "Using the statsmodels package, we can use a similar formulation as the R example below. # Standard imports import numpy as np import pandas as pd import statsmodels.formula.api as sms from matplotlib import pyplot as plt # Load the R mtcars dataset from a URL df = pd.read_csv('https://raw.githubusercontent.com/LOST-STATS/lost-stats.github.io/source/Data/mtcars.csv') # Include a linear, squared, and cubic term using the I() function. # N.B. Python uses ** for exponentiation (^ means bitwise xor) model1 = sms.ols('mpg ~ hp + I(hp**2) + I(hp**3) + cyl', data=df) print(model1.fit().summary()) # Include an interaction term and the variables by themselves using * # The interaction term is represented by hp:cyl model2 = sms.ols('mpg ~ hp * cyl', data=df) print(model2.fit().summary()) # Equivalently, you can request \"all quadratic interaction terms\" by doing model3 = sms.ols('mpg ~ (hp + cyl) ** 2', data=df) print(model3.fit().summary()) # Include only the interaction term and not the variables themselves with : # Hard to interpret! Occasionally useful though. model4 = sms.ols('mpg ~ hp : cyl', data=df) print(model4.fit().summary()) . ",
    "url": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html#python",
    "relUrl": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html#python"
  },"291": {
    "doc": "Interaction Terms and Polynomials",
    "title": "R",
    "content": "# Load mtcars data data(mtcars) # Include a linear, squared, and cubic term using the I() function model1 &lt;- lm(mpg ~ hp + I(hp^2) + I(hp^3) + cyl, data = mtcars) # Include a linear, squared, and cubic term using the poly() function # The raw = TRUE option will give the exact same result as model1 # Omitting this will give you orthogonal polynomial terms, # which are not correlated with each other but are more difficult to interpret model2 &lt;- lm(mpg ~ poly(hp, 3, raw = TRUE) + cyl, data = mtcars) # Include an interaction term and the variables by themselves using * model3 &lt;- lm(mpg ~ hp*cyl, data = mtcars) # Include only the interaction term and not the variables themselves with : # Hard to interpret! Occasionally useful though. model4 &lt;- lm(mpg ~ hp:cyl, data = mtcars) . ",
    "url": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html#r",
    "relUrl": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html#r"
  },"292": {
    "doc": "Interaction Terms and Polynomials",
    "title": "Stata",
    "content": "Stata allows interaction and polynomial terms using hashtags ## to join together variables to make interactions, or joining a variable with itself to get a polynomial. You must also specify whether each variable is continuous (prefix the variable with c.) or a factor (prefix with i.). * Load auto data sysuse auto.dta, clear * Use ## to interact variables together and also include the variables individually * foreign is a factor variable so we prefix it with i. * weight is continuous so we prefix it with c. reg mpg c.weight##i.foreign * Use # to include just the interaction term and not the variables themselves * If one is a factor, this will include the effect of the continuous variable * For each level of the factor reg mpg c.weight#i.foreign * Interact a variable with itself to create a polynomial term reg mpg c.weight##c.weight##c.weight foreign . ",
    "url": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html#stata",
    "relUrl": "/Model_Estimation/OLS/interaction_terms_and_polynomials.html#stata"
  },"293": {
    "doc": "Line Graph with Labels at the Beginning or End of Lines",
    "title": "Line Graph with Labels at the Beginning or End of Lines",
    "content": "A line graph is a common way of showing how a value changes over time (or over any other x-axis where there’s only one observation per x-axis value). It is also common to put several line graphs on the same set of axes so you can see how multiple values are changing together. When putting multiple line graphs on the same set of axes, a good idea is to label the different lines on the lines themselves, rather than in a legend, which generally makes things easier to read. ",
    "url": "/Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html",
    "relUrl": "/Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html"
  },"294": {
    "doc": "Line Graph with Labels at the Beginning or End of Lines",
    "title": "Keep in Mind",
    "content": ". | Check the resulting graph to make sure that labels are legible, visible in the graph area, and don’t overlap. | . ",
    "url": "/Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html#keep-in-mind",
    "relUrl": "/Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html#keep-in-mind"
  },"295": {
    "doc": "Line Graph with Labels at the Beginning or End of Lines",
    "title": "Also Consider",
    "content": ". | More generally, see Line graph and Styling line graphs. In particular, consider Styling line graphs in order to distinguish the lines by color, pattern, etc. in addition to labels | If there are too many lines to be able to clearly follow them, labels won’t help too much. Instead, consider Faceted graphs. | . ",
    "url": "/Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html#also-consider",
    "relUrl": "/Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html#also-consider"
  },"296": {
    "doc": "Line Graph with Labels at the Beginning or End of Lines",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html#implementations",
    "relUrl": "/Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html#implementations"
  },"297": {
    "doc": "Line Graph with Labels at the Beginning or End of Lines",
    "title": "R",
    "content": "# If necessary, install ggplot2, lubridate, and directlabels # install.packages(c('ggplot2','directlabels', 'lubridate')) library(ggplot2) library(directlabels) # Load in Google Trends Nobel Search Data # Which contains the Google Trends global search popularity index for the four # research-based Nobel prizes over a month. df &lt;- read.csv('https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Presentation/Figures/Data/Line_Graph_with_Labels_at_the_Beginning_or_End_of_Lines/Research_Nobel_Google_Trends.csv') # Properly treat our date variable as a date # Not necessary in all applications of this technique. df$date &lt;- lubridate::ymd(df$date) # Construct our standard ggplot line graph # Drawing separate lines by name # And using the log of hits for visibility ggplot(df, aes(x = date, y = log(hits), color = name)) + labs(x = \"Date\", y = \"Log of Google Trends Index\")+ geom_line()+ # Since we are about to add line labels, we don't need a legend theme(legend.position = \"none\") + # Add, from the directlabels package, # geom_dl, using method = 'last.bumpup' to put the # labels at the end, and make sure that if they intersect, # one is bumped up geom_dl(aes(label = name), method = 'last.bumpup') + # Extend the x axis so the labels are visible - # Try the graph a few times until you find a range that works scale_x_date(limits = c(min(df$date), lubridate::ymd('2019-10-25'))) . This results in: . ",
    "url": "/Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html#r",
    "relUrl": "/Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html#r"
  },"298": {
    "doc": "Line Graph with Labels at the Beginning or End of Lines",
    "title": "Stata",
    "content": "Unfortunately, performing this technique in Stata requires placing each text() label on the graph. However, this can be automated with the use of a for loop to build the code using locals. * Load in Google Trends Nobel Search Data * Which contains the Google Trends global search popularity index for the four * research-based Nobel prizes over a month. import delimited \"https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Presentation/Figures/Data/Line_Graph_with_Labels_at_the_Beginning_or_End_of_Lines/Research_Nobel_Google_Trends.csv\", clear * Convert the date variable to an actual date * (not necessary in all implementations) g ymddate = date(date, \"YMD\") * Format the new variable as a date so we see it properly on the x-axis format ymddate %td * Graph log(hits) for visibility g loghits = log(hits) * Get the different prize types to graph levelsof name, l(names) * Figure out the last time period in the data set quietly summarize ymddate local lastday = r(max) * Start constructing a local that contains all the line graphs to graph local lines * Start constructing a local that contains the text labels to add local textlabs * Loop through each one foreach n in `names' { * Add in the line graph code * by building on the local we already have (`lines') and adding a new twoway segment local lines `lines' (line loghits ymddate if name == \"`n'\") * Figure out the value this line hits on the last point on the graph quietly summ loghits if name == \"`n'\" &amp; ymddate == `lastday' * The text command takes the y-value (from the mean we just took) * the x-value (the last day on the graph), * and the text label (the name we are working with) * Plus place(r) to put it to the RIGHT of that point local textlabs `textlabs' text(`r(mean)' `lastday' \"`n'\", place(r)) } * Finally, graph our lines * with the twoway lines we've specified, followed by the text labels * We're sure to remove the legend with legend(off) * and extend the x-axis so we can see the labels with xscale(range()) quietly summarize ymddate local start = r(min) local end = r(max) + 5 twoway `lines', `textlabs' legend(off) xscale(range(`start' `end')) xtitle(\"Date\") ytitle(\"Log of Google Trends Index\") . This results in: . ",
    "url": "/Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html#stata",
    "relUrl": "/Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html#stata"
  },"299": {
    "doc": "Line Graphs",
    "title": "Line Graphs",
    "content": "A line graph is a visualization tool that shows how a value changes over time. A line graph can contain a single line or multiple lines in order to compare how multiple different values change over time. ",
    "url": "/Presentation/Figures/line_graphs.html",
    "relUrl": "/Presentation/Figures/line_graphs.html"
  },"300": {
    "doc": "Line Graphs",
    "title": "Keep in Mind",
    "content": ". | Keep things simple. With line graphs, more is not always better. It’s important that line graphs are kept clean and concise so that they can be interpreted quickly and easily. Including too many lines or axis tick marks can make your graph messy and difficult to read. | The time variable should be on the x-axis for straightforward interpretation. | . ",
    "url": "/Presentation/Figures/line_graphs.html#keep-in-mind",
    "relUrl": "/Presentation/Figures/line_graphs.html#keep-in-mind"
  },"301": {
    "doc": "Line Graphs",
    "title": "Also Consider",
    "content": ". | To enhance a basic line graph, see Styling Line Graphs and Line Graph with Labels at the Beginning or End of Lines. | . ",
    "url": "/Presentation/Figures/line_graphs.html#also-consider",
    "relUrl": "/Presentation/Figures/line_graphs.html#also-consider"
  },"302": {
    "doc": "Line Graphs",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Figures/line_graphs.html#implementations",
    "relUrl": "/Presentation/Figures/line_graphs.html#implementations"
  },"303": {
    "doc": "Line Graphs",
    "title": "Python",
    "content": "Here we will use seaborn.lineplot from the seaborn package, which builds on top of matplotlib. # Load packages import pandas as pd import matplotlib.pyplot as plt import seaborn as sns # Load in data Orange = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/datasets/Orange.csv') # Specify a line plot in Seaborn using # age and circumference on the x and y axis # and picking just Tree 1 from the data sns.lineplot(x = 'age', y = 'circumference', data = Orange.loc[Orange.Tree == 1]) # And title the axes plt.xlabel('Age (days since 12/31/1968)') plt.ylabel('Circumference') . The result is: . If we want to include all the trees on the graph, with color to distinguish them, we add a hue argument: . # Add on a hue axis to add objects of different color by tree # So we can graph all the trees sns.lineplot(x = 'age', y = 'circumference', hue = 'Tree', data = Orange) # And title the axes plt.xlabel('Age (days since 12/31/1968)') plt.ylabel('Circumference') . Which results in: . ",
    "url": "/Presentation/Figures/line_graphs.html#python",
    "relUrl": "/Presentation/Figures/line_graphs.html#python"
  },"304": {
    "doc": "Line Graphs",
    "title": "R",
    "content": "Basic Line Graph in R . To make a line graph in R, we’ll be using a dataset that’s already built in to R, called ‘Orange’. This dataset tracks the growth in circumference of several trees as they age. #load necessary packages if (!require(\"pacman\")) install.packages(\"pacman\") pacman::p_load(dplyr, lubridate) #load in dataset data(Orange) . This dataset has measurements for four different trees. To start off, we’ll only be graphing the growth of Tree #1, so we first need to subset our data. #subset data to just tree #1 tree_1_df &lt;- Orange %&gt;% filter(Tree == 1) . Then we will construct our plot using ggplot(). We’ll create our line graph using the following steps: . | First, call ggplot() and specify the tree_1_df dataset. Next, we need to specify the aesthetics of our graph (what variables go where). Do so with the aes() function, setting x = age and y = circumference. | To make the actual line of the line graph, we will add the line geom_line() to our ggplot line using the + symbol. Using the + symbol allows us to add different lines of code to the same graph in order to create new elements within it. | Putting those steps together, we get the following code resulting in our first line graph: | . ggplot(tree_1_df, aes(x = age, y = circumference)) + geom_line() . This does show us how the tree grows over time, but it’s rather plain and lacks important identifying information like a title and units of measurements for the axes. In order to enhance our graph, we again use the + symbol to add additional elements like line color, titles etc. and to change things like axis labels and title/label position. | We can specify the color of our line within the geom_line() function. | The function labs() allows us to add a title and also change the labels for the axes | Using the function theme() allows us to manipulate the apperance of our labels through the element_text function | Let’s change the line color, add a title and center it, and also add more information to our axes labels. | . ggplot(tree_1_df, aes(x = age, y = circumference)) + geom_line(color = \"orange\") + labs(x = \"Age (days since 12/31/1968)\", y = \"Circumference (mm)\", title = \"Orange Tree Circumference Growth by Age\") + theme(plot.title = element_text(hjust = 0.5)) . Line Graph with Multiple Lines in R . A great way to employ line graphs is to compare the changes of different values over the same time period. For this instance, we’ll be looking at how the four trees differ in their growth over time. We will be employing the full Orange dataset for this graph. To add multiple lines using data from the same dataframe, simply add the color argument to the aes() function within our ggplot() line. Set the color argument to the identifying variable within your data set, here, that variable is Tree, so we will set color = Tree. ggplot(Orange, aes(x = age, y = circumference, color = Tree)) + geom_line() + labs(x = \"Age (days since 12/31/1968)\", y = \"Circumference (mm)\", title = \"Orange Tree Circumference Growth by Age\") + theme(plot.title = element_text(hjust = 0.5)) . The steps will get you started with creating graphs in R. For more information on styling your graphs, again, visit Styling Line Graphs and Line Graph with Labels at the Beginning or End of Lines. Another great resource for line graph styling tips is this blog post created by Jodie Burchell. ",
    "url": "/Presentation/Figures/line_graphs.html#r",
    "relUrl": "/Presentation/Figures/line_graphs.html#r"
  },"305": {
    "doc": "Line Graphs",
    "title": "Stata",
    "content": "We can create a line graph in Stata using the twoway function with the line setting. * Load data on orange trees import delimited \"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/Orange.csv\", clear * Let's just graph the first tree using if Tree == 1 * We specify the y-axis variable circumference first followed by the x-axis variable age * We can add axis labels with xtitle and ytitle * And specify a color with lcolor (for line color) twoway line circumference age if tree == 1, xtitle(\"Age (days since 12/31/1968)\") ytitle(\"Circumference\") lcolor(red) . The result is: . We can also include all the trees on the same line graph: . * If we want all of our trees graphed on the same axis * We can specify each line separately using () * Use legend() so we know which line is which * Or label the lines directly using /Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html twoway (line circumference age if tree == 1) (line circumference age if tree == 2) (line circumference age if tree == 3) (line circumference age if tree == 4) (line circumference age if tree == 5), xtitle(\"Age (days since 12/31/1968)\") ytitle(\"Circumference\") legend(lab(1 \"Tree 1\") lab(2 \"Tree 2\") lab(3 \"Tree 3\") lab(4 \"Tree 4\") lab(5 \"Tree 5\")) . The result is: . ",
    "url": "/Presentation/Figures/line_graphs.html#stata",
    "relUrl": "/Presentation/Figures/line_graphs.html#stata"
  },"306": {
    "doc": "Linear Hypothesis Tests",
    "title": "Linear Hypothesis Tests",
    "content": "Most regression output will include the results of frequentist hypothesis tests comparing each coefficient to 0. However, in many cases, you may be interested in whether a linear sum of the coefficients is 0. For example, in the regression . \\[Outcome = \\beta_0 + \\beta_1\\times GoodThing + \\beta_2\\times BadThing\\] You may be interested to see if \\(GoodThing\\) and \\(BadThing\\) (both binary variables) cancel each other out. So you would want to do a test of \\(\\beta_1 - \\beta_2 = 0\\). Alternately, you may want to do a joint significance test of multiple linear hypotheses. For example, you may be interested in whether \\(\\beta_1\\) or \\(\\beta_2\\) are nonzero and so would want to jointly test the hypotheses \\(\\beta_1 = 0\\) and \\(\\beta_2=0\\) rather than doing them one at a time. Note the and here, since if either one or the other is rejected, we reject the null. ",
    "url": "/Model_Estimation/Statistical_Inference/linear_hypothesis_tests.html",
    "relUrl": "/Model_Estimation/Statistical_Inference/linear_hypothesis_tests.html"
  },"307": {
    "doc": "Linear Hypothesis Tests",
    "title": "Keep in Mind",
    "content": ". | Be sure to carefully interpret the result. If you are doing a joint test, rejection means that at least one of your hypotheses can be rejected, not each of them. And you don’t necessarily know which ones can be rejected! | Generally, linear hypothesis tests are performed using F-statistics. However, there are alternate approaches such as likelihood tests or chi-squared tests. Be sure you know which on you’re getting. | Conceptually, what is going on with linear hypothesis tests is that they compare the model you’ve estimated against a more restrictive one that requires your restrictions (hypotheses) to be true. If the test you have in mind is too complex for the software to figure out on its own, you might be able to do it on your own by taking the sum of squared residuals in your original unrestricted model (\\(SSR_{UR}\\)), estimate the alternate model with the restriction in place (\\(SSR_R\\)) and then calculate the F-statistic for the joint test using \\(F_{q,n-k-1} = ((SSR_R - SSR_{UR})/q)/(SSR_{UR}/(n-k-1))\\). | . ",
    "url": "/Model_Estimation/Statistical_Inference/linear_hypothesis_tests.html#keep-in-mind",
    "relUrl": "/Model_Estimation/Statistical_Inference/linear_hypothesis_tests.html#keep-in-mind"
  },"308": {
    "doc": "Linear Hypothesis Tests",
    "title": "Also Consider",
    "content": ". | The process for testing a nonlinear combination of your coefficients, for example testing if \\(\\beta_1\\times\\beta_2 = 1\\) or \\(\\sqrt{\\beta_1} = .5\\), is generally different. See Nonlinear hypothesis tests. | . ",
    "url": "/Model_Estimation/Statistical_Inference/linear_hypothesis_tests.html#also-consider",
    "relUrl": "/Model_Estimation/Statistical_Inference/linear_hypothesis_tests.html#also-consider"
  },"309": {
    "doc": "Linear Hypothesis Tests",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/Statistical_Inference/linear_hypothesis_tests.html#implementations",
    "relUrl": "/Model_Estimation/Statistical_Inference/linear_hypothesis_tests.html#implementations"
  },"310": {
    "doc": "Linear Hypothesis Tests",
    "title": "R",
    "content": "Linear hypothesis test in R can be performed for most regression models using the linearHypothesis() function in the car package. See this guide for more information. # If necessary # install.packages('car') library(car) data(mtcars) # Run our model m1 &lt;- lm(mpg ~ hp + disp + am + wt, data = mtcars) # Test a linear combination of coefficients linearHypothesis(m1, c('hp + disp = 0')) # Test joint significance of multiple coefficients linearHypothesis(m1, c('hp = 0','disp = 0')) # Test joint significance of multiple linear combinations linearHypothesis(m1, c('hp + disp = 0','am + wt = 0')) . ",
    "url": "/Model_Estimation/Statistical_Inference/linear_hypothesis_tests.html#r",
    "relUrl": "/Model_Estimation/Statistical_Inference/linear_hypothesis_tests.html#r"
  },"311": {
    "doc": "Linear Hypothesis Tests",
    "title": "Stata",
    "content": "Tests of coefficients in Stata can generally be performed using the built-in test command. * Load data sysuse auto.dta reg mpg headroom trunk prince rep78 * Make sure to run tests while the previous regression is still in memory * Test joint significance of multiple coefficients test headroom trunk * testparm does the same thing but allows wildcards to select coefficients * this will test the joint significance of every variable with an e in it testparm *e* * Test a linear combination of the coefficients test headroom + trunk = 0 * Test multiple linear combinations by accumulating them one at a time test headroom + trunk = 0 test price + rep78 = 0, accumulate . ",
    "url": "/Model_Estimation/Statistical_Inference/linear_hypothesis_tests.html#stata",
    "relUrl": "/Model_Estimation/Statistical_Inference/linear_hypothesis_tests.html#stata"
  },"312": {
    "doc": "Linear Mixed-Effects Regression",
    "title": "Linear Mixed-Effects Regression",
    "content": "Mixed-effects regression goes by many names, including hierarchical linear model, random coefficient model, and random parameter models. In a mixed-effects regression, some of the parameters are “random effects” which are allowed to vary over the sample. Others are “fixed effects”, which are not. Note that this use of the term “fixed effects” is not the same as in fixed effects regression. For example, consider the model . \\[y_{ij} = \\beta_{0j} + \\beta_{1j}X_{1ij} + \\beta_{2}X_{2ij} + e_{ij}\\] The intercept \\(\\beta_{0j}\\) has a $j$ subscript and is allowed to vary over the sample at the \\(j\\) level, where \\(j\\) may indicate individual or group, depending on context. The slope on \\(X_{1ij}\\), \\(\\beta_{1j}\\), is similarly allowed to vary over the sample. These are random effects. \\(\\beta_{2}\\) is not allowed to vary over the sample and so is fixed. The random parameters have their own “level-two” equations, which may or may not include level-two covariates. \\[\\beta_{0j} = \\gamma_{00} + \\gamma_{01}W_j + u_{0j}\\] \\[\\beta_{1j} = \\gamma_{10} + u_{1j}\\] For more information see Wikipedia. ",
    "url": "/Model_Estimation/Multilevel_Models/linear_mixed_effects_regression.html",
    "relUrl": "/Model_Estimation/Multilevel_Models/linear_mixed_effects_regression.html"
  },"313": {
    "doc": "Linear Mixed-Effects Regression",
    "title": "Keep in Mind",
    "content": ". | The assumptions necessary to use a mixed-effects model in general are the same as for most linear models. However, in addition, mixed-effects models assume that the error terms at different levels are unrelated. | At the second level, statistical power depends on the number of different \\(j\\) values there are. Mixed-effects models may perform poorly if the coefficient is allowed to vary over only a few groups. | There’s no need to stop at two levels - the second-level coefficients can also be allowed to vary at a higher level. | . ",
    "url": "/Model_Estimation/Multilevel_Models/linear_mixed_effects_regression.html#keep-in-mind",
    "relUrl": "/Model_Estimation/Multilevel_Models/linear_mixed_effects_regression.html#keep-in-mind"
  },"314": {
    "doc": "Linear Mixed-Effects Regression",
    "title": "Also Consider",
    "content": ". | There are many variations of mixed-effects models for working with non-linear data, see nonlinear mixed-effects models. | If the goal is making predictions within subgroups, you may want to consider multi-level regression with poststratification. | . ",
    "url": "/Model_Estimation/Multilevel_Models/linear_mixed_effects_regression.html#also-consider",
    "relUrl": "/Model_Estimation/Multilevel_Models/linear_mixed_effects_regression.html#also-consider"
  },"315": {
    "doc": "Linear Mixed-Effects Regression",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/Multilevel_Models/linear_mixed_effects_regression.html#implementations",
    "relUrl": "/Model_Estimation/Multilevel_Models/linear_mixed_effects_regression.html#implementations"
  },"316": {
    "doc": "Linear Mixed-Effects Regression",
    "title": "R",
    "content": "One common way to fit mixed-effects models in R is with the lmer function in the lme4 package. To fit fully Bayesian models you may want to consider instead using STAN with the rstan package. See the multi-level regression with poststratification page for more information. # Install lme4 if necessary # install.packages('lme4') # Load up lme4 library(lme4) # Load up university instructor evaluations data from lme4 data(InstEval) # We'll be treating lecture age as a numeric variable InstEval$lectage &lt;- as.numeric(InstEval$lectage) # Let's look at the relationship between lecture ratings andhow long ago the lecture took place # with a control for whether the lecture was a service lecture ols &lt;- lm(y ~ lectage + service, data = InstEval) summary(ols) # Now we will use lmer to allow the intercept to vary at the department level me1 &lt;- lmer(y ~ lectage + service + (1 | dept), data = InstEval) summary(me1) # Now we will allow the slope on lectage to vary at the department level me2 &lt;- lmer(y ~ lectage + service + (-1 + lectage | dept), data = InstEval) summary(me2) # Now both the intercept and lectage slope will vary at the department level me3 &lt;- lmer(y ~ lectage + service + (lectage | dept), data = InstEval) summary(me3) . ",
    "url": "/Model_Estimation/Multilevel_Models/linear_mixed_effects_regression.html#r",
    "relUrl": "/Model_Estimation/Multilevel_Models/linear_mixed_effects_regression.html#r"
  },"317": {
    "doc": "Linear Mixed-Effects Regression",
    "title": "Stata",
    "content": "Stata has a family of functions based around the mixed command that can estimate mixed-effects models. * Load NLS-W data sysuse nlsw88.dta, clear * We are going to estimate the relationship between hourly wage and job tenure * with a contorl for marital status * Without mixed effects reg wage tenure married * Now we will allow the intercept to vary with occupation mixed wage tenure married || occupation: * Next we will allow the slope on tenure to vary with occupation mixed wage tenure married || occupation: tenure, nocons * Now, both! mixed wage tenure married || occupation: tenure * Finally we will allow the intercept and tenure slope to vary over both occupation * and age mixed wage tenure married || occupation: tenure || age: tenure . ",
    "url": "/Model_Estimation/Multilevel_Models/linear_mixed_effects_regression.html#stata",
    "relUrl": "/Model_Estimation/Multilevel_Models/linear_mixed_effects_regression.html#stata"
  },"318": {
    "doc": "Logit Model",
    "title": "Logit Regressions",
    "content": "A logistical regression (Logit) is a statistical method for a best-fit line between a binary [0/1] outcome variable \\(Y\\) and any number of independent variables. Logit regressions follow a logistical distribution and the predicted probabilities are bounded between 0 and 1. For more information about Logit, see Wikipedia: Logit. ",
    "url": "/Model_Estimation/GLS/logit_model.html#logit-regressions",
    "relUrl": "/Model_Estimation/GLS/logit_model.html#logit-regressions"
  },"319": {
    "doc": "Logit Model",
    "title": "Keep in Mind",
    "content": ". | The beta coefficients from a logit model are maximum likelihood estimations. They are not the marginal effect, as you would see in an OLS estimation. So you cannot interpret the beta coefficient as a marginal effect of \\(X\\) on \\(Y\\). | To obtain the marginal effect, you need to perform a post-estimation command to discover the marginal effect. In general, you can ‘eye-ball’ the marginal effect by dividing the logit beta coefficient by 4. | . ",
    "url": "/Model_Estimation/GLS/logit_model.html#keep-in-mind",
    "relUrl": "/Model_Estimation/GLS/logit_model.html#keep-in-mind"
  },"320": {
    "doc": "Logit Model",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/GLS/logit_model.html#implementations",
    "relUrl": "/Model_Estimation/GLS/logit_model.html#implementations"
  },"321": {
    "doc": "Logit Model",
    "title": "Gretl",
    "content": "# Load auto data open auto.gdt # Run logit using the auto data, with mpg as the outcome variable # and headroom, trunk, and weight as predictors logit mpg const headroom trunk weight . ",
    "url": "/Model_Estimation/GLS/logit_model.html#gretl",
    "relUrl": "/Model_Estimation/GLS/logit_model.html#gretl"
  },"322": {
    "doc": "Logit Model",
    "title": "Python",
    "content": "There are a number of Python packages that can perform logit regressions but the most comprehensive is probably statsmodels. The code below is an example of how to use it. # Install pandas and statsmodels using pip or conda, if you don't already have them. import pandas as pd import statsmodels.formula.api as smf df = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/datasets/mtcars.csv', index_col=0) # Specify the model, regressing vs on mpg and cyl mod = smf.logit('vs ~ mpg + cyl', data=df) # Fit the model res = mod.fit() # Look at the results res.summary() # Compute marginal effects marg_effect = res.get_margeff(at='mean', method='dydx') # Show marginal effects marg_effect.summary() . ",
    "url": "/Model_Estimation/GLS/logit_model.html#python",
    "relUrl": "/Model_Estimation/GLS/logit_model.html#python"
  },"323": {
    "doc": "Logit Model",
    "title": "R",
    "content": "R can run a logit regression using the glm() function. However, to get marginal effects you will need to calculate them by hand or use a package. We will use the mfx package, although the margins package is another good option, which produces tidy model output. # If necessary, install the mfx package # install.packages('mfx') # mfx is only needed for the marginal effect, not the regression itself library(mfx) # Load mtcars data data(mtcars) # Use the glm() function to run logit # Here we are predicting engine type using # miles per gallon and number of cylinders as predictors my_logit &lt;- glm(vs ~ mpg + cyl, data = mtcars, family = binomial(link = 'logit')) # The family argument says we are working with binary data # and using a logit link function (rather than, say, probit) # The results summary(my_logit) # Marginal effects logitmfx(vs ~ mpg + cyl, data = mtcars) . ",
    "url": "/Model_Estimation/GLS/logit_model.html#r",
    "relUrl": "/Model_Estimation/GLS/logit_model.html#r"
  },"324": {
    "doc": "Logit Model",
    "title": "Stata",
    "content": "* Load auto data sysuse auto.dta * Logit Estimation logit foreign mpg weight headroom trunk * Recover the Marginal Effects (Beta Coefficient in OLS) margins, dydx(*) . ",
    "url": "/Model_Estimation/GLS/logit_model.html#stata",
    "relUrl": "/Model_Estimation/GLS/logit_model.html#stata"
  },"325": {
    "doc": "Logit Model",
    "title": "Logit Model",
    "content": " ",
    "url": "/Model_Estimation/GLS/logit_model.html",
    "relUrl": "/Model_Estimation/GLS/logit_model.html"
  },"326": {
    "doc": "Marginal Effects Plots for Interactions with Continuous Variables",
    "title": "Marginal Effects Plots for Interactions with Continuous Variables",
    "content": "In many contexts, the effect of one variable on another might be allowed to vary. For example, the relationship between income and mortality is nonlinear, so the effect of an additional dollar of income on mortality is different for someone earning $20,000/year than for someone earning $100,000/year. Or maybe the relationship between income and mortality differs depending on how many years of education you have. A marginal effects plot displays the effect of \\(X\\) on \\(Y\\) for different values of \\(Z\\) (or \\(X\\)). The plot will often include confidence intervals as well. The same code will often work if there’s not an explicit interaction, but you are, for example, estimating a logit model where the effect of one variable changes with the values of the others. ",
    "url": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_continuous_variables.html",
    "relUrl": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_continuous_variables.html"
  },"327": {
    "doc": "Marginal Effects Plots for Interactions with Continuous Variables",
    "title": "Keep in Mind",
    "content": ". | Interactions often have poor statistical power, and you will generally need a lot of observations to tell if the effect of $X$ on \\(Y\\) is different for two given different values of \\(Z\\). | Make sure your graph has clearly labeled axes, so readers can tell whether your y-axis is the predicted value of $Y$ or the marginal effect of \\(X\\) on \\(Y\\). | . ",
    "url": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_continuous_variables.html#keep-in-mind",
    "relUrl": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_continuous_variables.html#keep-in-mind"
  },"328": {
    "doc": "Marginal Effects Plots for Interactions with Continuous Variables",
    "title": "Also Consider",
    "content": ". | Consider performing a power analysis of interaction terms before running your analysis to see whether you have the statistical power for your interactions | Average marginal effects or marginal effects at the mean can be used to get a single marginal effect averaged over your sample, rather than showing how it varies across the sample. | Marginal effects plots for interactions with categorical variables | . ",
    "url": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_continuous_variables.html#also-consider",
    "relUrl": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_continuous_variables.html#also-consider"
  },"329": {
    "doc": "Marginal Effects Plots for Interactions with Continuous Variables",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_continuous_variables.html#implementations",
    "relUrl": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_continuous_variables.html#implementations"
  },"330": {
    "doc": "Marginal Effects Plots for Interactions with Continuous Variables",
    "title": "R",
    "content": "The interplot package can plot the marginal effect of a variable \\(X\\) (y-axis) against different values of some variable. If instead you want the predicted values of \\(Y\\) on the y-axis, look at the ggeffects package. # Install relevant packages, if necessary: # install.packages(c('ggplot2', 'interplot')) # Load in ggplot2 and interplot library(ggplot2) library(interplot) # Load in the txhousing data data(txhousing) # Estimate a regression with a nonlinear term cubic_model &lt;- lm(sales ~ listings + I(listings^2) + I(listings^3), data = txhousing) # Get the marginal effect of var1 (listings) # at different values of var2 (listings), with confidence ribbon. # This will return a ggplot object, so you can # customize using ggplot elements like labs(). interplot(cubic_model, var1 = \"listings\", var2 = \"listings\")+ labs(x = \"Number of Listings\", y = \"Marginal Effect of Listings\") # Try setting adding listings*date to the regression model # and then in interplot set var2 = \"date\" to get the effect of listings at different values of date . This results in: . ",
    "url": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_continuous_variables.html#r",
    "relUrl": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_continuous_variables.html#r"
  },"331": {
    "doc": "Marginal Effects Plots for Interactions with Continuous Variables",
    "title": "Stata",
    "content": "We will use the marginsplot command, which requires Stata 12 or higher. * Load in the National Longitudinal Survey of Youth - Women sample sysuse nlsw88.dta * Perform a regression with a nonlinear term regress wage c.tenure##c.tenure * Use margins to calculate the marginal effects * Put the variable we're interested in getting the effect of in dydx() * And the values we want to evaluate it at in at() margins, dydx(tenure) at(tenure = (0(1)26)) * (If we had interacted with another variable, say age, we would specify similarly, * with at(age = (start(count-by)end))) * Then, marginsplot * The recast() and recastci() options make the effect/CI show up as a line/area * Remove to get points/lines instead. marginsplot, xtitle(\"Tenure\") ytitle(\"Marginal Effect of Tenure\") recast(line) recastci(rarea) . This results in: . ",
    "url": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_continuous_variables.html#stata",
    "relUrl": "/Presentation/Figures/marginal_effects_plots_for_interactions_with_continuous_variables.html#stata"
  },"332": {
    "doc": "McFadden's Choice Model (Alternative-Specific Conditional Logit)",
    "title": "McFadden’s Choice Model (Alternative-Specific Conditional Logit)",
    "content": "Discrete choice models are a regression method used to predict a categorical dependent variable with more than two categories. For example, a discrete choice model might be used to predict whether someone is going to take a train, car, or bus to work. McFadden’s Choice Model is a discrete choice model that uses conditional logit, in which the variables that predict choice can vary either at the individual level (perhaps tall people are more likely to take the bus), or at the alternative level (perhaps the train is cheaper than the bus). For more information, see Wikipedia: Discrete Choice . ",
    "url": "/Model_Estimation/GLS/mcfaddens_choice_model.html#mcfaddens-choice-model-alternative-specific-conditional-logit",
    "relUrl": "/Model_Estimation/GLS/mcfaddens_choice_model.html#mcfaddens-choice-model-alternative-specific-conditional-logit"
  },"333": {
    "doc": "McFadden's Choice Model (Alternative-Specific Conditional Logit)",
    "title": "Keep in Mind",
    "content": ". | Just like other regression methods, the McFadden model does not guarantee that the estimates will be causal. Similarly, while the McFadden model is designed so that the results can be interpreted in terms of a “random utility” function, making inferences about utility functions does require additional assumptions. | The standard McFadden model assumes that the choice follows the Independence of Irrelevant Alternatives, which may be a strong assumption. There are variants of the McFadden model that relax this assumption. | If you are working with an estimation command that only allows alternative-specific predictors and not case-specific predictors, you can add them yourself by interacting the case-specific predictors with binary variables for the different alternatives. If \\(Income\\) is your case-specific variable and your alternatives are “train”, “bus”, and “car”, you’d add \\(Income \\times (mode == \"train\")\\), \\(Income \\times (mode == \"bus\")\\), and \\(Income \\times (mode == \"car\")\\) to your model. These are your case-specific predictors. | Choice model regressions often have specific demands on how your data is structured. These vary across estimation commands and software packages. However, a common one is this (others will be pointed out in specific Implementations below): The data must contain a variable indicating the choice cases (i.e. you choose a car, that’s one case, then I choose a car, that’s a different case), a variable with the alternatives being chosen between, a binary variable equal to 1 for the alternative actually chosen (this should be 1 or TRUE exactly once within each choice case), and then variables that are case-specific or alternative-specific. | . In the below table, \\(I\\) gives the choice case, \\(Alts\\) gives the options, \\(Chose\\) gives the choice, \\(X\\) is a variable that varies at the alternative level, and \\(Y\\) is a variable that varies at the case level. | I | Alts | Chose | X | Y | . | 1 | A | 1 | 10 | 3 | . | 1 | B | 0 | 20 | 3 | . | 1 | C | 0 | 10.5 | 3 | . | 2 | A | 0 | 8 | 5 | . | 2 | B | 1 | 9 | 5 | . | 3 | C | 0 | 1 | 5 | . This might be referred to as “long” choice data. “Wide” choice data is also common, and looks like: . | I | Chose | Y | XA | XB | XC | . | 1 | A | 3 | 10 | 20 | 10.5 | . | 2 | B | 5 | 8 | 9 | 1 | . ",
    "url": "/Model_Estimation/GLS/mcfaddens_choice_model.html#keep-in-mind",
    "relUrl": "/Model_Estimation/GLS/mcfaddens_choice_model.html#keep-in-mind"
  },"334": {
    "doc": "McFadden's Choice Model (Alternative-Specific Conditional Logit)",
    "title": "Also Consider",
    "content": ". | In order to relax the independence of irrelevant alternatives assumption and/or more closely model individual preferences, consider the mixed logit, nested logit or hierarchical Bayes conditional logit models. | . ",
    "url": "/Model_Estimation/GLS/mcfaddens_choice_model.html#also-consider",
    "relUrl": "/Model_Estimation/GLS/mcfaddens_choice_model.html#also-consider"
  },"335": {
    "doc": "McFadden's Choice Model (Alternative-Specific Conditional Logit)",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/GLS/mcfaddens_choice_model.html#implementations",
    "relUrl": "/Model_Estimation/GLS/mcfaddens_choice_model.html#implementations"
  },"336": {
    "doc": "McFadden's Choice Model (Alternative-Specific Conditional Logit)",
    "title": "R",
    "content": "We will implement McFadden’s choice model in R using the mlogit package, which can accept “wide” or “long” data in the mlogit.data function. # If necessary, install mlogit package # install.packages('mlogit') library(mlogit) # Get Car data, in \"wide\" choice format data(Car) # For this we need to specify the choice variable with choice # whether it's currently in wide or long format with shape # the column numbers of the alternative-specific variables with varying. # We need alt.levels to tell us what our alternatives are (1-6, as seen in choice). # We also need sep = \"\" since our wide-format variable names are type1, type2, etc. # If the variable names were type_1, type_2, etc., we'd need sep = \"_\". # If this were long data we'd also want: # the case identifier with id.var (for individuals) and/or chid.var # (for multiple choices within individuals) # And a variable indicating the alternatives with alt.var # But could skip the alt.levels and sep arguments mlogit.Car &lt;- mlogit.data(Car, choice = 'choice', shape = 'wide', varying = 5:70, alt.levels = 1:6, sep=\"\") # mlogit.Car is now in \"long\" format # Note that if we did start with \"long\" format we could probably skip the mlogit.data() step. # Now we can run the regression with mlogit(). # We \"regress\" the choice on the alternative-specific variables like type, fuel, and price # Then put a pipe separator | # and add our case-specific variables like college model &lt;- mlogit(choice ~ type + fuel + price | college, data = mlogit.Car) # Look at the results summary(model) . ",
    "url": "/Model_Estimation/GLS/mcfaddens_choice_model.html#r",
    "relUrl": "/Model_Estimation/GLS/mcfaddens_choice_model.html#r"
  },"337": {
    "doc": "McFadden's Choice Model (Alternative-Specific Conditional Logit)",
    "title": "Stata",
    "content": "Stata has the McFadden model built in. We will estimate the model using the older asclogit command as well as the cmclogit command that comes with Stata 16. These commands require “long” choice data, as described in the Keep in Mind section. * Load in car choice data webuse carchoice * To use asclogit, we \"regress\" our choice variable (purchase) * on any alternative-specific variables (dealers) * then we put our case ID variable consumerid in case() * and our variable specifying alternatives, car, in alternatives() * then finally we put any case-specific variables like gender and income, in casevars() asclogit purchase dealers, case(consumerid) alternatives(car) casevars(gender income) * To use cmclogit, we first declare our data to be choice data with cmset * specifying our case ID variable and then the set of alternatives cmset consumerid car * Now that Stata knows the structure, we can omit those parts from the asclogit * specification, but the rest stays the same! cmclogit purchase dealers, casevars(gender income) . Why bother with the cmclogit version? cmset gives you a lot more information about your data, and makes it easy to transition between different choice model types, including those incorporating panel data (each person makes multiple choices). ",
    "url": "/Model_Estimation/GLS/mcfaddens_choice_model.html#stata",
    "relUrl": "/Model_Estimation/GLS/mcfaddens_choice_model.html#stata"
  },"338": {
    "doc": "McFadden's Choice Model (Alternative-Specific Conditional Logit)",
    "title": "McFadden's Choice Model (Alternative-Specific Conditional Logit)",
    "content": " ",
    "url": "/Model_Estimation/GLS/mcfaddens_choice_model.html",
    "relUrl": "/Model_Estimation/GLS/mcfaddens_choice_model.html"
  },"339": {
    "doc": "Merging Shape Files",
    "title": "Merging Shape Files",
    "content": "When we work with spatial anaylsis, it is quite often we need to deal with data in different format and at different scales. For example, I have nc data with global pm2.5 estimation with \\(0.01\\times 0.01\\) resolution. But I want to see the pm2.5 estimation in municipal level. I need to integrate my nc file into my municipality shp file so that I can group by the data into municipal level and calculate the mean. Then, I can make a map of it. In this page, I will use Brazil’s pm2.5 estimation and its shp file in municipal level. ",
    "url": "/Geo-Spatial/merging_shape_files.html",
    "relUrl": "/Geo-Spatial/merging_shape_files.html"
  },"340": {
    "doc": "Merging Shape Files",
    "title": "Keep in Mind",
    "content": ". | It doesn’t have to be nc file to map into the shp file, any format that can read in and convert to a sf object works. But the data has to have geometry coordinates(longitude and latitude). | . ",
    "url": "/Geo-Spatial/merging_shape_files.html#keep-in-mind",
    "relUrl": "/Geo-Spatial/merging_shape_files.html#keep-in-mind"
  },"341": {
    "doc": "Merging Shape Files",
    "title": "Implementations",
    "content": " ",
    "url": "/Geo-Spatial/merging_shape_files.html#implementations",
    "relUrl": "/Geo-Spatial/merging_shape_files.html#implementations"
  },"342": {
    "doc": "Merging Shape Files",
    "title": "R",
    "content": "Unusually for LOST, the example data files cannot be accessed from the code directly. Please visit this page and download both files to your working directory before running this code. It is also strongly recommended that you find a high-powered computer or cloud service before attempting to run this code, as it requires a lot of memory. # If necesary # install.packages(c('ncdf4','sp','raster','dplyr','sf','ggplot2','reprex','ggsn')) # Load packages library(ncdf4) library(sp) library(raster) library(dplyr) library(sf) library(ggplot2) library(reprex) ### Step 1: Read in nc file as a dataframe* pm2010 = nc_open(\"GlobalGWRwUni_PM25_GL_201001_201012-RH35_Median_NoDust_NoSalt.nc\") nc.brick = brick(\"GlobalGWRwUni_PM25_GL_201001_201012-RH35_Median_NoDust_NoSalt.nc\") # Check the dimensions dim(nc.brick) # Turn into a data frame for use nc.df = as.data.frame(nc.brick[[1]], xy = T) head(nc.df) ### Step 2: Filter out a specific country. # Global data is very big. I am going to focus only on Brazil. nc.brazil = nc.df %&gt;% filter(x &gt;= -73.59 &amp; x &lt;= 34.47 &amp; y &gt;= -33.45 &amp; y &lt;= 5.16) rm(nc.df) head(nc.brazil) ### Step 3: Change the dataframe to a sf object using the st_as_sf function pm25_sf = st_as_sf(nc.brazil, coords = c(\"x\", \"y\"), crs = 4326, agr = \"constant\") rm(nc.brazil) head(pm25_sf) ### Step 4: Read in the Brazil shp file. we plan to merge to Brazil_map_2010 = st_read(\"geo2_br2010.shp\") head(Brazil_map_2010) ### Step 5: Intersect pm25 sf object with the shp file.* # Now let's use a sample from pm25 data and intersect it with the shp file. Since the sf object is huge, I recommend running the analysis on a cloud server pm25_sample = sample_n(pm25_sf, 1000, replace = FALSE) # Now look for the intersection between the pollution data and the Brazil map to merge them pm25_municipal_2010 = st_intersection(pm25_sample, Brazil_map_2010) head(pm25_municipal_2010) ### Step 6: Make a map using ggplot pm25_municipal_2010 = pm25_municipal_2010 %&gt;% select(1,6) pm25_municipal_2010 = st_drop_geometry(pm25_municipal_2010) Brazil_pm25_2010 = left_join(Brazil_map_2010, pm25_municipal_2010) ggplot(Brazil_pm25_2010) + # geom_sf creates the map we need geom_sf(aes(fill = -layer), alpha=0.8, lwd = 0, col=\"white\") + # and we fill with the pollution concentration data scale_fill_viridis_c(option = \"viridis\", name = \"PM25\") + ggtitle(\"PM25 in municipals of Brazil\")+ ggsn::blank() . ",
    "url": "/Geo-Spatial/merging_shape_files.html#r",
    "relUrl": "/Geo-Spatial/merging_shape_files.html#r"
  },"343": {
    "doc": "Nonstandard Errors",
    "title": "Nonstandard errors",
    "content": " ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/nonstandard_errors.html#nonstandard-errors",
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/nonstandard_errors.html#nonstandard-errors"
  },"344": {
    "doc": "Nonstandard Errors",
    "title": "Nonstandard Errors",
    "content": " ",
    "url": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/nonstandard_errors.html",
    "relUrl": "/Model_Estimation/Statistical_Inference/Nonstandard_Errors/nonstandard_errors.html"
  },"345": {
    "doc": "Penalized Regression",
    "title": "Penalized Regression",
    "content": "When running a regression, especially one with many predictors, the results have a tendency to overfit the data, reducing out-of-sample predictive properties. Penalized regression eases this problem by forcing the regression estimator to shrink its coefficients towards 0 in order to avoid the “penalty” term imposed on the coefficients. This process is closely related to the idea of Bayesian shrinkage, and indeed standard penalized regression results are equivalent to regression performed using certain Bayesian priors. Regular OLS selects coefficients \\(\\hat{\\beta}\\) to minimize the sum of squared errors: . \\[\\min\\sum_i(y_i - X_i\\hat{\\beta})^2\\] Non-OLS regressions similarly select coefficients to minimize a similar objective function. Penalized regression adds a penalty term \\(\\lambda\\lVert\\beta\\rVert_p\\) to that objective function, where \\(\\lambda\\) is a tuning parameter that determines how harshly to penalize coefficients, and \\(\\lVert\\beta\\rVert_p\\) is the \\(p\\)-norm of the coefficients, or \\(\\sum_j\\lvert\\beta\\rvert^p\\). \\[\\min\\left(\\sum_i(y_i - X_i\\hat{\\beta})^2 + \\lambda\\left\\lVert\\beta\\right\\rVert_p \\right)\\] Typically \\(p\\) is set to 1 for LASSO regression (least absolute shrinkage and selection operator), which has the effect of tending to set coefficients to 0, i.e. model selection, or to 2 for Ridge Regression. Elastic net regression provides a weighted mix of LASSO and Ridge penalties, commonly referring to the weight as \\(\\alpha\\). ",
    "url": "/Machine_Learning/penalized_regression.html",
    "relUrl": "/Machine_Learning/penalized_regression.html"
  },"346": {
    "doc": "Penalized Regression",
    "title": "Keep in Mind",
    "content": ". | To avoid being penalized for a constant term, or by differences in scale between variables, it is a very good idea to standardize each variable (subtract the mean and divide by the standard deviation) before running a penalized regression. | Penalized regression can be run for logit and other kinds of regression, not just linear regression. Using penalties with general linear models like logit is common. | Penalized regression coefficients are designed to improve out-of-sample prediction, but they are biased. If the goal is estimation of a parameter, rather than prediction, this should be kept in mind. A common procedure is to use LASSO to select variables, and then run regular regression models with the variables that LASSO has selected. | The \\(\\lambda\\) parameter is often chosen using cross-validation. Many penalized regression commands include an option to select \\(\\lambda\\) by cross-validation automatically. | LASSO models commonly include variables along with polynomial transformation of those variables and interactions, allowing LASSO to determine which transformations are worth keeping. | . ",
    "url": "/Machine_Learning/penalized_regression.html#keep-in-mind",
    "relUrl": "/Machine_Learning/penalized_regression.html#keep-in-mind"
  },"347": {
    "doc": "Penalized Regression",
    "title": "Also Consider",
    "content": ". | If it is not important to estimate coefficients but the goal is simply to predict an outcome, then there are many other machine learning methods that do so, and in some cases can handle higher dimensionality or work with smaller samples. | . ",
    "url": "/Machine_Learning/penalized_regression.html#also-consider",
    "relUrl": "/Machine_Learning/penalized_regression.html#also-consider"
  },"348": {
    "doc": "Penalized Regression",
    "title": "Implementations",
    "content": " ",
    "url": "/Machine_Learning/penalized_regression.html#implementations",
    "relUrl": "/Machine_Learning/penalized_regression.html#implementations"
  },"349": {
    "doc": "Penalized Regression",
    "title": "R",
    "content": "We will use the glmnet package. # Install glmnet and tidyverse if necessary # install.packages('glmnet', 'tidyverse') # Load glmnet library(glmnet) # Load iris data data(iris) # Create a matrix with all variables other than our dependent vairable, Sepal.Length # and interactions. # -1 to omit the intercept M &lt;- model.matrix(lm(Sepal.Length ~ (.)^2 - 1, data = iris)) # Add squared terms of numeric variables numeric.var.names &lt;- names(iris)[2:4] M &lt;- cbind(M,as.matrix(iris[,numeric.var.names]^2)) colnames(M)[16:18] &lt;- paste(numeric.var.names,'squared') # Create a matrix for our dependent variable too Y &lt;- as.matrix(iris$Sepal.Length) # Standardize all variables M &lt;- scale(M) Y &lt;- scale(Y) # Use glmnet to estimate penalized regression # We pick family = \"gaussian\" for linear regression; # other families work for other kinds of data, like binomial for binary data # In each case, we use cv.glmnet to pick our lambda value using cross-validation # using nfolds folds for cross-validation # Note that alpha = 1 picks LASSO cv.lasso &lt;- cv.glmnet(M, Y, family = \"gaussian\", nfolds = 20, alpha = 1) # We might want to see how the choice of lambda relates to out-of-sample error with a plot plot(cv.lasso) # After doing CV, we commonly pick the lambda.min for lambda, # which is the lambda that minimizes out-of-sample error # or lambda.1se, which is one standard error above lambda.min, # which penalizes more harshly. The choice depends on context. lasso.model &lt;- glmnet(M, Y, family = \"gaussian\", alpha = 1, lambda = cv.lasso$lambda.min) # coefficients are shown in the beta element. means LASSO dropped it lasso.model$beta # Running Ridge, or mixing the two with elastic net, simply means picking # alpha = 0 (Ridge), or 0 &lt; alpha &lt; 1 (Elastic Net) cv.ridge &lt;- cv.glmnet(M, Y, family = \"gaussian\", nfolds = 20, alpha = 0) ridge.model &lt;- glmnet(M, Y, family = \"gaussian\", alpha = 0, lambda = cv.ridge$lambda.min) cv.elasticnet &lt;- cv.glmnet(M, Y, family = \"gaussian\", nfolds = 20, alpha = .5) elasticnet.model &lt;- glmnet(M, Y, family = \"gaussian\", alpha = .5, lambda = cv.elasticnet$lambda.min) . ",
    "url": "/Machine_Learning/penalized_regression.html#r",
    "relUrl": "/Machine_Learning/penalized_regression.html#r"
  },"350": {
    "doc": "Penalized Regression",
    "title": "Stata",
    "content": "Penalized regression is one of the few machine learning algorithms that Stata does natively. This requires Stata 16. If you do not have Stata 16, you can alternately perform some forms of penalized regression by installing the lars package using ssc install lars. * Use NLSY-W data sysuse nlsw88.dta, clear * Construct all squared and interaction terms by loop so we don't have to specify them all * by hand in the regression function local numeric_vars = \"age grade hours ttl_exp tenure\" local factor_vars = \"race married never_married collgrad south smsa c_city industry occupation union\" * Add all squares foreach x in `numeric_vars' { g sq_`x' = `x'^2 } * Turn all factors into dummies so we can standardize them local faccount = 1 local dummy_vars = \"\" foreach x in `factor_vars' { xi i.`x', pre(f`count'_) local count = `count' + 1 } * Add all numeric-numeric interactions; these are easy * factor interactions would need a more thorough loop forvalues i = 1(1)5 { local next_i = `i'+1 forvalues j = `next_i'(1)5 { local namei = word(\"`numeric_vars'\",`i') local namej = word(\"`numeric_vars'\",`j') g interact_`i'_`j' = `namei'*`namej' } } * Standardize everything foreach var of varlist `numeric_vars' f*_* interact_* { qui summ `var' qui replace `var' = (`var' - r(mean))/r(sd) } * Use the lasso command to run LASSO * using sel(cv) to select lambda using cross-validation * we specify a linear model here, but logit/probit/poisson would work lasso linear wage `numeric_vars' f*_* interact_*, sel(cv) * get list of included coefficients lassocoef * We can use elasticnet to run Elastic Net * By default, alpha will be selected by cross-validation as well elasticnet linear wage `numeric_vars' f*_* interact_*, sel(cv) . ",
    "url": "/Machine_Learning/penalized_regression.html#stata",
    "relUrl": "/Machine_Learning/penalized_regression.html#stata"
  },"351": {
    "doc": "Probit Model",
    "title": "Probit Regressions",
    "content": "A Probit regression is a statistical method for a best-fit line between a binary [0/1] outcome variable \\(Y\\) and any number of independent variables. Probit regressions follow a standard normal probability distribution and the predicted values are bounded between 0 and 1. For more information about Probit, see Wikipedia: Probit. ",
    "url": "/Model_Estimation/GLS/probit_model.html#probit-regressions",
    "relUrl": "/Model_Estimation/GLS/probit_model.html#probit-regressions"
  },"352": {
    "doc": "Probit Model",
    "title": "Keep in Mind",
    "content": ". | The beta coefficients from a probit model are maximum likelihood estimations. They are not the marginal effect, as you would see in an OLS estimation. So you cannot interpret the beta coefficient as a marginal effect of \\(X\\) on \\(Y\\). | To obtain the marginal effect, you need to perform a post-estimation command to discover the marginal effect. In general, you can ‘eye-ball’ the marginal effect by dividing the probit beta coefficient by 2.5. | . ",
    "url": "/Model_Estimation/GLS/probit_model.html#keep-in-mind",
    "relUrl": "/Model_Estimation/GLS/probit_model.html#keep-in-mind"
  },"353": {
    "doc": "Probit Model",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/GLS/probit_model.html#implementations",
    "relUrl": "/Model_Estimation/GLS/probit_model.html#implementations"
  },"354": {
    "doc": "Probit Model",
    "title": "Gretl",
    "content": "# Load auto data open auto.gdt # Run probit using the auto data, with mpg as the outcome variable # and headroom, trunk, and weight as predictors probit mpg const headroom trunk weight . ",
    "url": "/Model_Estimation/GLS/probit_model.html#gretl",
    "relUrl": "/Model_Estimation/GLS/probit_model.html#gretl"
  },"355": {
    "doc": "Probit Model",
    "title": "Python",
    "content": "The statsmodels package has methods that can perform probit regressions. # Use pip or conda to install pandas and statsmodels import pandas as pd import statsmodels.formula.api as smf # Read in the data df = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/datasets/mtcars.csv', index_col=0) # Specify the model mod = smf.probit('vs ~ mpg + cyl', data=df) # Fit the model res = mod.fit() # Look at the results res.summary() # Compute marginal effects marge_effect = res.get_margeff(at='mean', method='dydx') # Show marginal effects marge_effect.summary() . ",
    "url": "/Model_Estimation/GLS/probit_model.html#python",
    "relUrl": "/Model_Estimation/GLS/probit_model.html#python"
  },"356": {
    "doc": "Probit Model",
    "title": "R",
    "content": "R can run a probit regression using the glm() function. However, to get marginal effects you will need to calculate them by hand or use a package. We will use the mfx package, although the margins package is another good option, which produces tidy model output. # If necessary, install the mfx package # install.packages('mfx') # mfx is only needed for the marginal effect, not the regression itself library(mfx) # Load mtcars data data(mtcars) # Use the glm() function to run probit # Here we are predicting engine type using # miles per gallon and number of cylinders as predictors my_probit &lt;- glm(vs ~ mpg + cyl, data = mtcars, family = binomial(link = 'probit')) # The family argument says we are working with binary data # and using a probit link function (rather than, say, logit) # The results summary(my_probit) # Marginal effects probitmfx(vs ~ mpg + cyl, data = mtcars) . ",
    "url": "/Model_Estimation/GLS/probit_model.html#r",
    "relUrl": "/Model_Estimation/GLS/probit_model.html#r"
  },"357": {
    "doc": "Probit Model",
    "title": "Stata",
    "content": "* Load auto data sysuse auto.dta * Probi Estimation probit foreign mpg weight headroom trunk * Recover the Marginal Effects (Beta Coefficient in OLS) margins, dydx(*) . ",
    "url": "/Model_Estimation/GLS/probit_model.html#stata",
    "relUrl": "/Model_Estimation/GLS/probit_model.html#stata"
  },"358": {
    "doc": "Probit Model",
    "title": "Probit Model",
    "content": " ",
    "url": "/Model_Estimation/GLS/probit_model.html",
    "relUrl": "/Model_Estimation/GLS/probit_model.html"
  },"359": {
    "doc": "Random Forest",
    "title": "Random Forest",
    "content": "Random forest is one of the most popular and powerful machine learning algorithms. A random forest works by building up a number of decision trees, each built using a bootstrapped sample and a subset of the variables/features. Each node in each decision tree is a condition on a single feature, selecting a way to split the data so as to maximize predictive accuracy. Each individual tree gives a classification. The average, or vote-counting of that classification across trees provides an overall prediction. More trees in the forest are associated with higher accuracy. A random forest classifier can be used for both classification and regression tasks. In terms of regression, it takes the average of the outputs by different trees. Random forest can work with large datasets with multiple dimensions. However, it may overfit data, especially for regression problems. ",
    "url": "/Machine_Learning/random_forest.html",
    "relUrl": "/Machine_Learning/random_forest.html"
  },"360": {
    "doc": "Random Forest",
    "title": "Keep in Mind",
    "content": ". | Individual features need to have low correlations with each other, and sometimes we may remove features that are strongly correlated with other features. | Random forest can deal with missing values, and may simply treat “missing” as another value that the variable can take. | . ",
    "url": "/Machine_Learning/random_forest.html#keep-in-mind",
    "relUrl": "/Machine_Learning/random_forest.html#keep-in-mind"
  },"361": {
    "doc": "Random Forest",
    "title": "Also Consider",
    "content": ". | If you are not familiar with decision tree, please go to the decision tree page first as decision trees are building blocks of random forests. | . ",
    "url": "/Machine_Learning/random_forest.html#also-consider",
    "relUrl": "/Machine_Learning/random_forest.html#also-consider"
  },"362": {
    "doc": "Random Forest",
    "title": "Implementations",
    "content": " ",
    "url": "/Machine_Learning/random_forest.html#implementations",
    "relUrl": "/Machine_Learning/random_forest.html#implementations"
  },"363": {
    "doc": "Random Forest",
    "title": "Python",
    "content": "Random forests can be used to perform both regression and classification tasks. In the example below, we’ll use the RandomForestClassifier from the popular sklearn machine learning library. RandomForestClassifier is an ensemble function that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. We’ll use this classifier to predict the species of iris based on its properties, using data from the iris dataset. You may need to install packages on the command line, using pip install package-name or conda install package-name, to run these examples (if you don’t already have them installed). import pandas as pd from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score # Read data df = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv\") # Prepare data X = df[[\"Sepal.Length\", \"Sepal.Width\", \"Petal.Length\", \"Petal.Width\"]] y = df[[\"Species\"]] # Split data into training and test set X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=1996 ) # Creating model using random forest model = RandomForestClassifier(max_depth=2) model.fit(X_train, y_train) # Predict values for test data y_pred = model.predict(X_test) # Evaluate model prediction print(f\"Accuracy is {accuracy_score(y_pred, y_test)*100:.2f} %\") . ",
    "url": "/Machine_Learning/random_forest.html#python",
    "relUrl": "/Machine_Learning/random_forest.html#python"
  },"364": {
    "doc": "Random Forest",
    "title": "R",
    "content": "There are a number of packages in R capable of training a random forest, including randomForest and ranger. Here we will use randomForest. We’ll be using a built-in dataset in R, called “Iris”. There are five variables in this dataset, including species, petal width and length as well as sepal length and width. #Load packages library(pacman) pacman::p_load(tidyverse, rvest, dplyr, caret, randomForest, Metrics, readr) #Read data in R data(iris) iris #Create features and target X &lt;- iris %&gt;% select(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) y &lt;- iris$Species #Split data into training and test sets index &lt;- createDataPartition(y, p=0.75, list=FALSE) X_train &lt;- X[ index, ] X_test &lt;- X[-index, ] y_train &lt;- y[index] y_test&lt;-y[-index] #Train the model iris_rf &lt;- randomForest(x = X_train, y = y_train , maxnodes = 10, ntree = 10) print(iris_rf) #Make predictions predictions &lt;- predict(iris_rf, X_test) result &lt;- X_test result['Species'] &lt;- y_test result['Prediction']&lt;- predictions head(result) #Check the classification accuracy (number of correct predictions out of total datapoints used to test the prediction) print(sum(predictions==y_test)) print(length(y_test)) print(sum(predictions==y_test)/length(y_test)) . ",
    "url": "/Machine_Learning/random_forest.html#r",
    "relUrl": "/Machine_Learning/random_forest.html#r"
  },"365": {
    "doc": "Random/Mixed Effects in Linear Regression",
    "title": "Random/Mixed Effects in Linear Regression",
    "content": "In panel data, we often have to deal with unobserved heterogeneity among the units of observation that are observed over time. If we assume that the unobserved heterogeneity is uncorrelated with the independent variables, we can use random effects model. Otherwise, we may consider fixed effects. In practice, random effects and fixed effects are often combined to implement a mixed effects model. Mixed refers to the fact that these models contain both fixed, and random effects. For more information, see Wikipedia: Random Effects Model . ",
    "url": "/Model_Estimation/Multilevel_Models/random_mixed_effects_estimation.html",
    "relUrl": "/Model_Estimation/Multilevel_Models/random_mixed_effects_estimation.html"
  },"366": {
    "doc": "Random/Mixed Effects in Linear Regression",
    "title": "Keep in Mind",
    "content": ". | To use random effects model, you must observe the same person multiple times (panel data). | If unobserved heterogeneity is correlated with independent variables, the random effects estimator is biased and inconsistent. | However, even if unobserved heterogeneity is expected to be correlated with independent variables, the fixed effects model may have high standard errors if the number of observation per unit of observation is very small. Random effects maybe considered in such cases. | Additionally, modeling the correlation between the indepdendent variables and the random effect by using variables in predicting the random effect can account for this problem | . ",
    "url": "/Model_Estimation/Multilevel_Models/random_mixed_effects_estimation.html#keep-in-mind",
    "relUrl": "/Model_Estimation/Multilevel_Models/random_mixed_effects_estimation.html#keep-in-mind"
  },"367": {
    "doc": "Random/Mixed Effects in Linear Regression",
    "title": "Also Consider",
    "content": ". | Consider Fixed effects if unobserved heterogeneity and independent variables are correlated or if only within-variation is desired. | Hauman Tests are often used to inform us about the appropiateness of fixed effects models vs. random effects models in which only the intercept is random. | Clustering your error | . ",
    "url": "/Model_Estimation/Multilevel_Models/random_mixed_effects_estimation.html#also-consider",
    "relUrl": "/Model_Estimation/Multilevel_Models/random_mixed_effects_estimation.html#also-consider"
  },"368": {
    "doc": "Random/Mixed Effects in Linear Regression",
    "title": "Implementations",
    "content": "We continue from our the example in Fixed effects. In that example we estimated a fixed effect model of the form: . \\[earnings_{it} = \\beta_0 + \\beta_1 prop\\_ working_{it} + \\delta_t + \\delta_i + \\epsilon_{it}\\] That is, average earnings of graduates of an institution depends on proportion employed, after controlling for time and institution fixed effects. But, some institutions have one observation, and the average number of observations is 5.1. We may be worried about the precision of our estimates. So, we may choose to use random effects for intercepts by institution to estimate the model even if we think \\(corr(prop\\_ working_{it}, \\delta_{i}) \\ne 0\\). That is, we choose possiblity of bias over variance. ",
    "url": "/Model_Estimation/Multilevel_Models/random_mixed_effects_estimation.html#implementations",
    "relUrl": "/Model_Estimation/Multilevel_Models/random_mixed_effects_estimation.html#implementations"
  },"369": {
    "doc": "Random/Mixed Effects in Linear Regression",
    "title": "R",
    "content": "Several packages can be used to implement a random effects model in R - such as lme4 and nlme. lme4 is more widely used. The example that follows uses the lme4 package. # If necessary, install lme4 if(!require(lme4)){install.packages(\"lme4\")} library(lme4) # Read in data from the College Scorecard df &lt;- read.csv('https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv') # Calculate proportion of graduates working df$prop_working &lt;- df$count_working/(df$count_working + df$count_not_working) # We write the mixed effect formula for estimation in lme4 as: # dependent_var ~ # covariates (that can include fixed effects) + # random effects - we need to specify if our model is random effects in intercepts or in slopes. In our example, we suspect random effects in intercepts at institutions. So we write \"...+(1 | inst_name), ....\" If we wanted to specify a model where the coefficient on prop_working was also varying by institution - we would use (1 + open | inst_name). # Here we regress average earnings graduates in an institution on prop_working, year fixed effects and random effects in intercepts for institutions. relm_model &lt;- lmer(earnings_med ~ prop_working + factor(df$year) + (1 | inst_name), data = df) # Display results summary(relm_model) # We note that comparing with the fixed effects model, our estimates are more precise. But, the correlation between X`s and errors suggest bias in our mixed effect model, and we do see a large increase in estimated beta. ",
    "url": "/Model_Estimation/Multilevel_Models/random_mixed_effects_estimation.html#r",
    "relUrl": "/Model_Estimation/Multilevel_Models/random_mixed_effects_estimation.html#r"
  },"370": {
    "doc": "Random/Mixed Effects in Linear Regression",
    "title": "Stata",
    "content": "We will estimate a mixed effects model using Stata using the built in xtreg command. * Obtain same data from Fixed Effect tutorial import delimited \"https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Fix ed_Effects_in_Linear_Regression/Scorecard.csv\", clear * Data cleaning * We are turning missings are written as \"NA\" into numeric destring count_not_working count_working earnings_med, replace force * Calculate the proportion working g prop_working = count_working/(count_working + count_not_working) * xtset requires that the individual identifier be a numeric variable encode inst_name, g(name_number) * Set the data as panel data with xtset xtset name_number * Use xtreg with the \"re\" option to run random effects on institution intercepts * Regressing earnings_med on prop_working * with random effects for name_number (implied by re) * and also year fixed effects (which we'll add manually with i.year) xtreg earnings_med prop_working i.year, re * We note that comparing with the fixed effects model, our estimates are more precise. But, correlation between X`s and errors suggest bias in our random effect model, and we do see a large increase in estimated beta. ",
    "url": "/Model_Estimation/Multilevel_Models/random_mixed_effects_estimation.html#stata",
    "relUrl": "/Model_Estimation/Multilevel_Models/random_mixed_effects_estimation.html#stata"
  },"371": {
    "doc": "Regression Discontinuity Design",
    "title": "Regression Discontinuity Design",
    "content": "Regression discontinuity (RDD) is a research design for the purposes of causal inference. It can be used in cases where treatment is assigned based on a cutoff value of a “running variable”. For example, perhaps students in a school take a test in 8th grade. Students who score 30 or below are assigned to remedial classes, while students scoring above 30 stay in regular classes. Regression discontinuity could be applied to this setting with test score as a running variable and 30 as the cutoff to look at the effects of remedial classes. Regression discontinuity works by focusing on the cutoff. It makes an estimate of what the outcome is within a narrow bandwidth to the left of the cutoff, and also makes an estimate of what the outcome is to the right of the cutoff. Then it compares them to generate a treatment effect estimate. See Wikpedia: Regression Discontinuity Design for more information. Regression discontinuity receives a lot of attention because it relies on what some consider to be plausible assumptions. If the running variable is finely measured and is not being manipulated, then one can argue that being just to the left or the right of a cutoff is effectively random (someone getting a 30 or 31 on the test can basically be down to bad luck on the day) and so this approach by itself can remove confounding from lots of factors. ",
    "url": "/Model_Estimation/Research_Design/regression_discontinuity_design.html",
    "relUrl": "/Model_Estimation/Research_Design/regression_discontinuity_design.html"
  },"372": {
    "doc": "Regression Discontinuity Design",
    "title": "Keep in Mind",
    "content": ". | There are many, many options to choose when performing an RDD. Bandwidth selection procedure, polynomial terms, bias correction, etc. etc.. Please check the help file for your command of choice closely, and ensure you know what kind of analysis you’re about to run. Don’t assume the defaults are correct. | Regression discontinuity relies on the absence of manipulation of the running variable. In the test score example, if the teachers scoring the exam nudge a few students from 30 to 31 so they can avoid remedial classes, RDD doesn’t work any more. | Because the method relies on isolating a narrow bandwidth around the cutoff, RDD doesn’t work quite the same if the running variable is discrete and split into a small number of groups. You want a running variable with a lot of different values! See Kolesár and Rothe (2018) for more information. | In order to improve statistical performance, regression discontinuity designs often incorporate information from data points far away from the cutoff to improve the estimate of what the outcome is near the cutoff. This can be done nonparametrically, but is most often done by fitting a separate polynomial function for the running variable on either side of the cutoff. A temptation is to use a very high-order polynomial (say, \\(x, x^2, x^3, x^4\\) and \\(x^5\\)) to improve fit. However, in general a low-order polynomial is probably a better idea. See Gelman and Imbens 2019 for more information. | Regression discontinuity designs are very well-suited to graphical demonstrations of the method. Software packages designed for RDD specifically will almost always provide an easy method for creating these graphs, and it is rare that you will not want to do this. However, do keep in mind that graphs can sometimes obscure meaningfully large effects. See Kirabo Jackson for an explanation. | Regression discontinuities can be sharp, where everyone to one side of the cutoff is treated and nobody on the other side is, or fuzzy, where the probability of treatment changes across the cutoff but assignment isn’t perfect. Most RDD packages can handle both. The intuition for both is similar, but the statistical properties of sharp designs are generally stronger. Fuzzy RDD can be thought of as similar to using an instrumental variables estimator in a case of imperfect random assignment in an experiment. Covariates are generally not necessary in a sharp RDD but may be advisable in a fuzzy one. | . ",
    "url": "/Model_Estimation/Research_Design/regression_discontinuity_design.html#keep-in-mind",
    "relUrl": "/Model_Estimation/Research_Design/regression_discontinuity_design.html#keep-in-mind"
  },"373": {
    "doc": "Regression Discontinuity Design",
    "title": "Also Consider",
    "content": ". | The Regression Kink Design is an extension of RDD that looks for a change in a relationship between the running variable and the outcome, i.e. the slope, at the cutoff, rather than a change in the predicted outcome. | Regression discontinuity designs are often accompanied by placebo tests, where the same RDD is run again, but with a covariate or some other non-outcome measure used as the outcome. If the RDD shows a significant effect for the covariates, this suggests that balancing did not occur properly and there may be an issue with the RDD assumptions. | Part of performing an RDD is selecting a bandwidth around the cutoff to focus on. This can be done by context, but more commonly there are data-based methods for selecting a bandwidth Check your RDD command of choice to see what methods are available for selecting a bandwidth. | . ",
    "url": "/Model_Estimation/Research_Design/regression_discontinuity_design.html#also-consider",
    "relUrl": "/Model_Estimation/Research_Design/regression_discontinuity_design.html#also-consider"
  },"374": {
    "doc": "Regression Discontinuity Design",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/Research_Design/regression_discontinuity_design.html#implementations",
    "relUrl": "/Model_Estimation/Research_Design/regression_discontinuity_design.html#implementations"
  },"375": {
    "doc": "Regression Discontinuity Design",
    "title": "Stata",
    "content": "A standard package for performing regression discontinuity in Stata is rdrobust, installable from scc. * If necessary * ssc install rdrobust * Load RDD of house elections from the R package rddtools, * and originally from Lee (2008) https://www.sciencedirect.com/science/article/abs/pii/S0304407607001121 import delimited \"https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Estimation/Data/Regression_Discontinuity_Design/house.csv\", clear * x is \"vote margin in the previous election\" and y is \"vote margin in this election\" * If we want to specify options for bandwidth selection, we can run rdbwselect directly. * Otherwise, rdrobust will run it with default options by itself * c(0) indicates that treatment is assigned at 0 (i.e. someone gets more votes than the opponent) rdbwselect y x, c(0) * Run a sharp RDD with a second-order polynomial term rdrobust y x, c(0) p(2) * Run a fuzzy RDD * We don't have a fuzzy RDD in this data, but let's create one, where * probability of treatment jumps from 20% to 60% at the cutoff g treatment = (runiform() &lt; .2)*(x &lt; 0) + (runiform() &lt; .6)*(x &gt;= 0) rdrobust y x, c(0) fuzzy(treatment) * Generate a standard RDD plot with a polynomial of 2 (default is 4) rdplot y x, c(0) p(2) . ",
    "url": "/Model_Estimation/Research_Design/regression_discontinuity_design.html#stata",
    "relUrl": "/Model_Estimation/Research_Design/regression_discontinuity_design.html#stata"
  },"376": {
    "doc": "Regression Discontinuity Design",
    "title": "R",
    "content": "There are several packages in R designed for the estimation of RDD. Three prominent options are rdd, rddtools, and rdrobust. See this article for comparisons between them in terms of their strengths and weaknesses. The article, considering the verisons of the packages available in 2017, recommends rddtools for assumption and sensitivity checks, and rdrobust for bandwidth selection and treatment effect estimation. We will consider rdrobust here. See the rddtools walkthrough for a detailed example of the use of rddtools. # If necessary # install.packages('rdrobust') library(rdrobust) # Load RDD of house elections from the R package rddtools, # and originally from Lee (2008) https://www.sciencedirect.com/science/article/abs/pii/S0304407607001121 df &lt;- read.csv(\"https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Estimation/Data/Regression_Discontinuity_Design/house.csv\") # x is \"vote margin in the previous election\" and y is \"vote margin in this election\" # If we want to specify options for bandwidth selection, we can run rdbwselect directly. # Otherwise, rdrobust will run it with default options by itself # c(0) indicates that treatment is assigned at 0 (i.e. someone gets more votes than the opponent) bandwidth &lt;- rdbwselect(df$y, df$x, c=0) # Run a sharp RDD with a second-order polynomial term rdd &lt;- rdrobust(df$y, df$x, c=0, p=2) summary(rdd) # Run a fuzzy RDD # We don't have a fuzzy RDD in this data, but let's create one, where # probability of treatment jumps from 20% to 60% at the cutoff N &lt;- nrow(df) df$treatment &lt;- (runif(N) &lt; .2)*(df$x &lt; 0) + (runif(N) &lt; .6)*(df$x &gt;= 0) rddfuzzy &lt;- rdrobust(df$y, df$x, c=0, p=2, fuzzy = df$treatment) summary(rddfuzzy) # Generate a standard RDD plot with a polynomial of 2 (default is 4) rdplot(df$y, df$x, c = 0, p = 2) . ",
    "url": "/Model_Estimation/Research_Design/regression_discontinuity_design.html#r",
    "relUrl": "/Model_Estimation/Research_Design/regression_discontinuity_design.html#r"
  },"377": {
    "doc": "Reshaping Data",
    "title": "Reshaping Data",
    "content": " ",
    "url": "/Data_Manipulation/Reshaping/reshape.html",
    "relUrl": "/Data_Manipulation/Reshaping/reshape.html"
  },"378": {
    "doc": "Reshape Panel Data from Long to Wide",
    "title": "Reshape Panel Data from Long to Wide",
    "content": "Panel data is data in which individuals are observed at multiple points in time. There are two standard ways of storing this data: . In wide format, there is one row per individual. Then, for each variable in the data set that varies over time, there is one column per time period. For example: . | Individual | FixedCharacteristic | TimeVarying1990 | TimeVarying1991 | TimeVarying1992 | . | 1 | C | 16 | 20 | 22 | . | 2 | H | 23.4 | 10 | 14 | . This format makes it easy to perform calculations across multiple years. In long format, there is one row per individual per time period: . | Individual | FixedCharacteristic | Year | TimeVarying | . | 1 | C | 1990 | 16 | . | 1 | C | 1991 | 20 | . | 1 | C | 1992 | 22 | . | 2 | H | 1990 | 23.4 | . | 2 | H | 1991 | 10 | . | 2 | H | 1992 | 14 | . This format makes it easy to run models like fixed effects. Reshaping is the method of converting wide-format data to long and vice versa. ",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html",
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html"
  },"379": {
    "doc": "Reshape Panel Data from Long to Wide",
    "title": "Keep in Mind",
    "content": ". | If your data has multiple observations per individual/time, then standard reshaping techniques generally won’t work. | It’s a good idea to check your data by directly looking at it both before and after a reshape to check that it worked properly. | . ",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html#keep-in-mind",
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html#keep-in-mind"
  },"380": {
    "doc": "Reshape Panel Data from Long to Wide",
    "title": "Also Consider",
    "content": ". | To go in the other direction, reshape from wide to long. | Determine the observation level of a data set. | . ",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html#also-consider",
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html#also-consider"
  },"381": {
    "doc": "Reshape Panel Data from Long to Wide",
    "title": "Implementations",
    "content": " ",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html#implementations",
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html#implementations"
  },"382": {
    "doc": "Reshape Panel Data from Long to Wide",
    "title": "Python",
    "content": "The pandas package has several functions to reshape data. For going from long data to wide data, there’s pivot and pivot_table, both of which are demonstrated in the example below. # Install pandas using pip or conda, if you don't already have it installed. import pandas as pd # Load WHO data on population as an example, which has 'country', 'year', # and 'population' columns. df = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/tidyr/population.csv', index_col=0) # In this example, we would like to have one row per country but the data have # multiple rows per country, each corresponding with # a year-country value of population. # Let's take a look at the first 5 rows: print(df.head()) # To reshape this into a dataframe with one country per row, we can use # the pivot function and set 'country' as the index. As we'd like to # split out years into different columns, we set columns to 'years', and the # values within this new dataframe will be population: df_wide = df.pivot(index='country', columns='year', values='population') # What if there are multiple year-country pairs? Pivot can't work # because it needs unique combinations. In this case, we can use # pivot_table which can aggregate any duplicate year-country pairs. To test it, let's # create some synthetic duplicate data for France and add it to the original # data. We'll pretend there was a second count of population that came in with # 5% higher values for all years. # Copy the data for France synth_fr_data = df.loc[df['country'] == 'France'] # Add 5% for all years synth_fr_data['population'] = synth_fr_data['population']*1.05 # Append it to the end of the original data df = pd.concat([df, synth_fr_data], axis=0) # Compute the wide data - averaging over the two estimates for France for each # year. df_wide = df.pivot_table(index='country', columns='year', values='population', aggfunc='mean') . ",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html#python",
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html#python"
  },"383": {
    "doc": "Reshape Panel Data from Long to Wide",
    "title": "R",
    "content": "There are many ways to reshape in R, including base-R reshape and the deprecated reshape2::melt and cast and tidyr::gather and spread. We will be using the tidyr package function pivot_wider, which requires tidyr version 1.0.0 or later. # install.packages('tidyr') library(tidyr) # Load in population, which has one row per country per year data(\"population\") # If we look at the data, we'll see that we have: # identifying information in \"country\", # a time indicator in \"year\", # and our values in \"population\" head(population) . Now we think: . | Think about the set of variables that contain the values we’re interested in reshaping. Here’s it’s population. This list of variable names will be our values_from argument. | Think about what we want the new variables to be called. The variable variable says which variable we’re looking at. So that will be our names_from argument. And we want to specify that each variable represents population in a given year (rather than some other variable, so we’ll add “pop_” as our names_prefix. | . pop_wide &lt;- pivot_wider(population, names_from = year, values_from = population, names_prefix = \"pop_\") . ",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html#r",
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html#r"
  },"384": {
    "doc": "Reshape Panel Data from Long to Wide",
    "title": "Stata",
    "content": "* Load blood pressure data in long format, which contains * blood pressure both before and after a treatment for some patients sysuse bplong.dta . The next steps involve thinking: . | Think about the set of variables that identify individuals. Here it’s patient. This will go in i(), so we have i(patient). | Think about the set of variables that vary across time. Here’s it’s bp. This will be one of our “stub”s. | Think about which variable separates the different time periods within individual. Here we have “when”, and this goes in j(), so we have j(when). | . * Syntax is: * reshape wide stub, i(individualvars) j(newtimevar) * So we have reshape wide bp i(patient) j(when) * Note that simply typing reshape * will show the syntax for the function . With especially large datasets, the Gtools package provides a much faster version of reshape known as greshape. The syntax can function exactly the same, though they provide alternative syntax that you may find more intuitive. * First, we will create a toy dataset that is very large to demonstrate the speed gains * If necessary, first install gtools: * ssc install gtools * Clear memory clear all * Turn on return message to see command run time set rmsg on * Set data size to 15 million observations set obs 15000000 * Create ten observations per person generate person_id = floor((_n-1)/10) * Number time periods from 1 to 10 for each person generate time_id = mod((_n-1), 10) + 1 *Create an income in each period generate income = round(rnormal(100, 20)) * Demonstrate the comparative speed of these two reshape approaches. * preserve and restore aren't a part of the reshape command; * they just store the current state of the data and then restore it, * so we can try our different reshape commands on the same data. *The traditional reshape command preserve reshape wide income, i(person_id) j(time_id) restore *The Gtools reshape command preserve greshape wide income, i(person_id) j(time_id) restore *The Gtools reshape command, alternative syntax preserve greshape wide income, by(person_id) keys(time_id) restore . Note: there is much more guidance to the usage of greshape on the Gtools reshape page. ",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html#stata",
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html#stata"
  },"385": {
    "doc": "Reshape Panel Data from Wide to Long",
    "title": "Reshape Panel Data from Wide to Long",
    "content": "Panel data is data in which individuals are observed at multiple points in time. There are two standard ways of storing this data: . In wide format, there is one row per individual. Then, for each variable in the data set that varies over time, there is one column per time period. For example: . | Individual | FixedCharacteristic | TimeVarying1990 | TimeVarying1991 | TimeVarying1992 | . | 1 | C | 16 | 20 | 22 | . | 2 | H | 23.4 | 10 | 14 | . This format makes it easy to perform calculations across multiple years. In long format, there is one row per individual per time period: . | Individual | FixedCharacteristic | Year | TimeVarying | . | 1 | C | 1990 | 16 | . | 1 | C | 1991 | 20 | . | 1 | C | 1992 | 22 | . | 2 | H | 1990 | 23.4 | . | 2 | H | 1991 | 10 | . | 2 | H | 1992 | 14 | . This format makes it easy to run models like fixed effects. Reshaping is the method of converting wide-format data to long and vice versa.. ",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html",
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html"
  },"386": {
    "doc": "Reshape Panel Data from Wide to Long",
    "title": "Keep in Mind",
    "content": ". | If your data has multiple observations per individual/time, then standard reshaping techniques generally won’t work. | It’s a good idea to check your data by directly looking at it both before and after a reshape to check that it worked properly. | . ",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html#keep-in-mind",
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html#keep-in-mind"
  },"387": {
    "doc": "Reshape Panel Data from Wide to Long",
    "title": "Also Consider",
    "content": ". | To go in the other direction, reshape from long to wide. | Determine the observation level of a data set. | . ",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html#also-consider",
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html#also-consider"
  },"388": {
    "doc": "Reshape Panel Data from Wide to Long",
    "title": "Implementations",
    "content": " ",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html#implementations",
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html#implementations"
  },"389": {
    "doc": "Reshape Panel Data from Wide to Long",
    "title": "Python",
    "content": "The most user friendly ways to use Python to reshape data from wide to long formats come from the pandas data analysis package. There’s a more easy to use wide_to_long function and melt for more complex cases. In this example, we will download the billboard dataset, which has multiple columns for different weeks when a record was in the charts (with the values in each column giving the chart position for that week). All of the columns that we would like to convert to long format begin with the prefix ‘wk’. The wide_to_long function accepts this prefix (as the stubnames= keyword parameter) and uses it to work out which columns to transform into a single column. # Install pandas using pip or conda, if you don't have it already installed import pandas as pd df = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/tidyr/billboard.csv', index_col=0) # stubnames is the prefix for the columns we want to convert to long. i is the # unique id for each row, and j will be the name of the new column. Finally, # the values from the original wide columns (the chart position) adopt the # stubname, so we rename 'wk' to 'position' in the last step. long_df = (pd.wide_to_long(df, stubnames='wk', i=['artist', 'track', 'date.entered'], j='week') .rename(columns={'wk': 'position'})) # The wide_to_long function is a special case of the 'melt' function, which # can be used in more complex cases. Here we melt any columns that have the # string 'wk' in their names. In the final step, we extract the number of weeks # from the prefix 'wk' using regex. The final dataframe is the same as above. long_df = pd.melt(df, id_vars=['artist', 'track', 'date.entered'], value_vars=[x for x in df.columns if 'wk' in x], var_name='week', value_name='position') long_df['week'] = long_df['week'].str.extract(r'(\\d+)') # A more complex case taken from the pandas docs: import numpy as np # In this case, there are two different patterns in the many columns # that we want to convert to two different long columns. We can pass # stubnames a list of these prefixes. It then splits the columns that # have the year suffix into two different long columns depending on # their first letter (A or B) # Create some synthetic data df = pd.DataFrame({\"A1970\" : {0 : \"a\", 1 : \"b\", 2 : \"c\"}, \"A1980\" : {0 : \"d\", 1 : \"e\", 2 : \"f\"}, \"B1970\" : {0 : 2.5, 1 : 1.2, 2 : .7}, \"B1980\" : {0 : 3.2, 1 : 1.3, 2 : .1}, \"X\" : dict(zip(range(3), np.random.randn(3))) }) # Set an index df[\"id\"] = df.index # Wide to multiple long columns df_long = pd.wide_to_long(df, [\"A\", \"B\"], i=\"id\", j=\"year\") . ",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html#python",
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html#python"
  },"390": {
    "doc": "Reshape Panel Data from Wide to Long",
    "title": "R",
    "content": "There are many ways to reshape in R, including base-R reshape and the deprecated reshape2::melt and cast and tidyr::gather and spread. There is also the incredibly fast data.table::melt(). We will be using the tidyr package function pivot_longer, which requires tidyr version 1.0.0 or later. # install.packages('tidyr') library(tidyr) # Load in billboard, which has one row per song # and one variable per week, for its chart position each week data(\"billboard\") # If we look at the data, we'll see that we have: # identifying information in \"artist\" and \"track\" # A variable consistent within individuals \"date.entered\" # and a bunch of variables containing position information # all named wk and then a number names(billboard) . Now we think: . | Think about the set of variables that contain time-varying information. Here’s it’s wk1-wk76. So we can give a list of all the variables we want to widen using the tidyselect helper function starts_with(): starts_with(\"wk\"). This list of variable names will be our col argument. | Think about what we want the new variables to be called. I’ll call the week time variable “week” (this will be the names_to argument), and the data values currently stored in wk1-wk76 is the “position” (values_to). | Think about the values you want to be in your new time variable. The column names are wk1-wk76 but we want the variable to have 1-76 instead, so we’ll take out the “wk” with names_prefix = \"wk\". | . billboard_long &lt;- pivot_longer(billboard, col = starts_with(\"wk\"), names_to = \"week\", names_prefix = \"wk\", values_to = \"position\", values_drop_na = TRUE) # values_drop_na says to drop any rows containing missing values of position. # If reshaping to create multiple variables, see the names_sep or names_pattern options. ",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html#r",
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html#r"
  },"391": {
    "doc": "Reshape Panel Data from Wide to Long",
    "title": "Stata",
    "content": "* Load blood pressure data in wide format, which contains * bp_before and bp_after sysuse bpwide.dta . The next steps involve thinking: . | Think about the set of variables that identify individuals. Here it’s patient. This will go in i(), so we have i(patient). | Think about the set of variables that vary across time. Here’s it’s bp_. Note the inclusion of the _, so that “before” and “after” will be our time periods. This will be one of our “stub”s. | Think about what we want the new time variable to be called. I’ll just call it “time”, and this goes in j(), so we have j(time). | . * Syntax is: * reshape long stub, i(individualvars) j(newtimevar) * So we have reshape long bp_ i(patient) j(time) s * Where the s indicates that our time variable is a string (\"before\", \"after\") * Note that simply typing reshape * will show the syntax for the function . With especially large datasets, the Gtools package provides a much faster version of reshape known as greshape. The syntax can function exactly the same, though they provide alternative syntax that you may find more intuitive. * If necessary, install gtools * ssc install gtools * First, we will create a toy dataset that is very large to demonstrate the speed gains * Clear memory clear all * Turn on return message to see command run time set rmsg on * Set data size to 15 million observations set obs 15000000 * Create an ID variable generate person_id = _n * Create 4 separate fake test scores per student generate test_score1 = round(rnormal(180, 30)) generate test_score2 = round(rnormal(180, 30)) generate test_score3 = round(rnormal(180, 30)) generate test_score4 = round(rnormal(180, 30)) * Demonstrate the comparative speed of these two reshape approaches * preserve and restore aren't a part of the reshape command; * they just store the current state of the data and then restore it, * so we can try our different reshape commands on the same data. * The traditional reshape command preserve reshape long test_score, i(person_id) j(test_number) restore *The Gtools reshape command preserve greshape long test_score, i(person_id) j(test_number) restore *The Gtools reshape command, alternative syntax preserve greshape long test_score, by(person_id) keys(test_number) restore . Note: there is much more guidance to the usage of greshape on the Gtools reshape page. ",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html#stata",
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html#stata"
  },"392": {
    "doc": "Rowwise Calculations",
    "title": "Rowwise Calculations",
    "content": "When working with a table of data, it’s not uncommon to want to perform a calculations across many columns. For example, taking the mean of a bunch of columns for each row. This is generally not difficult to do by hand if the number of variables being handled is small. For example, in most software packages, you could take the mean of columns A and B for each row by just asking for (A+B)/2. This becomes more difficult, though, when the list of variables gets too long to type out by hand, or when the calculation doesn’t play nicely with being given columns. In these cases, approaches explicitly designed for rowwise calculations are necessary. ",
    "url": "/Data_Manipulation/rowwise_calculations.html",
    "relUrl": "/Data_Manipulation/rowwise_calculations.html"
  },"393": {
    "doc": "Rowwise Calculations",
    "title": "Keep in Mind",
    "content": ". | When incorporating lots of variables, rowwise calculations often allow you to select those variables by group, such as “all variables starting with r_”. When doing this, check ahead of time to make sure you aren’t accidentally incorporating unintended variables. | . ",
    "url": "/Data_Manipulation/rowwise_calculations.html#keep-in-mind",
    "relUrl": "/Data_Manipulation/rowwise_calculations.html#keep-in-mind"
  },"394": {
    "doc": "Rowwise Calculations",
    "title": "Implementations",
    "content": " ",
    "url": "/Data_Manipulation/rowwise_calculations.html#implementations",
    "relUrl": "/Data_Manipulation/rowwise_calculations.html#implementations"
  },"395": {
    "doc": "Rowwise Calculations",
    "title": "Python",
    "content": "The pandas data analysis package provides several methods for performing row-wise (or column-wise) operations in Python. Many common operations, such as sum and mean, can be called directly (eg summing over multiple columns to create a new column). It’s useful to know the axis convention in pandas: operations that combine columns often require the user to pass axis=1 to the function, while operations that combine rows require axis=0. This convention follows the usual one for matrices of denoting individual elements first by the ith row and then by the jth column. Although not demonstrated in the example below, lambda functions can be used for more complex operations that aren’t built-in and apply to multiple rows or columns. # If necessary, install pandas using pip or conda import pandas as pd # Grab the data df = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/ggplot2/midwest.csv\", index_col=0) # Let's assume that we want to sum, row-wise, every column # that contains 'perc' in its column name and check that # the total is 300. Use a list comprehension to get only # relevant columns, sum across them (axis=1), and create a # new column to store them: df['perc_sum'] = df[[x for x in df.columns if 'perc' in x]].sum(axis=1) # We can now check whether, on aggregate, each row entry of this new column # is 300 (it's not!) df['perc_sum'].describe() . ",
    "url": "/Data_Manipulation/rowwise_calculations.html#python",
    "relUrl": "/Data_Manipulation/rowwise_calculations.html#python"
  },"396": {
    "doc": "Rowwise Calculations",
    "title": "R",
    "content": "There are a few ways to perform rowwise operations in R. If you are summing the columns or taking their mean, rowSums and rowMeans in base R are great. For something more complex, apply in base R can perform any necessary rowwise calculation, but pmap in the purrr package is likely to be faster. In all cases, the tidyselect helpers in the dplyr package can help you to select many variables by name. # If necessary # install.packages(c('purrr','ggplot2','dplyr')) # ggplot2 is only for the data data(midwest, package = 'ggplot2') # dplyr is for the tidyselect functions, the pipe %&gt;%, and select() to pick columns library(dplyr) # There are three sets of variables starting with \"perc\" - let's make sure they # add up to 300 as they maybe should # Use starts_with to select the variables # First, do it with rowSums, # either by picking column indices or using tidyselect midwest$rowsum_rowSums1 &lt;- rowSums(midwest[,c(12:16,18:20,22:26)]) midwest$rowsum_rowSums2 &lt;- midwest %&gt;% select(starts_with('perc')) %&gt;% rowSums() # Next, with apply - we're doing sum() here for the function # but it could be anything midwest$rowsum_apply &lt;- apply( midwest %&gt;% select(starts_with('perc')), MARGIN = 1, sum) # Next, two ways with purrr: library(purrr) # First, using purrr::reduce, which is good for some functions like summing # Note that . is the data set being sent by %&gt;% midwest &lt;- midwest %&gt;% mutate(rowsum_purrrReduce = reduce(select(., starts_with('perc')), `+`)) # More flexible, purrr::pmap, which works for any function # using pmap_dbl here to get a numeric variable rather than a list midwest &lt;- midwest %&gt;% mutate(rowsum_purrrpmap = pmap_dbl( select(.,starts_with('perc')), sum)) # So do we get 300? summary(midwest$rowsum_rowSums2) # Uh-oh... looks like we didn't understand the data after all. ",
    "url": "/Data_Manipulation/rowwise_calculations.html#r",
    "relUrl": "/Data_Manipulation/rowwise_calculations.html#r"
  },"397": {
    "doc": "Rowwise Calculations",
    "title": "Stata",
    "content": "Stata has a series of built-in row operations that use the egen command. See help egen for the full list, and look for functions beginning with row like rowmean. The full list includes: rowfirst and rowlast (first or last non-missing observation), rowmean, rowmedian, rowmax, rowmin, rowpctile, and rowtotal (the mean, median, max, min, given percentile, or sum of all the variables), and rowmiss and rownonmiss (the count of the number of missing or nonmissing observations across the variables). The egenmore package, which can be installed with ssc install egenmore, adds rall, rany, and rcount (checks a condition for each variable and returns whether all are true, any are true, or the number that are true), rownvals and rowsvals (number of unique values for numeric and string variables, respectively), and rsum2 (rowtotal with some additional options). * Get data on midwestern states import delimited using \"https://vincentarelbundock.github.io/Rdatasets/csv/ggplot2/midwest.csv\" * There are three sets of variables starting with \"perc\" - let's make sure they * add up to 300 as they should * Use * as a wildcard for variable names egen total_perc = rowtotal(perc*) summ total_perc * They don't! Uh oh. * Let's just check the education variables - should add up to 100 * Use - to include all variables from one to the other * based on their current order in the data egen total_ed = rowtotal(perchsd-percprof) * Oh that explains it... * These aren't exclusive categories (HSD, college overlap) * and also leaves out non-HS graduates. summ total_ed . ",
    "url": "/Data_Manipulation/rowwise_calculations.html#stata",
    "relUrl": "/Data_Manipulation/rowwise_calculations.html#stata"
  },"398": {
    "doc": "Scatterplot by Group on Shared Axes",
    "title": "Scatterplot by Group on Shared Axes",
    "content": "Scatterplots are a standard data visualization tool that allows you to look at the relationship between two variables \\(X\\) and \\(Y\\). If you want to see how the relationship between \\(X\\) and \\(Y\\) might be different for Group A as opposed to Group B, then you might want to plot the scatterplot for both groups on the same set of axes, so you can compare them. ",
    "url": "/Presentation/Figures/scatterplot_by_group_on_shared_axes.html",
    "relUrl": "/Presentation/Figures/scatterplot_by_group_on_shared_axes.html"
  },"399": {
    "doc": "Scatterplot by Group on Shared Axes",
    "title": "Keep in Mind",
    "content": ". | Scatterplots may not work well if the data is discrete, or if there are a large number of data points. | . ",
    "url": "/Presentation/Figures/scatterplot_by_group_on_shared_axes.html#keep-in-mind",
    "relUrl": "/Presentation/Figures/scatterplot_by_group_on_shared_axes.html#keep-in-mind"
  },"400": {
    "doc": "Scatterplot by Group on Shared Axes",
    "title": "Also Consider",
    "content": ". | Sometimes, instead of putting both Group A and Group B on the same set of axes, it makes more sense to plot them separately, and put the plots next to each other. See Faceted Graphs. | There are many ways to make the scatterplots of the two groups distinct. See Styling Scatterplots. | . ",
    "url": "/Presentation/Figures/scatterplot_by_group_on_shared_axes.html#also-consider",
    "relUrl": "/Presentation/Figures/scatterplot_by_group_on_shared_axes.html#also-consider"
  },"401": {
    "doc": "Scatterplot by Group on Shared Axes",
    "title": "Implementations",
    "content": " ",
    "url": "/Presentation/Figures/scatterplot_by_group_on_shared_axes.html#implementations",
    "relUrl": "/Presentation/Figures/scatterplot_by_group_on_shared_axes.html#implementations"
  },"402": {
    "doc": "Scatterplot by Group on Shared Axes",
    "title": "R",
    "content": "library(ggplot2) # Load auto data data(mtcars) # Make sure that our grouping variable is a factor # and labeled properly mtcars$Transmission &lt;- factor(mtcars$am, labels = c(\"Automatic\", \"Manual\")) # Put wt on the x-axis, mpg on the y-axis, ggplot(mtcars, aes(x = wt, y = mpg, # distinguish the Transmission values by color, color = Transmission)) + # make it a scatterplot with geom_point() geom_point()+ # And label properly labs(x = \"Car Weight\", y = \"MPG\") . This results in: . ",
    "url": "/Presentation/Figures/scatterplot_by_group_on_shared_axes.html#r",
    "relUrl": "/Presentation/Figures/scatterplot_by_group_on_shared_axes.html#r"
  },"403": {
    "doc": "Scatterplot by Group on Shared Axes",
    "title": "Stata",
    "content": "* Load auto data sysuse auto.dta * Start a twoway command * Then, for each group, put its scatter command in () * Using if to plot each group separately * And specifying mcolor or msymbol (etc.) to differentiate them twoway (scatter weight mpg if foreign == 0, mcolor(black)) (scatter weight mpg if foreign == 1, mcolor(blue)) * Add a legend option so you know what the colors mean twoway (scatter weight mpg if foreign == 0, mcolor(black)) (scatter weight mpg if foreign == 1, mcolor(blue)), legend(lab(1 Domestic) lab(2 Foreign)) xtitle(\"Weight\") ytitle(\"MPG\") . This results in: . ",
    "url": "/Presentation/Figures/scatterplot_by_group_on_shared_axes.html#stata",
    "relUrl": "/Presentation/Figures/scatterplot_by_group_on_shared_axes.html#stata"
  },"404": {
    "doc": "Simple Linear Regression",
    "title": "Simple Linear Regression",
    "content": "Ordinary Least Squares (OLS) is a statistical method that produces a best-fit line between some outcome variable \\(Y\\) and any number of predictor variables \\(X_1, X_2, X_3, ...\\). These predictor variables may also be called independent variables or right-hand-side variables. For more information about OLS, see Wikipedia: Ordinary Least Squares. ",
    "url": "/Model_Estimation/OLS/simple_linear_regression.html",
    "relUrl": "/Model_Estimation/OLS/simple_linear_regression.html"
  },"405": {
    "doc": "Simple Linear Regression",
    "title": "Keep in Mind",
    "content": ". | OLS assumes that you have specified a true linear relationship. | OLS results are not guaranteed to have a causal interpretation. Just because OLS estimates a positive relationship between \\(X_1\\) and \\(Y\\) does not necessarily mean that an increase in \\(X_1\\) will cause \\(Y\\) to increase. | OLS does not require that your variables follow a normal distribution. | . ",
    "url": "/Model_Estimation/OLS/simple_linear_regression.html#keep-in-mind",
    "relUrl": "/Model_Estimation/OLS/simple_linear_regression.html#keep-in-mind"
  },"406": {
    "doc": "Simple Linear Regression",
    "title": "Also Consider",
    "content": ". | OLS standard errors assume that the model’s error term is IID, which may not be true. Consider whether your analysis should use heteroskedasticity-robust standard errors or cluster-robust standard errors. | If your outcome variable is discrete or bounded, then OLS is by nature incorrectly specified. You may want to use probit or logit instead for a binary outcome variable, or ordered probit or ordered logit for an ordinal outcome variable. | If the goal of your analysis is predicting the outcome variable and you have a very long list of predictor variables, you may want to consider using a method that will select a subset of your predictors. A common way to do this is a penalized regression method like LASSO. | In many contexts, you may want to include interaction terms or polynomials in your regression equation. | . ",
    "url": "/Model_Estimation/OLS/simple_linear_regression.html#also-consider",
    "relUrl": "/Model_Estimation/OLS/simple_linear_regression.html#also-consider"
  },"407": {
    "doc": "Simple Linear Regression",
    "title": "Implementations",
    "content": " ",
    "url": "/Model_Estimation/OLS/simple_linear_regression.html#implementations",
    "relUrl": "/Model_Estimation/OLS/simple_linear_regression.html#implementations"
  },"408": {
    "doc": "Simple Linear Regression",
    "title": "Gretl",
    "content": "# Load auto data open https://github.com/LOST-STATS/lost-stats.github.io/blob/master/Data/auto.gdt # Run OLS using the auto data, with mpg as the outcome variable # and headroom, trunk, and weight as predictors ols mpg const headroom trunk weight . ",
    "url": "/Model_Estimation/OLS/simple_linear_regression.html#gretl",
    "relUrl": "/Model_Estimation/OLS/simple_linear_regression.html#gretl"
  },"409": {
    "doc": "Simple Linear Regression",
    "title": "Matlab",
    "content": "% Load auto data load('https://github.com/LOST-STATS/lost-stats.github.io/blob/master/Data/auto.mat') % Run OLS using the auto data, with mpg as the outcome variable % and headroom, trunk, and weight as predictors intercept = ones(length(headroom),1); X = [intercept headroom trunk weight]; [b,bint,r,rint,stats] = regress(mpg,X); . ",
    "url": "/Model_Estimation/OLS/simple_linear_regression.html#matlab",
    "relUrl": "/Model_Estimation/OLS/simple_linear_regression.html#matlab"
  },"410": {
    "doc": "Simple Linear Regression",
    "title": "Python",
    "content": "# Use 'pip install statsmodels' or 'conda install statsmodels' # on the command line to install the statsmodels package. # Import the relevant parts of the package: import statsmodels.api as sm import statsmodels.formula.api as smf # Get the mtcars example dataset mtcars = sm.datasets.get_rdataset(\"mtcars\").data # Fit OLS regression model to mtcars ols = smf.ols(formula='mpg ~ cyl + hp + wt', data=mtcars).fit() # Look at the OLS results print(ols.summary()) . ",
    "url": "/Model_Estimation/OLS/simple_linear_regression.html#python",
    "relUrl": "/Model_Estimation/OLS/simple_linear_regression.html#python"
  },"411": {
    "doc": "Simple Linear Regression",
    "title": "R",
    "content": "# Load Data # data(mtcars) ## Optional: automatically loaded anyway # Run OLS using the mtcars data, with mpg as the outcome variable # and cyl, hp, and wt as predictors olsmodel &lt;- lm(mpg ~ cyl + hp + wt, data = mtcars) # Look at the results summary(olsmodel) . ",
    "url": "/Model_Estimation/OLS/simple_linear_regression.html#r",
    "relUrl": "/Model_Estimation/OLS/simple_linear_regression.html#r"
  },"412": {
    "doc": "Simple Linear Regression",
    "title": "SAS",
    "content": "/* Load Data */ proc import datafile=\"C:mtcars.dbf\" out=fromr dbms=dbf; run; /* OLS regression */ proc reg; model mpg = cyl hp wt; run; . ",
    "url": "/Model_Estimation/OLS/simple_linear_regression.html#sas",
    "relUrl": "/Model_Estimation/OLS/simple_linear_regression.html#sas"
  },"413": {
    "doc": "Simple Linear Regression",
    "title": "Stata",
    "content": "* Load auto data sysuse https://github.com/LOST-STATS/lost-stats.github.io/blob/master/Data/auto.dta * Run OLS using the auto data, with mpg as the outcome variable * and headroom, trunk, and weight as predictors regress mpg headroom trunk weight . ",
    "url": "/Model_Estimation/OLS/simple_linear_regression.html#stata",
    "relUrl": "/Model_Estimation/OLS/simple_linear_regression.html#stata"
  },"414": {
    "doc": "Spatial Joins",
    "title": "Spatial Joins",
    "content": "Spatial joins are crucial for merging different types of data in geospatial analysis. For example, if you want to know how many libraries (points) are in a city, county, or state (polygon). This skill allows you to take data from different types of spatial data (vector data like points, lines, and polygons, and raster data (with a little more work)) sets and merge them together using unique identifiers. Joins are typically interesections of objects, but can be expressed in different ways. These include: equals, covers, covered by, within, touches, near, crosses, and more. These are all functions within the sf function in R or the geopandas package in Python. For more on the different types of intersections in 2D projections, see the Wikipedia page on spatial relations. ",
    "url": "/Geo-Spatial/spatial_joins.html",
    "relUrl": "/Geo-Spatial/spatial_joins.html"
  },"415": {
    "doc": "Spatial Joins",
    "title": "Keep in Mind",
    "content": ". | Geospatial packages in R and Python tend to have a large number of complex dependencies, which can make installing them painful. Best practice is to install geospatial packages in a new virtual environment. | When it comes to the package we are using in R for the US boundaries, it is much easier to install via the devtools. This will save you the trouble of getting errors when installing the data packages for the boundaries. Otherwise, your mileage may vary. When I installed USAboundariesData via USAboundaries, I received errors. | . devtools::install_github(\"ropensci/USAboundaries\") devtools::install_github(\"ropensci/USAboundariesData\") . | Note: Even with the R installation via devtools, you may be prompted to install the “USAboundariesData” package and need to restart your session. | . ",
    "url": "/Geo-Spatial/spatial_joins.html#keep-in-mind",
    "relUrl": "/Geo-Spatial/spatial_joins.html#keep-in-mind"
  },"416": {
    "doc": "Spatial Joins",
    "title": "Implementations",
    "content": " ",
    "url": "/Geo-Spatial/spatial_joins.html#implementations",
    "relUrl": "/Geo-Spatial/spatial_joins.html#implementations"
  },"417": {
    "doc": "Spatial Joins",
    "title": "Python",
    "content": "The geopandas package is the easiest way to start doing geo-spatial analysis in Python. This example of a spatial merge closely follows one from the documentation for geopandas. # Geospatial packages tend to have many elaborate dependencies. The quickest # way to get going is to use a clean virtual environment and then # 'conda install geopandas' followed by # 'conda install -c conda-forge descartes' # descartes is what allows geopandas to plot data. import geopandas as gpd # Grab a world map world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres')) # Plot the map of the world world.plot() # Grab data on cities cities = gpd.read_file(gpd.datasets.get_path('naturalearth_cities')) # We can plot the cities too - but they're just dots of lat/lon without any # context for now cities.plot() # The data don't actually need to be combined to be viewed on a map as long as # they are using the same 'crs', or coordinate reference system. # Force cities and world to share crs: cities = cities.to_crs(world.crs) # Combine them on a plot: base = world.plot(color='white', edgecolor='black') cities.plot(ax=base, marker='o', color='red', markersize=5) # We want to perform a spatial merge, but there are many kinds in 2D # projections, including withins, touches, crosses, and overlaps. We want to # use an intersects spatial join - ie we want to combine each city (a lat/lon # point) with the shapes of countries and determine which city goes in which # country (even if it's on the boundary). We use the 'sjoin' function: cities_with_country = gpd.sjoin(cities, world, how=\"inner\", op='intersects') cities_with_country.head() # name_left geometry pop_est continent \\ # Vatican City POINT (12.45339 41.90328) 62137802 Europe # San Marino POINT (12.44177 43.93610) 62137802 Europe # Rome POINT (12.48131 41.89790) 62137802 Europe # Vaduz POINT (9.51667 47.13372) 8754413 Europe # Vienna POINT (16.36469 48.20196) 8754413 Europe # name_right iso_a3 gdp_md_est # Italy ITA 2221000.0 # Italy ITA 2221000.0 # Italy ITA 2221000.0 # Austria AUT 416600.0 # Austria AUT 416600.0 . ",
    "url": "/Geo-Spatial/spatial_joins.html#python",
    "relUrl": "/Geo-Spatial/spatial_joins.html#python"
  },"418": {
    "doc": "Spatial Joins",
    "title": "R",
    "content": "Acknowledgments to Ryan A. Peek for his guide that I am reimagining for LOST. We will need a few packages to do our analysis. If you need to install any packages, do so with install.packages(‘name_of_package’), then load it if necessary. library(here) library(sf) library(dplyr) library(viridis) library(ggplot2) library(USAboundaries) library(GSODR) . | We will work with polygon data from the USA boundaries initially, then move on to climate data point data via the Global Surface Summary of the Day (gsodr) package and join them together. | We start with the boundaries of the United States to get desirable polygons to work with for our analysis. To pay homage to the states of my alma maters, we will do some analysis with Oregon, Ohio, and Michigan. | . #Selecting the United States Boundaries, but omitting Alaska, Hawaii, and Puerto Rico for it to be scaled better usa &lt;- us_boundaries(type=\"state\", resolution = \"low\") %&gt;% filter(!state_abbr %in% c(\"PR\", \"AK\", \"HI\")) #Ohio with high resolution oh &lt;- USAboundaries::us_states(resolution = \"high\", states = \"OH\") #Oregon with high resolution or &lt;- USAboundaries::us_states(resolution = \"high\", states = \"OR\") #Michigan with high resolution mi &lt;- USAboundaries::us_states(resolution = \"high\", states = \"MI\") #Insets for the identified states #Oregon or_box &lt;- st_make_grid(or, n = 1) #Ohio oh_box &lt;- st_make_grid(oh, n = 1) #Michigan mi_box &lt;- st_make_grid(mi, n = 1) #We can also include the counties boundaries within the state too! #Oregon or_co &lt;- USAboundaries::us_counties(resolution = \"high\", states = \"OR\") #Ohio oh_co &lt;- USAboundaries::us_counties(resolution = \"high\", states = \"OH\") #Michigan mi_co &lt;- USAboundaries::us_counties(resolution = \"high\", states = \"MI\") . Now we can plot it out. Oregon highlighted . plot(usa$geometry) plot(or$geometry, add=T, col=\"gray50\", border=\"black\") plot(or_co$geometry, add=T, border=\"green\", col=NA) plot(or_box, add=T, border=\"yellow\", col=NA, lwd=2) . Ohio highlighted . plot(usa$geometry) plot(oh$geometry, add=T, col=\"gray50\", border=\"black\") plot(oh_co$geometry, add=T, border=\"yellow\", col=NA) plot(oh_box, add=T, border=\"blue\", col=NA, lwd=2) . Michigan highlighted . plot(usa$geometry) plot(mi$geometry, add=T, col=\"gray50\", border=\"black\") plot(mi_co$geometry, add=T, border=\"gray\", col=NA) plot(mi_box, add=T, border=\"green\", col=NA, lwd=2) . All three highlighted at once. plot(usa$geometry) plot(mi$geometry, add=T, col=\"gray50\", border=\"black\") plot(mi_co$geometry, add=T, border=\"gray\", col=NA) plot(mi_box, add=T, border=\"green\", col=NA, lwd=2) plot(oh$geometry, add=T, col=\"gray50\", border=\"black\") plot(oh_co$geometry, add=T, border=\"yellow\", col=NA) plot(oh_box, add=T, border=\"blue\", col=NA, lwd=2) plot(or$geometry, add=T, col=\"gray50\", border=\"black\") plot(or_co$geometry, add=T, border=\"green\", col=NA) plot(or_box, add=T, border=\"yellow\", col=NA, lwd=2) . Now that there are polygons established and identified, we can add in some point data to join to our currently existing polygon data and do some analysis with it. To do this we will use the Global Surface Summary of the Day (gsodr) package for climate data. We will take the metadata from the GSODR package via ‘isd_history’, make it spatial data, then filter out only those observations in our candidate states of Oregon, Ohio, and Michigan. load(system.file(\"extdata\", \"isd_history.rda\", package = \"GSODR\")) #We want this to be spatial data isd_history &lt;- as.data.frame(isd_history) %&gt;% st_as_sf(coords=c(\"LON\",\"LAT\"), crs=4326, remove=FALSE) #There are many observations, so we want to narrow it to our three candidate states isd_history_or &lt;- dplyr::filter(isd_history, CTRY==\"US\", STATE==\"OR\") isd_history_oh &lt;- dplyr::filter(isd_history, CTRY==\"US\", STATE==\"OH\") isd_history_mi &lt;- dplyr::filter(isd_history, CTRY==\"US\", STATE==\"MI\") . This filtering should take you from around 26,700 observation sites around the world to approximately 200 in Michigan, 85 in Ohio, and 100 in Oregon. These numbers may vary based on when you independently do your analysis. Let’s see these stations plotted in each state individually: . Note: the codes in the ‘border’ and ‘bg’ identifiers are from the viridis package. You can get some awesome color scales using that package. You can also use standard names. Oregon . plot(isd_history_or$geometry, cex=0.5) plot(or$geometry, col=alpha(\"gray\", 0.5), border=\"#1F968BFF\", lwd=1.5, add=TRUE) plot(isd_history_or$geometry, add=T, pch=21, bg=\"#FDE725FF\", cex=0.7, col=\"black\") title(\"Oregon GSOD Climate Stations\") . Ohio . plot(isd_history_oh$geometry, cex=0.5) plot(oh$geometry, col=alpha(\"red\", 0.5), border=\"gray\", lwd=1.5, add=TRUE) plot(isd_history_oh$geometry, add=T, pch=21, bg=\"black\", cex=0.7, col=\"black\") title(\"Ohio GSOD Climate Stations\") . Michigan . plot(isd_history_mi$geometry, cex=0.5) plot(mi$geometry, col=alpha(\"green\", 0.5), border=\"blue\", lwd=1.5, add=TRUE) plot(isd_history_mi$geometry, add=T, pch=21, bg=\"white\", cex=0.7, col=\"black\") title(\"Michigan GSOD Climate Stations\") . ",
    "url": "/Geo-Spatial/spatial_joins.html#r",
    "relUrl": "/Geo-Spatial/spatial_joins.html#r"
  },"419": {
    "doc": "Spatial Joins",
    "title": "Now, for the magic:",
    "content": "We are going to start with selecting polygons from points. This is not necessarily merging the data together, but using a spatial join to filter out polygons (counties, states, etc.) from points (climate data stations) . We will start by selecting the Oregon counties that have climate data stations within their boundaries: . or_co_isd_poly &lt;- or_co[isd_history, ] plot(or_co_isd_poly$geometry, col=alpha(\"green\",0.7)) title(\"Oregon Counties with GSOD Climate Stations\") . Now for all of our three candidate states: . cand_co &lt;- USAboundaries::us_counties(resolution = \"high\", states = c(\"OR\", \"OH\", \"MI\")) cand_co_isd_poly &lt;- cand_co[isd_history, ] plot(cand_co_isd_poly$geometry, col=alpha(\"blue\",0.7)) title(\"Counties in Candidate States with GSOD Climate Stations\") . We see how we can filter out polygons from attributes or intersecting relationships with points, but what if we want to merge data from the points into the polygon or vice versa? . We will use the data set for Oregon for the join example. Notice in our point dataset that there are no county names. Only station/city names. Let us join the county polygons with the climate station points and add the county names to the station data. We do this using the st_join function, which comes from the sf package. isd_or_co_pts &lt;- st_join(isd_history, left = FALSE, or_co[\"name\"]) #Rename the county name variable county instead of name, since we already have NAME for the station location colnames(isd_or_co_pts)[which(names(isd_or_co_pts) == \"name\")] &lt;- \"county\" plot(isd_or_co_pts$geometry, pch=21, cex=0.7, col=\"black\", bg=\"orange\") plot(or_co$geometry, border=\"gray\", col=NA, add=T) . You now have successfully joined the county name data into your new point data set! Those points in the plot now contain the county information for data analysis purposes. You can join in any attribute you would like, or by leaving it as: . isd_or_co_pts &lt;- st_join(isd_history, left = FALSE, or_co) . You add all attributes from the polygon into the point data frame! . Also note that st_join is the default function that joins any type of intersection. You can be more precise our particular about your conditions with the other spatial joins: . st_within only joins elements that are completely within the defined area . st_equal only joins elements that are spatially equal. Meaning that A is within B and B is within A. You can use these to pare down your selections and joins to specific relationships. Good luck with your geospatial analysis! . ",
    "url": "/Geo-Spatial/spatial_joins.html#now-for-the-magic",
    "relUrl": "/Geo-Spatial/spatial_joins.html#now-for-the-magic"
  },"420": {
    "doc": "Styling Line Graphs",
    "title": "Styling Line Graphs",
    "content": "There are several ways of styling line graphs. The following examples demonstrate how to modify the appearances of the lines (type and sizes), as well chart titles and axes labels. ",
    "url": "/Presentation/Figures/styling_line_graphs.html",
    "relUrl": "/Presentation/Figures/styling_line_graphs.html"
  },"421": {
    "doc": "Styling Line Graphs",
    "title": "Keep in Mind",
    "content": ". | To get started on how to plot line graphs, see here. | Elements for customization include line thickness, line type (solid, dashed, etc.), shade, transparency, and color. | Color is one of the easiest ways to distinguish a large number of line graphs. If you have many line graphs overlaid and have to use black-and-white, consider different shades of black/gray. | . ",
    "url": "/Presentation/Figures/styling_line_graphs.html#keep-in-mind",
    "relUrl": "/Presentation/Figures/styling_line_graphs.html#keep-in-mind"
  },"422": {
    "doc": "Styling Line Graphs",
    "title": "Implementation",
    "content": " ",
    "url": "/Presentation/Figures/styling_line_graphs.html#implementation",
    "relUrl": "/Presentation/Figures/styling_line_graphs.html#implementation"
  },"423": {
    "doc": "Styling Line Graphs",
    "title": "R",
    "content": "## If necessary ## install.packages(c('ggplot2','cowplot')) ## load packages library(ggplot2) ## Cowplot is just to join together the four graphs at the end library(cowplot) ## load data (the Economics dataset comes with ggplot2) eco_df &lt;- economics ## basic plot p1 &lt;- ggplot() + geom_line(aes(x=date, y = uempmed), data = eco_df) p1 ## Change line color and chart labels ## Note here that color is *outside* of the aes() argument, and so this will color the line ## If color were instead *inside* aes() and set to a factor variable, ggplot would create ## a different line for each value of the factor variable, colored differently. p2 &lt;- ggplot() + ## choose a color of preference geom_line(aes(x=date, y = uempmed), color = \"navyblue\", data = eco_df) + ## add chart title and change axes labels labs( title = \"Median Duration of Unemployment\", x = \"Date\", y = \"\") + ## Add a ggplot theme theme_light() ## center the chart title theme(plot.title = element_text(hjust = 0.5)) + p2 ## plotting multiple charts (of different line types and sizes) p3 &lt;-ggplot() + geom_line(aes(x=date, y = uempmed), color = \"navyblue\", size = 1.5, data = eco_df) + geom_line(aes(x=date, y = psavert), color = \"red2\", linetype = \"dotted\", size = 0.8, data = eco_df) + labs( title = \"Unemployment Duration (Blue) and Savings Rate (Red)\", x = \"Date\", y = \"\") + theme_light() + theme(plot.title = element_text(hjust = 0.5)) p3 ## Plotting a different line type for each group ## There isn't a natural factor in this data so let's just duplicate the data and make one up eco_df$fac &lt;- factor(1, levels = c(1,2)) eco_df2 &lt;- eco_df eco_df2$fac &lt;- 2 eco_df2$uempmed &lt;- eco_df2$uempmed - 2 + rnorm(nrow(eco_df2)) eco_df &lt;- rbind(eco_df, eco_df2) p4 &lt;- ggplot() + ## This time, color goes inside aes geom_line(aes(x=date, y = uempmed, color = fac), data = eco_df) + ## add chart title and change axes labels labs( title = \"Median Duration of Unemployment\", x = \"Date\", y = \"\") + ## Add a ggplot theme theme_light() + ## center the chart title theme(plot.title = element_text(hjust = 0.5), ## Move the legend onto some blank space on the diagram legend.position = c(.25,.8), ## And put a box around it legend.background = element_rect(color=\"black\")) + ## Retitle the legend that pops up to explain the discrete (factor) difference in colors ## (note if we just want a name change we could do guides(color = guide_legend(title = 'Random Factor')) instead) scale_color_manual(name = \"Random Factor\", # And specify the colors for the factor levels (1 and 2) by hand if we like values = c(\"1\" = \"red\", \"2\" = \"blue\")) p4 # Put them all together with cowplot for LOST upload plot_grid(p1,p2,p3,p4, nrow=2) . The four plots generated by the code are (in order p1, p2, then p3 and p4): . ",
    "url": "/Presentation/Figures/styling_line_graphs.html#r",
    "relUrl": "/Presentation/Figures/styling_line_graphs.html#r"
  },"424": {
    "doc": "2x2 Difference in Difference",
    "title": "INTRODUCTION",
    "content": "Causal inference with cross-sectional data is fundamentally tricky. | People, firms, etc. are different from one another in lots of ways. | Can only get a clean comparison when you have a (quasi-)experimental setup, such as an experiment or an regression discontinuity. | . Difference-in-difference makes use of a treatment that was applied to one group at a given time but not another group. It compares how each of those groups changed over time (comparing them to themselves to eliminate between-group differences) and then compares the treatment group difference to the control group difference (both of which contain the same time gaps, eliminating differences over time). ",
    "url": "/Model_Estimation/Research_Design/two_by_two_difference_in_difference.html#introduction",
    "relUrl": "/Model_Estimation/Research_Design/two_by_two_difference_in_difference.html#introduction"
  },"425": {
    "doc": "2x2 Difference in Difference",
    "title": "KEEP IN MIND",
    "content": ". | For Difference-in-differences to work, parallel trends must hold. That is, nothing else should be changing the gap between treated and control states at the same time as the treatment. While it is not a formal test of parallel trends, researchers often look at whether the gap between treated and control states is constant in pre-treatment years. | Suppose in \\(t = 0\\) (“Pre-period”), and \\(t = 1\\) (“Post-period”). We want to estimate \\(\\tau = Post - Pre\\), or \\(Y(post)-Y(pre)= Y(t=1)-Y(t=0)=\\tau\\). | . ",
    "url": "/Model_Estimation/Research_Design/two_by_two_difference_in_difference.html#keep-in-mind",
    "relUrl": "/Model_Estimation/Research_Design/two_by_two_difference_in_difference.html#keep-in-mind"
  },"426": {
    "doc": "2x2 Difference in Difference",
    "title": "ALSO CONSIDER",
    "content": ". | This page discusses “2x2” difference-in-difference design, meaning there are two groups, and treatment occurs at a single point in time. Many difference-in-difference applications instead use many groups, and treatments that are implemented at different times (a “rollout” design). Traditionally these models have been estimated using fixed effects for group and time period, i.e. “two-way” fixed effects. However, this approach with difference-in-difference can heavily bias results if treatment effects differ across groups, and alternate estimators are preferred. See Goodman-Bacon 2018 and Callaway and Sant’anna 2019. | . ",
    "url": "/Model_Estimation/Research_Design/two_by_two_difference_in_difference.html#also-consider",
    "relUrl": "/Model_Estimation/Research_Design/two_by_two_difference_in_difference.html#also-consider"
  },"427": {
    "doc": "2x2 Difference in Difference",
    "title": "IMPLEMENTATIONS",
    "content": " ",
    "url": "/Model_Estimation/Research_Design/two_by_two_difference_in_difference.html#implementations",
    "relUrl": "/Model_Estimation/Research_Design/two_by_two_difference_in_difference.html#implementations"
  },"428": {
    "doc": "2x2 Difference in Difference",
    "title": "R",
    "content": "In this case, we need to discover whether legalized marijuana could change the murder rate. Some states legalized marijuana in 2014. So we measure the how the murder rate changes from before 2014 to after between legalized states and states without legalization. Step 1: . | First of all, we need to load Data and Package, we call this data set “DiD”. library(tidyverse) library(broom) library(here) library(readxl) DiD &lt;- read_excel(\"https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Two_by_Two_Difference_in_Difference/did_crime.xlsx\")) . | . Step 2: . Notice that the data has already been collapsed to the treated-year level. That is, there is one observation of the murder rate for each year for the treated states (all averaged together), and one observation of the murder rate for each year for the untreated states (all averaged together). We create the indicator variable called after to indicate whether it is in the treated period of being after the year of 2014 (1), or the before period of between 2000-2013 (0). The variable treat indicates that the state legalizes marijuana in 2014. Notice that treat = 1 in these states even before 2014. If the year is after 2014 and the state decided to legalize marijuana, the indicator variable “treatafter” is “1” . DiD &lt;- DiD %&gt;% mutate(after = year &gt;= 2014) %&gt;% mutate(treatafter = after*treat) . Step 3: . Then we need to plot the graph to visualize the impact of legalize marijuana on murder rate by using ggplot. mt &lt;- ggplot(DiD,aes(x=year, y=murder, color = treat)) + geom_point(size=3)+geom_line() + geom_vline(xintercept=2014,lty=4) + labs(title=\"Murder and Time\", x=\"Year\", y=\"Murder Rate\") mt . It looks like, before the legalization occurred, murder rates in treated and untreated states were very similar, lending plausibility to the parallel trends assumption. Step 4: . We need to measure the impact of impact of legalize marijuana. If we include treat, after, and treatafter in a regression, the coefficient on treatafter can be interpreted as “how much bigger was the before-after difference for the treated group?” which is the DiD estimate. reg&lt;-lm(murder ~ treat+treatafter+after, data = DiD) summary(reg) . After legalization, the murder rate dropped by 0.3% more in treated than untreated states, suggesting that legalization reduced the murder rate. ",
    "url": "/Model_Estimation/Research_Design/two_by_two_difference_in_difference.html#r",
    "relUrl": "/Model_Estimation/Research_Design/two_by_two_difference_in_difference.html#r"
  },"429": {
    "doc": "2x2 Difference in Difference",
    "title": "2x2 Difference in Difference",
    "content": " ",
    "url": "/Model_Estimation/Research_Design/two_by_two_difference_in_difference.html",
    "relUrl": "/Model_Estimation/Research_Design/two_by_two_difference_in_difference.html"
  },"430": {
    "doc": "Home",
    "title": "Home",
    "content": "# Home Welcome to the **Library of Statistical Techniques** (LOST)! LOST is a publicly-editable website with the goal of making it easy to execute statistical techniques in statistical software. Each page of the website contains a statistical technique &mdash; which may be an estimation method, a data manipulation or cleaning method, a method for presenting or visualizing results, or any of the other kinds of things that statistical software typically does. For each of those techniques, the LOST page will contain code for performing that method in a variety of packages and languages. It may also contain information (or links) with thorough descriptions of the method, but the focus here is on implementation. How can you do it in your language of choice? If there are multiple ways, how are those ways different? Is the way you used to do it outdated, or does it do something unexpected? What's the `R` equivalent of that command you know about in `Stata` or `SAS`, or vice versa? In short, LOST is a Rosetta Stone for statistical software. If you are interested in contributing to LOST, please see the [Contributing](https://lost-stats.github.io/Contributing/Contributing.html) page. LOST was originated in 2019 by Nick Huntington-Klein and is maintained by volunteer contributors. The project's GitHub page is [here](https://github.com/LOST-STATS/lost-stats.github.io). ",
    "url": "/",
    "relUrl": "/"
  }
}
