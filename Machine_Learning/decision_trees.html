
<!DOCTYPE html>
<html lang="en-US">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge">
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" href="/assets/css/just-the-docs-default.css">
    <script type="text/javascript" src="/assets/js/vendor/lunr.min.js"></script>
  <script type="text/javascript" src="/assets/js/just-the-docs.js"></script>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Decision Trees | LOST</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Decision Trees" />
<meta property="og:locale" content="en_US" />
<meta property="og:site_name" content="LOST" />
<script type="application/ld+json">
{"@type":"WebPage","url":"/Machine_Learning/decision_trees.html","headline":"Decision Trees","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } } });
  </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_CHTML">
  </script>
</head>
<body>
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
    <symbol id="svg-link" viewBox="0 0 24 24">
      <title>Link</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link">
        <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
      </svg>
    </symbol>
    <symbol id="svg-search" viewBox="0 0 24 24">
      <title>Search</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search">
        <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line>
      </svg>
    </symbol>
    <symbol id="svg-menu" viewBox="0 0 24 24">
      <title>Menu</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu">
        <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line>
      </svg>
    </symbol>
    <symbol id="svg-arrow-right" viewBox="0 0 24 24">
      <title>Expand</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right">
        <polyline points="9 18 15 12 9 6"></polyline>
      </svg>
    </symbol>
    <symbol id="svg-doc" viewBox="0 0 24 24">
      <title>Document</title>
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file">
        <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline>
      </svg>
    </symbol>
  </svg>
  <div class="side-bar">
    <div class="site-header">
      <a href="/" class="site-title lh-tight">
  LOST
</a>
      <a href="#" id="menu-button" class="site-button">
        <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg>
      </a>
    </div>
    <nav role="navigation" aria-label="Main" id="site-nav" class="site-nav">
      <ul class="nav-list"><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="/Data_Manipulation/data_manipulation.html" class="nav-list-link">Data Manipulation</a><ul class="nav-list "><li class="nav-list-item "><a href="/Data_Manipulation/collapse_a_data_set.html" class="nav-list-link">Collapse a Data Set</a></li><li class="nav-list-item "><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="/Data_Manipulation/Combining_Datasets/combining_datasets_overview.html" class="nav-list-link">Combining Datasets</a><ul class="nav-list"><li class="nav-list-item ">
                        <a href="/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html" class="nav-list-link">Vertical Combination</a>
                      </li><li class="nav-list-item ">
                        <a href="/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html" class="nav-list-link">Horizontal Combination (Deterministic)</a>
                      </li></ul></li><li class="nav-list-item "><a href="/Data_Manipulation/Creating_Dummy_Variables/creating_dummy_variables.html" class="nav-list-link">Creating Dummy Variables</a></li><li class="nav-list-item "><a href="/Data_Manipulation/determine_the_observation_level_of_a_data_set.html" class="nav-list-link">Determine the Observation Level of a Data Set</a></li><li class="nav-list-item "><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="/Data_Manipulation/Reshaping/reshape.html" class="nav-list-link">Reshaping Data</a><ul class="nav-list"><li class="nav-list-item ">
                        <a href="/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html" class="nav-list-link">Reshape Panel Data from Wide to Long</a>
                      </li><li class="nav-list-item ">
                        <a href="/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html" class="nav-list-link">Reshape Panel Data from Long to Wide</a>
                      </li></ul></li><li class="nav-list-item "><a href="/Data_Manipulation/rowwise_calculations.html" class="nav-list-link">Rowwise Calculations</a></li></ul></li><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="/Geo-Spatial/Geo-spatial.html" class="nav-list-link">Geo-Spatial</a><ul class="nav-list "><li class="nav-list-item "><a href="/Geo-Spatial/choropleths.html" class="nav-list-link">Choropleths</a></li><li class="nav-list-item "><a href="/Geo-Spatial/geocoding.html" class="nav-list-link">Geocoding</a></li><li class="nav-list-item "><a href="/Geo-Spatial/merging_shape_files.html" class="nav-list-link">Merging Shape Files</a></li><li class="nav-list-item "><a href="/Geo-Spatial/spatial_joins.html" class="nav-list-link">Spatial Joins</a></li></ul></li><li class="nav-list-item active"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="/Machine_Learning/Machine_Learning.html" class="nav-list-link">Machine Learning</a><ul class="nav-list "><li class="nav-list-item "><a href="/Machine_Learning/artificial_neural_network.html" class="nav-list-link">Artificial Neural Network</a></li><li class="nav-list-item "><a href="/Machine_Learning/boosted_regression_trees.html" class="nav-list-link">Boosted Regression Trees</a></li><li class="nav-list-item "><a href="/Machine_Learning/causal_forest.html" class="nav-list-link">Causal Forest</a></li><li class="nav-list-item  active"><a href="/Machine_Learning/decision_trees.html" class="nav-list-link active">Decision Trees</a></li><li class="nav-list-item "><a href="/Machine_Learning/penalized_regression.html" class="nav-list-link">Penalized Regression</a></li><li class="nav-list-item "><a href="/Machine_Learning/random_forest.html" class="nav-list-link">Random Forest</a></li></ul></li><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="/Model_Estimation/Model_Estimation.html" class="nav-list-link">Model Estimation</a><ul class="nav-list "><li class="nav-list-item "><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="/Model_Estimation/OLS/OLS.html" class="nav-list-link">Ordinary Least Squares</a><ul class="nav-list"><li class="nav-list-item ">
                        <a href="/Model_Estimation/OLS/simple_linear_regression.html" class="nav-list-link">Simple Linear Regression</a>
                      </li><li class="nav-list-item ">
                        <a href="/Model_Estimation/OLS/interaction_terms_and_polynomials.html" class="nav-list-link">Interaction Terms and Polynomials</a>
                      </li><li class="nav-list-item ">
                        <a href="/Model_Estimation/OLS/fixed_effects_in_linear_regression.html" class="nav-list-link">Fixed Effects in Linear Regression</a>
                      </li></ul></li><li class="nav-list-item "><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="/Model_Estimation/GLS/GLS.html" class="nav-list-link">Generalised Least Squares</a><ul class="nav-list"><li class="nav-list-item ">
                        <a href="/Model_Estimation/GLS/heckman_correction_model.html" class="nav-list-link">Heckman Correction Model</a>
                      </li><li class="nav-list-item ">
                        <a href="/Model_Estimation/GLS/logit_model.html" class="nav-list-link">Logit Model</a>
                      </li><li class="nav-list-item ">
                        <a href="/Model_Estimation/GLS/mcfaddens_choice_model.html" class="nav-list-link">McFadden's Choice Model (Alternative-Specific Conditional Logit)</a>
                      </li><li class="nav-list-item ">
                        <a href="/Model_Estimation/GLS/probit_model.html" class="nav-list-link">Probit Model</a>
                      </li></ul></li><li class="nav-list-item "><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="/Model_Estimation/Multilevel_Models/Multilevel_Models.html" class="nav-list-link">Multilevel Models</a><ul class="nav-list"><li class="nav-list-item ">
                        <a href="/Model_Estimation/Multilevel_Models/linear_mixed_effects_regression.html" class="nav-list-link">Linear Mixed-Effects Regression</a>
                      </li><li class="nav-list-item ">
                        <a href="/Model_Estimation/Multilevel_Models/random_mixed_effects_estimation.html" class="nav-list-link">Random/Mixed Effects in Linear Regression</a>
                      </li></ul></li><li class="nav-list-item "><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="/Model_Estimation/Research_Design/Research_Design.html" class="nav-list-link">Research Design</a><ul class="nav-list"><li class="nav-list-item ">
                        <a href="/Model_Estimation/Research_Design/instrumental_variables.html" class="nav-list-link">Instrumental Variables</a>
                      </li><li class="nav-list-item ">
                        <a href="/Model_Estimation/Research_Design/regression_discontinuity_design.html" class="nav-list-link">Regression Discontinuity Design</a>
                      </li><li class="nav-list-item ">
                        <a href="/Model_Estimation/Research_Design/two_by_two_difference_in_difference.html" class="nav-list-link">2x2 Difference in Difference</a>
                      </li></ul></li><li class="nav-list-item "><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="/Model_Estimation/Statistical_Inference/Statistical_Inference.html" class="nav-list-link">Statistical Inference</a><ul class="nav-list"><li class="nav-list-item ">
                        <a href="/Model_Estimation/Statistical_Inference/linear_hypothesis_tests.html" class="nav-list-link">Linear Hypothesis Tests</a>
                      </li><li class="nav-list-item ">
                        <a href="/Model_Estimation/Statistical_Inference/Nonstandard_Errors/nonstandard_errors.html" class="nav-list-link">Nonstandard Errors</a>
                      </li></ul></li></ul></li><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="/Presentation/Presentation.html" class="nav-list-link">Presentation</a><ul class="nav-list "><li class="nav-list-item "><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="/Presentation/Figures/Figures.html" class="nav-list-link">Figures</a><ul class="nav-list"><li class="nav-list-item ">
                        <a href="/Presentation/Figures/Scatterplots.html" class="nav-list-link">Scatterplots</a>
                      </li><li class="nav-list-item ">
                        <a href="/Presentation/Figures/Styling_Scatterplots.html" class="nav-list-link">Styling Scatterplots</a>
                      </li><li class="nav-list-item ">
                        <a href="/Presentation/Figures/bar_graphs.html" class="nav-list-link">Bar Graphs</a>
                      </li><li class="nav-list-item ">
                        <a href="/Presentation/Figures/density_plots.html" class="nav-list-link">Density Plots</a>
                      </li><li class="nav-list-item ">
                        <a href="/Presentation/Figures/faceted_graphs.html" class="nav-list-link">Faceted Graphs</a>
                      </li><li class="nav-list-item ">
                        <a href="/Presentation/Figures/heatmap_colored_correlation_matrix.html" class="nav-list-link">Heatmap Colored Correlation Matrix</a>
                      </li><li class="nav-list-item ">
                        <a href="/Presentation/Figures/histograms.html" class="nav-list-link">Histograms</a>
                      </li><li class="nav-list-item ">
                        <a href="/Presentation/Figures/line_graph_with_labels_at_the_beginning_or_end.html" class="nav-list-link">Line Graph with Labels at the Beginning or End of Lines</a>
                      </li><li class="nav-list-item ">
                        <a href="/Presentation/Figures/line_graphs.html" class="nav-list-link">Line Graphs</a>
                      </li><li class="nav-list-item ">
                        <a href="/Presentation/Figures/marginal_effects_plots_for_interactions_with_continuous_variables.html" class="nav-list-link">Marginal Effects Plots for Interactions with Continuous Variables</a>
                      </li><li class="nav-list-item ">
                        <a href="/Presentation/Figures/scatterplot_by_group_on_shared_axes.html" class="nav-list-link">Scatterplot by Group on Shared Axes</a>
                      </li><li class="nav-list-item ">
                        <a href="/Presentation/Figures/styling_line_graphs.html" class="nav-list-link">Styling Line Graphs</a>
                      </li><li class="nav-list-item ">
                        <a href="/Presentation/Figures/formatting_graph_legends.html" class="nav-list-link">Formatting Graph Legends</a>
                      </li></ul></li><li class="nav-list-item "><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="/Presentation/Tables/Tables.html" class="nav-list-link">Tables</a><ul class="nav-list"><li class="nav-list-item ">
                        <a href="/Presentation/Tables/Balance_Tables.html" class="nav-list-link">Balance Tables</a>
                      </li><li class="nav-list-item ">
                        <a href="/Presentation/Tables/Regression_Tables.html" class="nav-list-link">Regression Tables</a>
                      </li><li class="nav-list-item ">
                        <a href="/Presentation/Tables/Summary_Statistics_Tables.html" class="nav-list-link">Summary Statistics Tables</a>
                      </li></ul></li></ul></li><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="/Time_Series/Time_Series.html" class="nav-list-link">Time Series</a><ul class="nav-list "><li class="nav-list-item "><a href="/Time_Series/AR-models.html" class="nav-list-link">AR Models</a></li><li class="nav-list-item "><a href="/Time_Series/ARCH_Model.html" class="nav-list-link">ARCH Model</a></li><li class="nav-list-item "><a href="/Time_Series/ARMA-models.html" class="nav-list-link">ARMA Models</a></li><li class="nav-list-item "><a href="/Time_Series/GARCH_Model.html" class="nav-list-link">GARCH Model</a></li><li class="nav-list-item "><a href="/Time_Series/Granger_Causality.html" class="nav-list-link">Granger Causality</a></li><li class="nav-list-item "><a href="/Time_Series/creating_time_series_dataset.html" class="nav-list-link">Creating Time Series Dataset</a></li></ul></li><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="/Other/Other.html" class="nav-list-link">Other</a><ul class="nav-list "><li class="nav-list-item "><a href="/Other/create_a_conda_package.html" class="nav-list-link">Create a Conda Package (Python)</a></li><li class="nav-list-item "><a href="/Other/get_a_list_of_files.html" class="nav-list-link">Get a List of Files</a></li><li class="nav-list-item "><a href="/Other/import_a_foreign_data_file.html" class="nav-list-link">Import a Foreign Data File</a></li></ul></li><li class="nav-list-item"><a href="/Desired_Nonexistent_Pages/desired_nonexistent_pages.html" class="nav-list-link">Desired Nonexistent Pages</a></li><li class="nav-list-item"><a href="/Contributing/Contributing.html" class="nav-list-link">Contributing</a></li><li class="nav-list-item"><a href="/" class="nav-list-link">Home</a></li></ul>
    </nav>
    <footer class="site-footer">
      This site uses <a href="https://github.com/pmarsceill/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.
    </footer>
  </div>
  <div class="main" id="top">
    <div id="main-header" class="main-header">
        <div class="search">
          <div class="search-input-wrap">
            <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search LOST" aria-label="Search LOST" autocomplete="off">
            <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label>
          </div>
          <div id="search-results" class="search-results"></div>
        </div>
    </div>
    <div id="main-content-wrap" class="main-content-wrap">
          <nav aria-label="Breadcrumb" class="breadcrumb-nav">
            <ol class="breadcrumb-nav-list">
                <li class="breadcrumb-nav-list-item"><a href="/Machine_Learning/Machine_Learning.html">Machine Learning</a></li>
              <li class="breadcrumb-nav-list-item"><span>Decision Trees</span></li>
            </ol>
          </nav>
      <div id="main-content" class="main-content" role="main">
          <p>Decision trees are among the most common and useful machine learning methodologies. While they are a relatively simple method, they are incredibly easy to understand and implement for both classification and regression problems.</p>
<p>A decision tree “grows” by creating a cutoff point (often called a split) at a single point in the data that maximizes accuracy. The tree’s prediction is then based on the mean of the region that results from the input data.</p>
<p>For both regression and classification trees, it is important to optimize the number of splits that we allow the tree to make. If there is no limit, the trees would be able to create as many splits as the data will allow. This would mean the tree could perfectly “predict” every value from the training dataset, but would perform terribly out of sample (i.e., overfit the data). As such, it is important to keep a reasonable limit on the number of splits. This is achieved by creating a penalty that the algorithm has to pay in order to perform another split. If the increase in accuracy is worth more than the penalty, it will make the split.</p>
<p>For regression trees, the decision to split along a continuum of values is often made by minimizing the residual sum of squares:</p>
\[minimize \sum(y-prediction)^2\]
<p>This should be highly reminiscent of ordinary least squares. Where this differs is in the number of splits created, the binary nature of the splits, and its nonlinear nature.</p>
<p>The methodology behind classificiation is very similar, except the splits are decided by minimizing purity, such as the Gini index:</p>
\[G= 1 - \sum_{i = 1}^{C} (p_{i})^2\]
<p>The goal here is to create regions with as of classifications as possible, as such, a smaller Gini index implies a more pure region.</p>
      <h2 id="keep-in-mind">
          <a href="#keep-in-mind" aria-labelledby="keep-in-mind" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Keep in Mind
      </h2>
<ul>
  <li>While decision trees are easy to interpret and understand, they often underpreform relative to other machine learning methodologies.</li>
  <li>Even though they may not offer the best predictions, decision trees excel at identifying key variables in the data.</li>
</ul>
      <h2 id="also-consider">
          <a href="#also-consider" aria-labelledby="also-consider" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Also Consider
      </h2>
<ul>
  <li>Decision trees are the basis for all tree-based methodologies. More robust methods, such as <a href="/Machine_Learning/random_forest.html">Random Forests</a>, are a collection of decision trees that aggregate their decisions into a single prediction. These forests are often more useful for predictive modeling.</li>
</ul>
      <h1 id="implementations">
          <a href="#implementations" aria-labelledby="implementations" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Implementations
      </h1>
      <h2 id="python">
          <a href="#python" aria-labelledby="python" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Python
      </h2>
<p>The easiest way to get started with decision trees in Python is to use the <a href="https://scikit-learn.org/stable/index.html"><strong>scikit-learn</strong></a> package. In the example below, we’ll use data on the passengers of the Titanic to build a classification tree that predicts whether passengers survived or not (binary outcome) based on
properties such as passenger age, gender as recorded in the data, and class of cabin. As ever with machine learning, it’s essential that an out-of-sample set, also known as a test set, is retained and used to score the final model.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Install sklearn and pandas using pip or conda, if you don't have them already.
# Note that the 'f-strings' used in the print statements below are only available in Python&gt;=3.6.
</span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">plot_confusion_matrix</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="n">titanic</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"https://raw.githubusercontent.com/Evanmj7/Decision-Trees/master/titanic.csv"</span><span class="p">,</span>
                      <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Let's ensure the columns we want to treat as continuous are indeed continuous by using pd.to_numeric
# The errors = 'coerce' keyword argument will force any values that cannot be
# cast into continuous variables to become NaNs.
</span><span class="n">continuous_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s">'age'</span><span class="p">,</span> <span class="s">'fare'</span><span class="p">]</span>
<span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">continuous_cols</span><span class="p">:</span>
    <span class="n">titanic</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">titanic</span><span class="p">[</span><span class="n">col</span><span class="p">],</span> <span class="n">errors</span><span class="o">=</span><span class="s">'coerce'</span><span class="p">)</span>

<span class="c1"># Set categorical cols &amp; convert to dummies
</span><span class="n">cat_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s">'sex'</span><span class="p">,</span> <span class="s">'pclass'</span><span class="p">]</span>
<span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">cat_cols</span><span class="p">:</span>
    <span class="n">titanic</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">titanic</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="s">'category'</span><span class="p">).</span><span class="n">cat</span><span class="p">.</span><span class="n">codes</span>

<span class="c1"># Clean the dataframe. An alternative would be to retain some rows with missing values by giving
# a special value to nan for each column, eg by imputing some values, but one should be careful not to
# use information from the test set to impute values in the training set if doing this. Strictly speaking,
# we shouldn't be dropping the nans from the test set here (as we pretend we don't know what's in it) - but
# for the sake of simplicity, we will.
</span><span class="n">titanic</span> <span class="o">=</span> <span class="n">titanic</span><span class="p">.</span><span class="n">dropna</span><span class="p">()</span>

<span class="c1"># Create list of regressors
</span><span class="n">regressors</span> <span class="o">=</span> <span class="n">continuous_cols</span> <span class="o">+</span> <span class="n">cat_cols</span>
<span class="c1"># Predicted var
</span><span class="n">y_var</span> <span class="o">=</span> <span class="p">[</span><span class="s">'survived'</span><span class="p">]</span>

<span class="c1"># Create a test (25% of data) and train set
</span><span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">titanic</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>

<span class="c1"># Now let's create an empty decision tree to solve the classification problem:
</span><span class="n">clf</span> <span class="o">=</span> <span class="n">tree</span><span class="p">.</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                                  <span class="n">ccp_alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># The last option, ccp_alpha, prunes low-value complexity from the tree to help
# avoid overfitting.
</span>
<span class="c1"># Fit the tree with the data
</span><span class="n">clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train</span><span class="p">[</span><span class="n">regressors</span><span class="p">],</span> <span class="n">train</span><span class="p">[</span><span class="n">y_var</span><span class="p">])</span>

<span class="c1"># Let's take a look at the tree:
</span><span class="n">tree</span><span class="p">.</span><span class="n">plot_tree</span><span class="p">(</span><span class="n">clf</span><span class="p">)</span>

<span class="c1"># How does it perform on the train and test data?
</span><span class="n">train_accuracy</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">clf</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train</span><span class="p">[</span><span class="n">regressors</span><span class="p">],</span> <span class="n">train</span><span class="p">[</span><span class="n">y_var</span><span class="p">]),</span> <span class="mi">4</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">f'Accuracy on train set is </span><span class="si">{</span><span class="n">train_accuracy</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>

<span class="n">test_accuracy</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">clf</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="n">regressors</span><span class="p">],</span> <span class="n">test</span><span class="p">[</span><span class="n">y_var</span><span class="p">]),</span> <span class="mi">4</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">f'Accuracy on test set is </span><span class="si">{</span><span class="n">test_accuracy</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>

<span class="c1"># Show the confusion matrix
</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">test</span><span class="p">[</span><span class="n">regressors</span><span class="p">],</span> <span class="n">test</span><span class="p">[</span><span class="n">y_var</span><span class="p">])</span>

<span class="c1"># Although it won't be the same from run to run, this model scored around 80%
# out of sample, and has slightly more false positives than false negatives.
</span></code></pre></div></div>
      <h2 id="r">
          <a href="#r" aria-labelledby="r" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> R
      </h2>
<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load packages</span><span class="w">
</span><span class="c1"># install.packages("pacman") ## already installed</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">pacman</span><span class="p">)</span><span class="w">
</span><span class="n">p_load</span><span class="p">(</span><span class="n">rpart</span><span class="p">,</span><span class="n">rpart.plot</span><span class="p">,</span><span class="n">caret</span><span class="p">,</span><span class="n">rattle</span><span class="p">)</span><span class="w">
</span><span class="c1"># We will utilize data regarding passengers on their survival. We have multiple pieces of information on every passenger, including passenger age, sex, cabin number, and class. </span><span class="w">

</span><span class="c1"># Our goal is to build a decision tree that can predict whether or not passengers survived the wreck, making it a classification tree. These same methodologies can be used and applied to a regression tree framework.</span><span class="w">

</span><span class="c1"># Read in the data</span><span class="w">
</span><span class="n">titanic</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.csv</span><span class="p">(</span><span class="s2">"https://raw.githubusercontent.com/Evanmj7/Decision-Trees/master/titanic.csv"</span><span class="p">)</span><span class="w">

</span><span class="c1"># Set a seed for reproducability</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">1234</span><span class="p">)</span><span class="w">

</span><span class="c1"># The data is clean for the most part, but some variables have been read in as factors instead of numeric variables, so we can fix that with the following code.</span><span class="w">
</span><span class="n">titanic</span><span class="o">$</span><span class="n">age</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">as.numeric</span><span class="p">(</span><span class="n">titanic</span><span class="o">$</span><span class="n">age</span><span class="p">)</span><span class="w">
</span><span class="n">titanic</span><span class="o">$</span><span class="n">fare</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">as.numeric</span><span class="p">(</span><span class="n">titanic</span><span class="o">$</span><span class="n">fare</span><span class="p">)</span><span class="w">

</span><span class="c1"># As with all machine learning methodologies, we want to create a test and a training dataset</span><span class="w">

</span><span class="c1"># Take a random sample of the data, here we have chosen to use 75% for training and 25% for validation</span><span class="w">
</span><span class="n">samp_size</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">floor</span><span class="p">(</span><span class="m">0.75</span><span class="o">*</span><span class="n">nrow</span><span class="p">(</span><span class="n">titanic</span><span class="p">))</span><span class="w">
</span><span class="n">train_index</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sample</span><span class="p">(</span><span class="nf">seq_len</span><span class="p">(</span><span class="n">nrow</span><span class="p">(</span><span class="n">titanic</span><span class="p">)),</span><span class="n">size</span><span class="o">=</span><span class="n">samp_size</span><span class="p">,</span><span class="n">replace</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span><span class="w">

</span><span class="n">train</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">titanic</span><span class="p">[</span><span class="n">train_index</span><span class="p">,</span><span class="w"> </span><span class="p">]</span><span class="w">
</span><span class="n">test</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">titanic</span><span class="p">[</span><span class="o">-</span><span class="n">train_index</span><span class="p">,</span><span class="w"> </span><span class="p">]</span><span class="w">

</span><span class="c1"># Now that we have our test and train datasets, we can build our trees. Here, we will use the package "rpart". Other packages, such as "ranger" are also viable options.</span><span class="w">

</span><span class="c1"># Here we can pick some variables we think would be good, the tree will decide which ones are best. Some data we have isn't useful, such as an individual's name or the random ID we assigned passengers, so there is no need to include them.</span><span class="w">

</span><span class="n">basic_tree</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rpart</span><span class="p">(</span><span class="w">
  </span><span class="n">survived</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">pclass</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">sex</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">age</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">fare</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">embarked</span><span class="p">,</span><span class="w"> </span><span class="c1"># our formula</span><span class="w">
  </span><span class="n">data</span><span class="o">=</span><span class="n">train</span><span class="p">,</span><span class="w">
  </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"class"</span><span class="p">,</span><span class="w"> </span><span class="c1"># tell the model we are doing classification</span><span class="w">
  </span><span class="n">minsplit</span><span class="o">=</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="c1"># set a minimum number of splits</span><span class="w">
  </span><span class="n">cp</span><span class="o">=</span><span class="m">.02</span><span class="w"> </span><span class="c1"># set an optional penalty rate. It is often useful to try out many different ones, use the caret package to test many at once</span><span class="w">
</span><span class="p">)</span><span class="w">

</span><span class="n">basic_tree</span><span class="w">

</span><span class="c1"># plot it using the packages we loaded above</span><span class="w">
</span><span class="n">fancyRpartPlot</span><span class="p">(</span><span class="n">basic_tree</span><span class="p">,</span><span class="n">caption</span><span class="o">=</span><span class="s2">"Basic Decision Tree"</span><span class="p">)</span><span class="w">

</span><span class="c1"># This plot gives a very intuitive visual representation on what is going on behind the scenes.</span><span class="w">

</span><span class="c1"># Now we should predict using the test data we left out!</span><span class="w">
</span><span class="n">predictions</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">basic_tree</span><span class="p">,</span><span class="n">newdata</span><span class="o">=</span><span class="n">test</span><span class="p">,</span><span class="n">type</span><span class="o">=</span><span class="s2">"class"</span><span class="p">)</span><span class="w">

</span><span class="c1"># Make the numeric responses as well as the variables that we are testing on into factors</span><span class="w">
</span><span class="n">predictions</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.factor</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span><span class="w">
</span><span class="n">test</span><span class="o">$</span><span class="n">survived</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.factor</span><span class="p">(</span><span class="n">test</span><span class="o">$</span><span class="n">survived</span><span class="p">)</span><span class="w">

</span><span class="c1"># Create a confusion matrix which tells us how well we did.</span><span class="w">
</span><span class="n">confusionMatrix</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span><span class="n">test</span><span class="o">$</span><span class="n">survived</span><span class="p">)</span><span class="w">

</span><span class="c1"># This particular model got ~80% accuracy. This varies each time if you do not set a seed. Much better than a coin toss, but not great. With some additional tuning a decision tree can be much more accurate! Try it for yourself by changing the factors that go into the prediction and the penalty rates.</span><span class="w">

</span></code></pre></div></div>
          <hr>
          <footer>
              <div class="d-flex mt-2">
                  <p class="text-small text-grey-dk-000 mb-0">
                    <a href="https://github.com/lost-stats/lost-stats.github.io/edit/source/Machine_Learning/decision_trees.md" id="edit-this-page">Edit this page on GitHub.</a>
                  </p>
              </div>
          </footer>
      </div>
    </div>
      <div class="search-overlay"></div>
  </div>
</body>
</html>
